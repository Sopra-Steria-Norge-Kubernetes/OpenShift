{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Container Services by Sopra Steria","text":""},{"location":"#getting-started-with-openshift-container-platform-as-a-service","title":"Getting Started with OpenShift -  Container Platform as a Service","text":"<p>Welcome to the user guide for getting started with the Container Platform as a Service (CPaaS) provided by Sopra Steria, delivered with OpenShift. OpenShift simplifies application development, deployment, and management by leveraging the robustness and resiliency of the underlying VMware infrastructure. </p> <p>Built on top of Kubernetes, OpenShift offers a wide range of \"add-ons\" that enhance containerization, providing a rich set of tools and features. In this guide, we will walk you through the steps to kickstart your journey with OpenShift and unleash the full potential of containerized applications.</p> <p> </p>"},{"location":"About%20Container-Platform-as-a-Service/developing-container-platforma-as-a-service/","title":"Developing Container Platform as a Service","text":"<p>The Container Platform as a Service offering is subject to continous development based on user input. A customer champion attends regular meeting with customer platform engineers to ensure that the service fits customer needs and that feature requests are properly prioritized. Feature requests from customers are gathered in a common backlog, and new features are made available across all customers. </p> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/introduction/","title":"Introduction","text":"<p>Sopra Steria\u2019s Container Platform as a Service (CPaaS) is a managed Kubernetes solution that simplifies the deployment and operation of containerized applications. Built on Red Hat OpenShift, it provides a secure, production-ready environment tailored for both modern DevOps workflows and regulated enterprise use cases.</p> <p>By abstracting away Kubernetes and infrastructure complexity, the platform allows developers to focus on delivering business value through applications and services\u2014faster and more securely.</p> <p>The figure below illustrates the architecture and key components of the offering:</p> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/introduction/#key-benefits","title":"Key Benefits","text":"<ul> <li>Accelerated development - Standardized resources and workflows enable secure, scalable application delivery from day one.</li> <li>Developer autonomy \u2013 Teams manage their own release pipelines and deployment strategies within defined guardrails.</li> <li>Built-in DevSecOps \u2013 Integrated tooling for GitOps, vulnerability scanning, and compliance enforcement.</li> <li>Regulatory flexibility \u2013 Supports deployment in public cloud, private cloud, and sovereign infrastructures.</li> <li>Continuous evolution  - A dedicated Kubernetes team continuously improves the platform in response to customer needs and based on product evolvement.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/introduction/#service-variants","title":"Service Variants","text":"<p>To meet diverse needs across organizations, Sopra Steria delivers the platform through several service variants\u2014each tailored to different levels of control, isolation, and built-in functionality:</p> Service Description Dedicated Container Platform A fully isolated, customer-specific container platform for hosting and managing multiple Kubernetes or OpenShift clusters. Ideal for sovereign cloud needs or regulated environments. Cluster as a Service A minimal, standalone Kubernetes or OpenShift cluster managed by Sopra Steria. Includes core platform services like lifecycle management and patching. Cluster as a Service Premium A managed cluster delivered with ready-to-use developer environments (Tenants), including GitOps, RBAC, secret management, observability, and security policies."},{"location":"About%20Container-Platform-as-a-Service/introduction/#deployment-models","title":"Deployment Models","text":"<p>CPaaS is available on multiple infrastructure backends:</p> <ul> <li>Public cloud: AWS, Azure, GCP</li> <li>Private cloud: Sopra Steria\u2019s SolidCloud</li> <li>Sovereign cloud: Dedicated, isolated infrastructure hosted in either Sopra Steria or customer data centers</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/introduction/#service-levels-support","title":"Service Levels &amp; Support","text":"<ul> <li>Enterprise-grade SLA designed and customised to meet each customer\u2019s operational and business needs</li> <li>High availability delivered through fault-tolerant OpenShift control and runtime nodes</li> <li>Proactive monitoring with Prometheus/Grafana and a dedicated Remote Operations Centre</li> <li>24x7 support aligned with ITIL/ISO20000-compliant incident and change management processes</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/introduction/#who-is-it-for","title":"Who Is It For?","text":"<p>Sopra Steria Container management services is ideal for:</p> <ul> <li>Enterprises adopting container platforms for modern applications</li> <li>Organizations with compliance and sovereignty needs</li> <li>Teams seeking built-in DevSecOps and GitOps workflows without managing infrastructure</li> </ul> <p>\u2e3b</p>"},{"location":"About%20Container-Platform-as-a-Service/managing-tenant-configuration/","title":"Managing tenant configuration","text":"<p>Tenants are defined in the customers git. Changes in tenant configuration are submitted through pull requests. Once a pull request is approved, the changes are immediately synced to the environment through a GitOps process. This solution for ordering and configuring tenants ensures that both the Service Provider and the customer have insight into the current tenant configuration at any given time.</p> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/roles-and-responsibilities/","title":"Roles and Responsibilities","text":""},{"location":"About%20Container-Platform-as-a-Service/roles-and-responsibilities/#introduction","title":"Introduction","text":"<p>This document outlines the roles and responsibilities for Container platform as a Service (CPaaS) on a OpenShift Container Platform (OCP). It's designed to improve communication, boost collaboration, and clarify expectations for all stakeholders. Understanding these roles and duties is vital for the product's efficiency and success. All stakeholders should review this guide to understand their contributions better.</p>"},{"location":"About%20Container-Platform-as-a-Service/roles-and-responsibilities/#roles","title":"Roles","text":"<ul> <li>Sopra Steria (Provider): Sopra Steria is entrusted with its provision, management, and optimization, ensuring seamless alignment with industry standards and user requirements for the OCP. Sopra steria is also responsible for developing the Tenant concept for the OCP and help facilitate onboarding of new customer Tenants. </li> <li>Developer/User (Customer): Representing the end-users, these individuals leverage the platform for their developmental needs. Their active engagement and feedback are instrumental in refining the platform to better cater to user needs. This role is the consumer of the OCP Tenants.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/roles-and-responsibilities/#responsibilities","title":"Responsibilities","text":"<p>Below is a summary of the responsibilities of the different roles:</p> Area Sopra Steria (provider) Developer/user (customer) Additional Description Underlying Infrastructure -- -- -- Deploy and manage underlying infrastructure \u2714\ufe0f Lifecycle management of underlying infrastructure \u2714\ufe0f Kubernetes Cluster -- -- -- Deploy and configure OpenShift Container Platform (OCP) foundation \u2714\ufe0f Deploy and configure OpenShift clusters \u2714\ufe0f Deploy and configure OpenShift cluster-operators \u2714\ufe0f Cluster-operators are considered operators that are needed to configure and manage the OCP. Deploy and configure OpenShift operators for managed add-ons \u2714\ufe0f Deploy and configure cluster-scoped application operators \u2714\ufe0f \u2714\ufe0f Cluster-scoped application operators provide self-service features for developers. Developers are responsible for maintaining operator instances. Deploy and configure OpenShift namespace-scoped application operators \u2714\ufe0f Namespace-scoped application operators provide self-service features for developers. Developers are responsible these operators. Build and deploy applications in the OpenShift clusters \u2714\ufe0f Kubernetes Lifecycle management -- -- Lifecycle management of OpenShift cluster refers to the comprehensive process of planning, deploying, maintaining, updating, and eventually decommissioning an OpenShift cluster throughout its entire life span. Lifecycle management of OpenShift cluster \u2714\ufe0f Lifecycle management of OpenShift operators for infrastructure \u2714\ufe0f Lifecycle management of OpenShift cluster-scoped operators for applications \u2714\ufe0f Lifecycle management of OpenShift namespace-scoped operators for applications \u2714\ufe0f Capacity management -- -- -- Monitor underlying storage capacity \u2714\ufe0f Sopra Steria will continuously monitor the OCP nodes to ensure optimal performance and scalability. Monitor underlying compute capacity \u2714\ufe0f Sopra Steria will continuously monitor the OCP nodes to ensure optimal performance and scalability. Increase/Decrease storage and compute resources as needed \u2714\ufe0f Sopra Steria will manage and adjust resources based on capacity needs to maintain platform performance. Monitoring application consumption of resource in accordance with the tenant resource quota. \u2714\ufe0f Each customer is responsible for monitoring the limits and quotas of their tenant. Sopra Steria has provided guidelines for how this should be done. Ordering increased Tenant storage and compute capacity \u2714\ufe0f Customers are responsible for requesting more resources as needed based on their monitoring and demands. Increasing Tenant storage and compute capacity based on customers orders \u2714\ufe0f When the customers order more or less storage and compute capacity, Sopra steria is responsible for configuring the Tenants Quotas and limits. Network and connectivity -- -- -- Network Infrastructure Setup \u2714\ufe0f Setting up and maintaining the underlying network infrastructure and the connection to Solid Cloud 2 Network Managementt towards Sopra Steria components \u2714\ufe0f Establishing and maintaining firewall rules towards Sopra steria managed resources. Network Connectivity Establishment \u2714\ufe0f Establishing connections to customer components and other external components (Internet and 3rd parties). Requesting and Ordering Network Resources \u2714\ufe0f Ordering required firewall rules and other network resources for customer components outside of OCP. Assessing Network Security Measures* \u2714\ufe0f \u2714\ufe0f Evaluating and implementing security measures for the connections to external components. Implementing need for external connectivity based on customer orders \u2714\ufe0f Ordering needed firewall rules and network security policies to customer resources outside OCP. Load Balancing Implementation* \u2714\ufe0f Implementing and managing load balancers for traffic distribution towards the OCP cluster. Network Monitoring and Logging* \u2714\ufe0f Monitoring network traffic and logging for audit and troubleshooting purposes. DNS Management \u2714\ufe0f Managing DNS records for services running in the cluster. Order DNS records for custom routes \u2714\ufe0f Ingress and Egress Controllers \u2714\ufe0f Setting up and managing incoming and outgoing traffic controllers. Implementation and management of internal OpenShift network \u2714\ufe0f Expose applications on ingress and other applications on the OCP \u2714\ufe0f Network Policy Management \u2714\ufe0f Defining and implementing network policies controlling pod communication. Desired state configuration -- -- -- Deploy and manage desired state configuration tools such as ArgoCD \u2714\ufe0f ArgoCD practices the \"GitOps\" paradigm, where a Git repository serves as the source of truth for defining the desired state of applications and infrastructure. Deploy and manage developer Tenants based on customers orders \u2714\ufe0f Tenants are not native OpenShift concepts but act as wrappers around multiple OpenShift resources, offering a standardized development workflow, scalable resource access, and appropriate limits and access controls for each team. Connecting the Customer GIT repository in a tenant towards the GitOps tool ArgoCD managed by Sopra Steria \u2714\ufe0f ArgoCD continuously monitors and compares the actual state of the Kubernetes cluster (as deployed resources) with the desired state defined in the Git repository Deploy applications from GIT Repository managed in the developers Tenant \u2714\ufe0f Application Connectivity \u2714\ufe0f Ensuring proper communication configurations for applications. Custom Network Policies \u2714\ufe0f Defining specific network policies as per application requirements. Service Exposition Methods \u2714\ufe0f Deciding how services are exposed outside the cluster. Custom DNS Configurations for Applications \u2714\ufe0f Providing any custom DNS configurations for applications running in the developer tenant. Storage capabilities and Persistent volume -- -- -- Ensuring the underlying storage infrastructure's health, performance, and availability. \u2714\ufe0f Providing storage classes with  Read-Write-Many (RWX) capabilities \u2714\ufe0f Providing storage classes with  block and filesystem capabilities \u2714\ufe0f Creating and implementing needed Persistent Volumes (PVs) for applications \u2714\ufe0f Creating and implementing needed Persistent Volumes Claims (PVCs) for applications \u2714\ufe0f Access Control -- -- -- Configuring and managing OIDC with Azure AD as the identity provider in OCP \u2714\ufe0f Registering the Azure AD OIDC enterprise application for OCP and providing Sopra Steria with the configuration details \u2714\ufe0f Managing OIDC token and session lifecycles \u2714\ufe0f Configure standard Role-Based Access Control (RBAC) \u2714\ufe0f Synchronizing Azure AD groups and users with OCP based on configuration in Customers Tenants \u2714\ufe0f Provide Azure AD groups in the developer Tenants to be synchronised and assign users in AD groups. \u2714\ufe0f Private registry -- -- -- Deploy and manage private registry foundation \u2714\ufe0f Backup and restore of container images and artefacts \u2714\ufe0f Consume private registry \u2714\ufe0f Backup services -- -- -- Deploy and manage backup service foundation \u2714\ufe0f Regularly verifying that backups are valid and restorable. Regularly verify the integrity of backups* \u2714\ufe0f Regularly test restore capabilities of backups* \u2714\ufe0f Monitor backup jobs and setup alerts for failures* \u2714\ufe0f Configure off-site backup \u2714\ufe0f Provide a Off-site backup target \u2714\ufe0f Performe off-site restore \u2714\ufe0f Agree on backup policy and schedule \u2714\ufe0f \u2714\ufe0f This includes establishing how long backups are stored before being deleted or archived. Establish, run and schedule local backup \u2714\ufe0f Restore local backup \u2714\ufe0f Kubernetes monitoring and logging -- -- -- Deploy and manage monitoring and alerting foundation \u2714\ufe0f Configure monitoring and alerting of cluster health \u2714\ufe0f Facilitate the possibility of standard monitoring and alerting of applications \u2714\ufe0f Configure standard monitoring and alerting for applications \u2714\ufe0f Configure custom monitoring and alerting for applications \u2714\ufe0f Monitor Kubernetes resources (pods, nodes, services) \u2714\ufe0f Monitor user access and activities in the cluster \u2714\ufe0f Monitor user access and activities in tenants \u2714\ufe0f Conduct performance benchmarking for applications \u2714\ufe0f Create and manage monitoring dashboards for OCP \u2714\ufe0f Create and manage standard monitoring dashboards for applications in Tenants \u2714\ufe0f Create and manage custom monitoring dashboards for applications in Tenants \u2714\ufe0f Deploy and manage logging foundation \u2714\ufe0f Configure export to external log endpoint \u2714\ufe0f Provide log search dashboard for infrastructure components \u2714\ufe0f Configure custom logging and dashboards for applications \u2714\ufe0f Analyze logs for anomalies and unexpected activities* \u2714\ufe0f Integrate critical alerts with incident management systems \u2714\ufe0f Security and compliance -- -- -- Deploy and manage security foundation \u2714\ufe0f Report compliance metrics* \u2714\ufe0f Send security events to customer SOC* \u2714\ufe0f Enabling scanning of security vulnerabilities in developer applications* \u2714\ufe0f Scan for security vulnerabilities in developer applications* \u2714\ufe0f Handle security vulnerabilities in developer applications \u2714\ufe0f <ul> <li>Responsibilities that are not ready now must be implemented before total production.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/","title":"Service Breakdown: Container Platform as a Service","text":"<p>Sopra Steria\u2019s Container Platform as a Service (CPaaS) is a managed Kubernetes/OpenShift offering that enables customers to build, run, and scale containerized applications securely and efficiently \u2014 whether in the cloud or on-premises.</p> <p>CPaaS is delivered through a layered service model and can be tailored to meet different needs for compliance, control, and developer enablement.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#platform-overview","title":"Platform Overview","text":"<p>The diagram below illustrates how the service is composed</p> <p></p> <p>The service is built in three layers, which can be tailored to match each customer\u2019s specific requirements.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#1-infrastructure-layer","title":"1. Infrastructure Layer","text":"<ul> <li>Delivered from Sopra Steria\u2019s SolidCloud, public cloud (Azure, AWS, GCP), or a sovereign cloud setup in Sopra Steria\u2019s data center, optionally extended or isolated to the customer\u2019s own data center</li> <li>Provides core services such as compute, storage, networking, and security</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#2-foundation-services-included-in-all-clusters","title":"2. Foundation Services (included in all clusters)","text":"<ul> <li>Cluster lifecycle management (deployments, patching, upgrades)</li> <li>Networking (API exposure and ingress)</li> <li>Authentication and RBAC (OIDC integration)</li> <li>Persistent storage</li> <li>Monitoring, logging, and basic compliance tooling</li> <li>Self-service access (GUI and API)</li> <li>Administration of core Kubernetes/OpenShift services</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#3-optional-developer-services","title":"3. Optional Developer Services","text":"<p>These add-ons provide full developer environments and operational support:</p> <ul> <li>GitOps and CI/CD pipelines</li> <li>Secret management and RBAC policies</li> <li>Backup and restore</li> <li>Application runtime environments (Tenants)</li> <li>Private container registry</li> <li>Advanced security policies and observability</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#service-variants","title":"Service Variants","text":"<p>Based on your organization\u2019s needs, you choose one or more of the following service variants:</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#dedicated-container-platform","title":"Dedicated Container Platform","text":"<p>A fully isolated platform environment designed for customers with strict security, compliance and data residency requirements.</p> <ul> <li>Runs on dedicated infrastructure, operated by Sopra Steria or hosted on-premises</li> <li>Ideal for sovereign cloud or regulated environments</li> <li>After setup, you can purchase one or more dedicated clusters from Sopra Steria (CaaS or Premium)</li> <li>Optional developer services can be added per cluster</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#cluster-as-a-service-caas","title":"Cluster as a Service (CaaS)","text":"<p>A dedicated OpenShift cluster delivered with the full foundation service layer.</p> <ul> <li>Hosted on Sopra Steria SolidCloud or public cloud</li> <li>Ideal for teams with existing DevOps capability</li> <li>You manage your own pipelines, RBAC, tenants, and GitOps setup</li> <li>Developer services are not included by default</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#cluster-as-a-service-premium","title":"Cluster as a Service Premium","text":"<p>A dedicated OpenShift cluster delivered with built-in developer environments.</p> <ul> <li>Includes GitOps, RBAC, observability, secrets management, and runtime environments</li> <li>Ideal for teams who want Sopra Steria to manage platform enablement</li> <li>Delivered on SolidCloud or public cloud</li> </ul> <p>Read more about the features included - Core Concepts</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#how-it-fits-together","title":"How It Fits Together","text":"<p>All customers receive dedicated OpenShift clusters. The main differences between the service variants lie in:</p> <ul> <li>The level of infrastructure and operational isolation</li> <li>The scope of developer enablement and automation included</li> </ul> Platform Layer Dedicated Container Platform Cluster as a Service Cluster as a Service Premium Cluster infrastructure \u2705 Dedicated physical (customer-specific) \u2705 Cluster runs on shared infrastructure platform (workload isolated per customer) \u2705 Same as CaaS (isolated per customer) Control &amp; management stack \u2705 Fully isolated management tooling and access control (per platform) \ud83d\udd04 Operated via Sopra Steria\u2019s shared control plane \ud83d\udd04 Same as CaaS Foundation services \ud83d\udd01 Optional per cluster \u2705 Included \u2705 Included (This is the service) Developer services \ud83d\udd01 Optional per cluster \u274c Not included. each add-on is optional \u2705 Included <p>\u2705 = Included or dedicated \ud83d\udd04 = Logically separated, but managed through a shared Sopra Steria operational platform \ud83d\udd01 = Available optionally per cluster \u274c = Not included</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Service-breakdown/#which-setup-is-right-for-you","title":"Which Setup Is Right for You?","text":"<p>Choosing the right setup depends on your organization\u2019s compliance needs, internal platform maturity and how much of the operational responsibility you want to take on.</p> If you are... Recommended Setup A public sector or regulated organization requiring strict control, data residency, or isolation Dedicated Container Platform + one or more managed clusters A customer with a strong, internal platform team that understands OpenShift/Kubernetes, and has a mature DevOps culture Cluster as a Service A team with limited DevOps capacity that needs Sopra Steria to handle the platform setup, pipelines, tenants and security guardrails Cluster as a Service Premium"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/","title":"Core Concepts - OpenShift Teams and Tenants","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/#overview","title":"Overview","text":"<p>OpenShift Container Platform as a Service consists of three key concepts to deliver a standardise environment to developers:</p> <ul> <li>Clusters: A Kubernetes/OpenShift cluster to run containers which is ordered from Sopra Steria as a managed service. </li> <li>OpenShift Teams: A logical \u201clanding zone\u201d within a cluster that provides shared resources and centralized management to multiple runtime Environemnts called OpenShift Tenants </li> <li>OpenShift Tenants: Isolated runtime environments dedicated to a single application or microservices, with their own resources, configurations, and access controls.</li> </ul> <p>Understanding the relationship between these concepts is essential for making full use of Sopra Steria\u2019s framework and accelerating your time to market. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/#what-are-openshift-teams","title":"What are OpenShift Teams?","text":"<p>OpenShift Teams are a management layer that sits above OpenShift Tenants, providing shared resources and centralized administration for a teams microservices. It is used to manage multiple OpenShift Tenants and their namespaces and projects. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/#features-and-benefits","title":"Features and Benefits","text":"<ul> <li>Observability Dashboards: Grafana instance with datasources for all team tenants, enabling unified monitoring across all team applications.</li> <li>Secret Management: ClusterSecretStores accessible by all team tenants, reducing duplication by sharing credentials and configurations.</li> <li>GitOps Credentials: Shared repository access credentials for ArgoCD, simplifying administration by managing multiple tenants from one central location.</li> <li>Consistent Policies: Apply team-level policies to all member tenants, ensuring uniform governance and streamlined management.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/#getting-started-with-openshift-team","title":"Getting Started with OpenShift Team","text":"<p>Quick Start: OpenShift Teams</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/#what-are-openshift-tenants","title":"What are OpenShift Tenants?","text":"<p>OpenShift Tenants are isolated runtime environments dedicated to single applications or microservices. Each tenant provides its own resource quota shared among the containers running within its runtime environments, ensuring resource isolation and management.</p> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/#features-and-benefits_1","title":"Features and Benefits","text":"<ul> <li>Backup Service: Define backup schedules, specify which resources are backed up, and configure retention policies to ensure data protection and business continuity.</li> <li>Runtime Environments: OpenShift namespaces configured with resource quotas and network policies to isolate workloads, protecting applications from interference.</li> <li>Resource Management: Enforce resource quotas and limits to optimize usage and cost, guaranteeing CPU and memory resources for predictable performance.</li> <li>Role-Based Access Control (RBAC): Apply least-privilege access controls to secure tenant resources, enabling granular and secure operations.</li> <li>Network Traffic Rules: Manage ingress and egress traffic via defined lists of allowed URLs and IP sets, controlling access to and from tenant applications for enhanced security.</li> <li>Secret Management: Integrate with Azure KeyVault for secure dynamic retrieval of application secrets; alternatively, use Bitnami Sealed Secrets for version-controlled secret management, centralizing and securing sensitive information.</li> <li>Grafana Dashboards: Optionally provision dedicated Grafana instances for customized monitoring and observability, supporting tenant-specific insights.</li> <li>GitOps Integration: Use Git repositories to define and version-control tenant resources, enabling streamlined continuous deployment with ArgoCD and facilitating rapid releases.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/core-concepts/#getting-started-with-openshift-tenant","title":"Getting Started with OpenShift Tenant","text":"<p>Quick Start: OpenShift Tenants</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-persistent-volumes-using-volumesnapshots/","title":"Backup Persistent Volumes using VolumeSnapshots","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-persistent-volumes-using-volumesnapshots/#introduction","title":"Introduction","text":"<p>This user guide describes how backup using VolumeSnapshots are working in OpenShift. This process is controlled by the developers and they may backup and restore using this method as they wish. </p> <p></p> <p>The diagram illustrates the process of managing persistent storage within an OpenShift Container Platform (OCP) cluster for Tenant A. A deployment (Deployment X) utilizes a persistent volume (PV X) to store its data. To ensure data protection, a snapshot of this volume is created, which captures the state of the data at a specific point in time. This snapshot can later be used to restore the deployment to its previous state if needed, providing a mechanism for data recovery and consistency. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-persistent-volumes-using-volumesnapshots/#backup-and-restore-persistent-volumes","title":"Backup and restore persistent volumes","text":"<p>Container Platform as a Service allows developers to take point-in-time snapshots of persistent volume claims and restore the snapshots to a new persistent volume claim.</p> <ol> <li> <p>Navigate to Storage -&gt; PersistentVolumeClaims in the OpenShift console. </p> </li> <li> <p>Click the three dots to the right of the pvc you want to snapshot. </p> </li> <li> <p>Proceed with default settings, and click \"Create\" </p> </li> <li> <p>Navigate to Storage -&gt; VolumeSnapshots to find your recently created snapshot. </p> </li> <li> <p>To restore a PVC from the snapshot click the three dots next to the right of the VolumeSnapshot. </p> </li> <li> <p>Proceed with default settings to create a new pvc. </p> </li> <li> <p>Find your restored snapshot under Storage -&gt; PersistentVolumeClaims. </p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/","title":"Backup process for tenants using Velero","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#introduction","title":"Introduction","text":"<p>This document provides an overview of the backup and restore processes implemented within our OpenShift platform, specifically focusing on the utilization of Velero for backup management and the external backup storage endpoint. </p> <p>In addition this document explains the backup setup, the types of backups we perform, how you can specify your backup scheduele, and our overall approach to keeping tenant data secure and recoverable.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#design","title":"Design","text":"<p>This diagram shows the backup and restore process for Tenant A in an OpenShift cluster using Velero and external S3 endpoint as the external storage. Velero backs up the deployment and persistent volume (PV X) and sends the data to an S3-compatible storage</p> <p>The backup data is stored externally for safety. If needed, Velero can restore this data from the S3 storage back into the OpenShift cluster. This setup helps protect tenant data and makes sure it can be recovered if something goes wrong.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#backup","title":"Backup","text":"<p>We provide backup schedueles for the customer\u2019s tenant as part of the tenant order process. Within the order, you can specify how frequently backups should be taken for a namespace, choose a name for the backup, and determine the time to live (<code>ttl</code>) for which the backup will be stored. Additionally, you have the flexibility to set up multiple backup schedules, allowing for daily, weekly, and monthly backups if needed.</p> <p>Info</p> <p>For each backup schedule, a complete backup is performed, covering all resources within the specified environment as well as all environments defined in the tenant order.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#backup-object","title":"Backup Object","text":"<p>The backup object executed for each schedule is defined as follows:</p> Backup object<pre><code>---\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: demo-backup\n  namespace: openshift-adp\n  annotations:\n    argocd.argoproj.io/sync-options: Prune=false\n    argocd.argoproj.io/sync-wave: \"2\"\n    argocd.argoproj.io/compare-options: IgnoreExtraneous\n    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true\nspec:\n  schedule: 0 1 * * *\n  template:\n    ttl: 168h0m0s\n    includedNamespaces:\n      - demo-namespace\n    defaultVolumesToFsBackup: true\n</code></pre> <p>If you need an overview of the backups taken for a specific namespace, you can find this information in your tenant order or by contacting Sopra Steria through your customer portal.</p> <p>To learn more about how to order a backup for a namespace, please refer to this guide:</p> <ul> <li>Hot to configure a backup scheduele for your tenant</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#user-resources","title":"User Resources","text":"<p>The table below shows who is responsible for performing backups, selecting which user namespaces and tenant data need to be backed up, determining the backup frequency, and deciding how long the backups should be stored. It provides a clear view of the roles and responsibilities involved in managing the backup process for user resources and tenant data.</p> Resources to be included Backup name Scheduele Time to live  Sopra Steria &amp; Customer<sup>1</sup>  Customer  Customer  Customer"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#restore","title":"Restore","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#process","title":"Process","text":"<p>User environments can be restored on demand, but to restore data from the Persistent Volumes in these environments, we must first disable ArgoCD\u2019s self-healing feature that automatically tries to recreate deleted objects and resources. </p> <p>This step is crucial because, for the restoration process to be successful, the environment must be cleared of all existing data. Once the environment is empty, we can proceed with restoring the data, ensuring that all resources are accurately restored. </p> <p></p> <p>After the restore, ArgoCD\u2019s self-healing can be re-enabled to maintain the desired state of the environment.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#submitting-a-restore-request","title":"Submitting a Restore Request","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#instructions","title":"Instructions","text":"<p>Environments can be restored on demand. Please contact Sopra Steria through your customer portal and provide the following details for your restore request</p> <ul> <li>The name of the backup specified in the tenant order</li> <li>Which namespace(s) you want to back up</li> <li>Which day you want to restore a backup from</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Backup/backup-process-for-tenants-using-velero/#disable-argocd-self-healing","title":"Disable ArgoCD Self-Healing","text":"<p>Info</p> <p>This is not necessary in a DR-situation as we will apply GitOps after restore is complete. </p> <p>Disable self-healing in ArgoCD to prevent it from recreating resources from GitOps. This can be done by setting selfHeal to false in the tenant order. </p> <p>It\u2019s important to ensure this setting is applied to the specific tenants or environments for which you have requested backups, as this will allow for a smooth and successful restoration process without interference from automated processes.</p> Tenant<pre><code>...\n  argocd:\n...\n    syncPolicy:\n      selfHeal: false\n      prune: true\n...\n</code></pre> <p>Before starting the restoration, ensure that the change to disable self-healing has been approved through a pull request. Submit this request to Sopra Steria for review, so they can confirm that the change is appropriate and won\u2019t interfere with other processes. </p> <p>Once it\u2019s approved, Sopra Steria can proceed with the restoration without iinterference from ArgoCD.</p> <ol> <li> <p>Sopra Steria ensures that all namespaces within a tenant are set up for backup, but it is the Customer who decides whether the tenant should be backed up or not.\u00a0\u21a9</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Introduction-GitOps/","title":"Introduction to GitOps","text":"<p>This page introduces how to use OpenShift GitOps with Argo CD to manage and deploy Kubernetes resources from Git. </p> <p>Official Documentation: </p> <ul> <li>OpenShift GitOps</li> <li>ArgoCD</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Introduction-GitOps/#benefits-of-gitops","title":"Benefits of GitOps","text":"<p>GitOps uses Git repositories as the single source of truth. ArgoCD ensures cluster state matches repository definitions.</p> <p>Key benefits:</p> <ul> <li>Faster releases: Automated deployments</li> <li>Improved reliability: Consistent environments, reduced drift</li> <li>Enhanced security: Auditable, compliant deployments</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Introduction-GitOps/#gitops-workflow-with-openshift-tenants","title":"GitOps Workflow with OpenShift Tenants","text":"<p>To setup GitOps for you OpenShift Tenant follow the Getting Started guide. Below is a short summary of the steps to push changes to OpenShift cluster:</p> <ol> <li>Define tenant with desired GitOps method \u2192 submit to OCP Admin (Tenant Quick Start)</li> <li>Choose <code>user-defined</code> and/or <code>auto-defined</code> application creation methods (GitOps Setup)</li> <li>Define Kubernetes resources in tenant's main Git repository (Getting Started)</li> <li>ArgoCD syncs resources from repository to cluster</li> </ol> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Introduction-GitOps/#best-practices","title":"Best Practices","text":"<p>For additional info see GitOps Best Practices for ArgoCD implementation guidelines and more information.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/auto-merging/","title":"Auto-Merging Existing of Teams &amp; Tenants","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/auto-merging/#introduction","title":"Introduction","text":"<p>We've created two seperate auto-merge workflow to approve and merge team and tenant changes that don't need manual checks. This helps speed up the process and lets developers focus more on important work instead of getting stuck in routine tasks. </p> <p>Auto-merging happens only when all status checks pass. This way, we ensure that only well-tested and verified changes go through, keeping the system stable and reliable. Auto-merging is restricted to directories where Sopra Steria has CODEOWNER rights, specifically in the <code>/wave-*/</code> directories and the <code>/team-definitions/</code> directory. This restriction is enforced by GitHub's branch protection rules, ensuring that only changes meeting all criteria and passing all checks are automatically merged. This approach guarantees that our automated system maintains the high standards we expect for our codebase.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/auto-merging/#team-what-requires-manual-approval","title":"Team - What Requires Manual Approval","text":"<p>The table below contains which changes in a team values file that would trigger a manual approval.</p> <p>Note</p> <ul> <li> <p>When deleting an existing team or creating a brand new team from scratch, it will trigger for manual approval</p> </li> <li> <p>If the team values file contains fields that does not exist in the helm chart or a required field is not specified, it will trigger for manual approval</p> </li> </ul> Category Field Allowed Actions Restrictions requests[] Cpu DecreaseChange unit(Gi - Mi) IncreaseRemove unit Memory DecreaseChange unit(Gi - Mi) IncreaseRemove unit"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/auto-merging/#tenant-what-requires-manual-approval","title":"Tenant - What Requires Manual Approval","text":"<p>The table below contains which changes in a tenant values file that would trigger a manual approval.</p> <p>Note</p> <ul> <li> <p>When deleting an existing tenant or creating a brand new tenant from scratch, it will trigger for manual approval</p> </li> <li> <p>If the tenant values file contains fields that does not exist in the helm chart or a required field is not specified, it will trigger for manual approval</p> </li> </ul> Category Field Allowed Actions Restrictions limits[] Cpu DecreaseChange unit(Gi - Mi) IncreaseRemove unit Memory DecreaseChange unit(Gi - Mi) IncreaseRemove unit requests[] Cpu DecreaseChange unit(Gi - Mi) IncreaseRemove unit Memory DecreaseChange unit(Gi - Mi) IncreaseRemove unit environments[] name Add ChangeRemove"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/auto-merging/#required-status-checks-to-pass-before-merging","title":"Required Status Checks to Pass Before Merging","text":"<p>To ensure that no changes outside of monitored fields are merged, specific status checks must pass before any changes can be merged. This process includes several key steps:</p> <p>Validate team/tenant files: </p> <ul> <li>Validates the syntax and performs a series of checks to ensure that the team or tenant orders adheres to specified standards and requirements.</li> </ul> <p>Check Auto-merge / check-files-in-pr: </p> <ul> <li>This check verifies the presence of files in the pull request (PR) and uploads temporary files for comparison against existing files in the main branch.</li> </ul> <p>Check Auto-merge / check-if-files-can-auto-merge:</p> <ul> <li>This step downloads the temporary files, performs a comparison, and validates the changes to ensure they can be safely auto-merged.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/","title":"Custom Auto-Defined TargetRevision","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/#introduction","title":"Introduction","text":"<p>This solution allows setting the <code>targetRevision</code> at the application level for different environments in OpenShift. The generator picks up component names and creates  <code>targetRevision</code> values based on the application folder name instead of using <code>HEAD</code>. This enables more controlled and predictable versioning of applications in different environments.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/#pre-requisites","title":"Pre-requisites","text":"<p>To use this solution, the following parameters must be set:</p> <ul> <li><code>argocd.enable_auto_defined_apps</code>: Must be set to <code>true</code> to enable the use of <code>applicationsets</code> for deployments. <pre><code>argocd:\n  enable_auto_defined_apps: true\n</code></pre></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/#configuration","title":"Configuration","text":"<p>When <code>environments[].custom_auto_defined_targetRevision</code> is set to <code>true</code>, the applicationset (auto-defined applications) will not use <code>targetRevision</code> equal to <code>HEAD</code>, but instead to the application folder name.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/#example-of-targetrevision-values","title":"Example of targetRevision values","text":"<ul> <li> <p>test: <code>&lt;FOLDER_NAME&gt;-test</code></p> <ul> <li>Example: <code>app-one-name-test</code></li> </ul> </li> <li> <p>dev: <code>&lt;FOLDER_NAME&gt;-dev</code></p> <ul> <li>Example: <code>app-two-name-dev</code></li> </ul> </li> <li> <p>prod: <code>HEAD</code></p> <ul> <li>In accordance with good GitOps practices, <code>main</code> should initially be <code>prod</code>. Therefore, we will use <code>HEAD</code> (latest commit in <code>main</code>) for <code>prod</code>.</li> </ul> </li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/#tenant-configuration","title":"Tenant Configuration","text":"<p>An example configuration of a tenant might look like this:</p> <p><pre><code>...\nenvironments:\n  - name: test\n    custom_auto_defined_targetRevision: true\n  - name: dev\n    custom_auto_defined_targetRevision: false\nargocd:\n  enable_user_defined_apps: false\n  enable_auto_defined_apps: true\n...\n</code></pre>  Read more about this feature in the Environment section here:</p> <ul> <li>How to configure an Environment</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/#argocd","title":"ArgoCD","text":"<p>The changes will also be visible in ArgoCD as the application cards now have an update targetRevision which shows either the <code>&lt;folder-name&gt;-&lt;environment&gt;</code> or <code>HEAD</code>.</p> custom_auto_defined_targetRevision: true custom_auto_defined_targetRevision: false"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/cutom-auto-defined-target-revision/#summary","title":"Summary","text":"<p>This solution ensures that applications can have distinct versions for different environments by using application folder names as the <code>targetRevision</code>. This provides better control over which versions run in <code>test</code> and <code>prod</code> environments.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/","title":"OpenShift GitOps Best Practices","text":"<p>This page provides best practices for configuring Argo CD and optimizing GitOps workflows, including guidance on sync options and their recommended usage.</p> <p>Official Documentation: ArgoCD Documentation</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#configuration","title":"Configuration","text":"<p>Info</p> <p>The ArgoCD Application Controller is configured to auto-sync every 6 minutes. </p> <p>This interval is set to reduce CPU load, thereby improving the overall performance of the Application Controller.</p> <p>For detailed instructions on configuring ArgoCD for your tenant, refer to:</p> <ul> <li>How to Configure Argo CD</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#argocd-workflow","title":"ArgoCD Workflow","text":"<p>The ArgoCD Workflow is explain below in simple steps:</p> <ol> <li> <p>Make changes to the code in the repository connected to ArgoCD.</p> <ul> <li>The ArgoCD ApplicationSets will automatically sync resources from the source repository.</li> </ul> </li> <li> <p>By default, the Application Controller will sync your changes 6 minutes after they are pushed. If you'd prefer not to wait, follow these steps:</p> <ol> <li>Open your ArgoCD GitOps Developer URL and locate the application where you've made the code changes.</li> <li>In the application view, click <code>REFRESH</code>. A standard refresh checks if there have been changes in your source repository, such as a new commit or a new version of a Helm chart in the registry.</li> <li>If no updates appear, try clicking <code>HARD REFRESH</code>. This action invalidates the application's manifest cache, forcing the manifests to be re-rendered regardless of changes in the source.</li> <li>If the issue persists, click <code>SYNC</code>. This will reconcile the current cluster state with the desired state as defined in Git.</li> </ol> </li> <li> <p>Wait for the synchronization process to complete.</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#options-in-argocd","title":"Options in ArgoCD","text":"<p>ArgoCD provides several options for managing applications, making it easier to monitor and maintain the resources in your cluster. Below is an overview of the key options:</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#refresh-options-in-argocd","title":"Refresh Options In ArgoCD","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#refresh","title":"Refresh","text":"<p>The <code>Refresh</code> option is used to update the application state from Git. This is useful for checking if there have been any changes without necessarily triggering a full sync.</p> <ul> <li> <p>Refresh: Updates the application state based on changes in Git or Helm sources. It checks for updates but does not force a full cache refresh.</p> </li> <li> <p>Hard Refresh: Invalidates the cache and forces a full update of the application's state by re-reading all resources from Git or Helm. This is useful if you suspect discrepancies between the cache and the actual state.</p> </li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#sync-options-in-argocd","title":"Sync Options in ArgoCD","text":"<p>These options will be explained in the section below.</p> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#general-sync-options","title":"General Sync Options","text":"<ol> <li> <p>PRUNE    When selected, ArgoCD will remove any resources in the cluster that are no longer present in the Git repository. This ensures that obsolete resources are cleaned up during synchronization.</p> </li> <li> <p>DRY RUN    This option allows you to simulate a sync without actually applying any changes. It shows you what changes would be made if you were to perform a full sync, but doesn't alter the live environment.</p> </li> <li> <p>APPLY ONLY    When checked, only the resources that are defined in the Git repository are updated or applied, but resources that are no longer in the Git repository will not be pruned or deleted.</p> </li> <li> <p>FORCE    This option forces the sync operation by overriding any validation checks or conflicts. It is useful when you need to apply changes even if there are conflicts with the current state of resources in the cluster.</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#additional-sync-options","title":"Additional Sync Options","text":"<ol> <li> <p>SKIP SCHEMA VALIDATION    Skips the validation of the resource's schema before applying changes. This can be useful if you are deploying custom resources or using configurations that might not adhere strictly to schema validation.</p> </li> <li> <p>AUTO-CREATE NAMESPACE    When selected, ArgoCD will automatically create the target namespace if it doesn't already exist. This is useful when deploying applications to a fresh cluster or a namespace that hasn't been set up yet.</p> </li> <li> <p>PRUNE LAST    This ensures that resource pruning (deleting obsolete resources) happens as the final step of the sync process. It ensures that existing resources are updated or replaced before any removals occur.</p> </li> <li> <p>APPLY OUT OF SYNC ONLY    When this is checked, only resources that are currently out of sync (i.e., that differ from the desired state in Git) are updated. Resources that are already in sync will be left untouched.</p> </li> <li> <p>RESPECT IGNORE DIFFERENCES    This option respects any specific fields or resources that have been marked to \"ignore differences.\" These differences might be those defined in an override or annotation to avoid syncing certain parts of the manifest.</p> </li> <li> <p>SERVER-SIDE APPLY    This syncs the resources using Kubernetes' server-side apply, which enables declarative updates to resources by merging the desired state with the server\u2019s current state.</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#prune-propagation-policy","title":"Prune Propagation Policy","text":"<ul> <li> <p>foreground    This specifies the deletion policy when pruning resources. \"Foreground\" ensures that resources are deleted in a way that respects dependencies, i.e., ArgoCD will wait for dependent resources to be fully deleted before removing the resource itself.</p> </li> <li> <p>background    In the background mode, ArgoCD will delete the resource immediately and allow Kubernetes to handle the deletion of any dependent resources in the background. This can be quicker, but it may leave some orphaned resources if dependencies aren't managed correctly.</p> </li> <li> <p>orphan    The orphan option allows ArgoCD to delete resources without waiting for their dependents or ensuring that dependents are handled. This option should be used with caution, as it can leave behind resources that were dependent on the deleted resource, potentially causing inconsistencies in the cluster.</p> </li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-best-practices/#other-options","title":"Other Options","text":"<ol> <li> <p>REPLACE    This forces the replacement of all resources instead of just updating them. It deletes and recreates the resources, which may result in downtime but ensures a full reset of the state.</p> </li> <li> <p>RETRY    Automatically retries the sync operation in case of failures, potentially with backoff logic to space out retries.</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/","title":"GitOps Setup","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#introduction","title":"Introduction","text":"<p>This page gives an introduction to how to setup OpenShift GitOps with your OpenShift Tenants and Git repository.</p> <p>Note</p> <p>Before you start, make sure you have a basic understanding of GitOps, ArgoCD, and OpenShift. This guide assumes familiarity with these technologies.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#gitops-methods","title":"GitOps Methods","text":"<p>CaaS provides two primary methods for deploying an synchronising infrastructures:</p> <ol> <li> <p>Auto-defined ArgoCD applications: A ArgoCD applicationSet automatically creates applications when resources are defined in the target path.</p> <ol> <li>Target path: <code>&lt;basepath&gt;applicationsets/&lt;environments[].name&gt;/*</code></li> <li>OpenShift Tenant variable: <code>argocd.enable_auto_defined_apps</code></li> </ol> </li> <li> <p>User-defined ArgoCD applications: The user creates its own ArgoCD application definitions. Only recommended to use if more functionality needed than what is provided in the Auto-defined method. The ArgoCD application definition must be defined in the correct target path.</p> <ol> <li>Target path: <code>&lt;basepath&gt;applications/&lt;environments[].name&gt;/*</code></li> <li>OpenShift Tenant variable: <code>argocd.enable_user_defined_apps</code></li> </ol> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#pre-requisites","title":"Pre-requisites","text":"<p>Before you can utilise the GitOps capabilties on OpenShift you need to setup your OpenShift Tenant and/or team correctly. Below are links to both tenant and team quick start guides:</p> <ul> <li>Team Quickstart</li> <li>Tenant Quickstart</li> </ul> <p>Info</p> <p>More information on how to connect ArgoCD to your repository can be found here.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#getting-started","title":"Getting started","text":"<p>With your tenant configuration successfully completed, the next crucial step is establishing the structure in your main Git repository. </p> <p>This section will illustrate both the auto-defined and user-defined methods for managing applications with ArgoCD. We will use the <code>poseidon1_main_repository</code> folder as our example to walk you through the process. Check the example folder here.</p> <p>Consider this folder as your repository blueprint, with the basepath set to an empty string, providing a clear and applicable framework for your GitOps setup. Let\u00b4s get started! </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#auto-defined-argocd-applications","title":"Auto-defined ArgoCD applications","text":"<p>This approach is the recommended approach for working with ArgoCD. It gives an intuitive and easy way of working with multiple ArgoCD applications without implementing a nameingstandard across tenants.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#how-it-works-in-the-background","title":"How it works in the background","text":"<p>The ArgoCD applicationsets looks in the target path <code>poseidon1_main_repo/applicationsets/&lt;environment_name&gt;</code>, so every folder created in this path will create a application in ArgoCD. The name of the application will have the following name  <code>&lt;tenant_name&gt;-&lt;environment_name&gt;-apps-&lt;folder_name&gt;</code>. For instance if we create a folder called <code>applicationsets/dev/demo-kubernetes</code> it will be called <code>poseidon1-dev-apps-demo-kubernetes</code> in ArgoCD. </p> <p>In addition to the environments a base folder is created in the repository to easier share definitions across environments. The structure of the applicationsets will look like this: </p> <pre><code>\u251c\u2500\u2500 applicationsets\n\u2502   \u251c\u2500\u2500 base\n\u2502   \u251c\u2500\u2500 dev\n\u2502   \u2514\u2500\u2500 test\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#deployment-examples","title":"Deployment Examples","text":"<p>Applications can be deployed with kustomization files, helm templates, helm repositories or plain kubernetes YAML definitions. The examples below show different methods for deploying resources to a cluster:</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#1-using-yaml-definitions-of-kubernetes-resources","title":"1) Using YAML definitions of Kubernetes resources","text":"<p>Kubernetes resources can be deployed to the cluster by simpling adding files with YAML definition of Kubernetes resources. Folder <code>applicationsets/dev/ex1-kubernetes-resources</code> illustrates an example of how this is done for a service resource. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#2-kustomization-file","title":"2) Kustomization file","text":"<p>The recommended way of provisioning infrastructure through ArgoCD is using a Kustomization file. A Kustomization file can be used to define multiple Kubernetes resoucres. It will only deploy the Kubernetes resources listed in the kustomization (<code>kustomization.yaml</code>) file like this:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - deployment.yaml\n  - service.yaml\n</code></pre> <p>This will deploy <code>deployment.yaml</code> and <code>service.yaml</code> from the current folder. </p> <p>With Kustomize you can use yaml-files from a common base, and you can patch enviroment spesific values. Here's an example where files from a base (<code>../../base/application</code>) is used and the number of replicas for the Deloyment <code>app-deployment</code> is patched to <code>3</code>:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n- ../../base/application\n\npatches:\n- target:\n    kind: Deployment\n    name: app-deployment\n  patch: |\n    - op: replace\n      path: /spec/replicas\n      value: 3\n</code></pre> <p>Finally, you can leverage Git and its references to extract your base configuration into an external repository, allowing you to version control your configuration by utilizing branches and tags in Git.</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n- https://&lt;git remo url&gt;//base/application?ref={&lt;branch&gt;,&lt;tag&gt;,&lt;commit id&gt;}\n\npatches:\n- target:\n    kind: Deployment\n    name: app-deployment\n  patch: |\n    - op: replace\n      path: /spec/replicas\n      value: 3\n</code></pre> <p>Use Kustomize cli to test your configuration.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#3-helm","title":"3) Helm","text":"<p>Helm is another effective tool for deploying and managing Kubernetes applications. It simplifies the process of defining, installing, and upgrading even the most complex Kubernetes applications. Here we will list different examples for deploying helm:</p> <ol> <li> <p>Local Helm chart: If you want to define a helm chart locally folder <code>applicationsets/dev/ex3-helm-1</code> gives an example for how this can be done. Here a simple service and serviceaccount is defined under the folder <code>templates/</code> and the application is created by reading the <code>values.yaml</code> and <code>Chart.yaml</code> file.</p> </li> <li> <p>External Helm repository: If you want to use an external Helm repository folder <code>applicationsets/dev/ex3-helm-2</code> gives an example of how this can be done through a kustomization file. Kustomize uses a helmCharts field which has the ability to use the helm command line program in a subprocess to inflate a helm chart, generating YAML as part of (or as the entirety of) a kustomize base. </p> <ol> <li>This only works for public repositories. If you want to use a private repository it is recommended to create an ArgoCD application with the user-defined method. </li> </ol> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#user-defined-argocd-applications","title":"User-defined ArgoCD applications","text":"<p>The user-defined method for creating ArgoCD applications is only recommended if the auto-defined methos don\u00b4t meet your applications requirements. The reason for this is that this method has some limitiations:</p> <ul> <li>Developers need to have more in-depth knowledge of ArgoCD</li> <li>It can be difficult to mange many ArgoCD applications in one Tenant</li> <li>A naming standard is required since the ArgoCD applications will have to be deployed in the namespace <code>gitops-developers</code>. This is a common namespace across all Tenants. Talk to your Tenant administrator regarding the naming standard.</li> </ul> <p>When can it be useful to use user-defined ArgoCD applications:</p> <ul> <li>If you need to use a private helm repository</li> <li>If you want more freedom with your ArgoCD application definitions. </li> </ul> <p>The following file <code>poseidon1_main_repo/applications/dev/ex1-helm-app.yml</code> shows an example of how to deploy an application utilising a helm chart.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#further-reading","title":"Further Reading","text":"<p>Now that you understand better how to setup your CD pipeline on Openshift using ArgoCD, it can be useful to better understand how to use the tool. Check out GitOps Best Practices for more information about how to use ArgoCD </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/gitops-setup/#helpful-resources","title":"Helpful Resources:","text":"<ul> <li>HelmChartInflationGenerator</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/multiple-values-files/","title":"Multiple Values Files","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/multiple-values-files/#user-defined-applications","title":"User-Defined Applications","text":"<p>To use multiple values files with the user-defined Argo CD application method, your file structure should look something like the following:</p> <p></p> <p>In your application code you need to specify the path to your common values file and environment specific values file. Below is an example:</p> helm-app.yml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: helm-demo-3-apps-of-apps-test\n  namespace: gitops-developers # REMEMBER TO USE THE APPROPRIATE NAMESPACE HERE\nspec:\n  project: poseidon1\n  sources:\n    # value file repo\n    # when using SSH key authentication the repo-url should be: 'git@github.com:&lt;repo_path&gt;'\n    # when using PAT token authentication the repo-url should be: 'https://github.com/&lt;repo_path&gt;'\n    - repoURL: git@github.com:TeamPoseidonOCP/poseidon1_main_repo.git\n      targetRevision: HEAD\n      ref: valuesfile\n    # helm chart repo\n    - repoURL: https://teamposeidonocp.github.io/helm-test-repo/\n      chart: helm-test\n      targetRevision: 0.0.1\n      helm:\n        releaseName: helm-test\n        valueFiles:\n          - $valuefile/applications/common_values/values_COMMON.yaml\n          - $valuefile/applications/dev/environment_values-helm-app.yaml\n  destination:\n    server: \"https://kubernetes.default.svc\"\n    namespace: poseidon1-dev\n  syncPolicy:\n    automated:\n      selfHeal: true\n      prune: true\n      allowEmpty: true\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/multiple-values-files/#multiple-values-files-private-helm-registry","title":"Multiple Values Files - Private Helm Registry","text":"<p>When using private helm registries, it is possible to utilize multiple values file with the user-defined application method. However, Kustomize does not currently support the use of private registries which limits the use case for auto-defined applicationsets. To see how you can utilize multiple values files with the user-defined method visit: Using a Private Helm Registry</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/","title":"Authentication Methods for ArgoCD with GitHub","text":"<p>For the authentication methods, you must add the GitHub repository URL and the basepath for your applications:</p> <ul> <li><code>argocd.main_git_repository.repourl</code>: Plaintext HTTPS Git URL for repository where your applications are stored </li> <li><code>argocd.main_git_repository.basepath</code>: Basepath is where argoCD will look to find your application definions. ArgoCD will look in your git repository for path: <code>$basepath/$environment/applications/</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#authenticate-with-a-github-app","title":"Authenticate with a GitHub App","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#create-a-github-app-for-argocd","title":"Create a GitHub App for ArgoCD","text":"<p>Follow the steps below to create a GitHub App suited for ArgoCD:</p> <ol> <li> <p>Follow the steps in the Github documentation to register a GitHub App.</p> </li> <li> <p>For connecting to ArgoCD, ensure that your application has at least <code>Read-only</code> permissions to the <code>Contents</code> and <code>metadata</code> of the repository.  This is the minimum requirement. See  ArgoCD documentation for further explanation.</p> </li> <li> <p>Generate a Private key inside the GitHub App. This will later be mapped and encrypted to the tenant variable <code>github_app.private_key</code></p> </li> <li> <p>Go to the tab Install App, install your new app, and choose the repositories you want to include. Finally, press install.</p> </li> <li> <p>From the web URL inside your app extract the <code>githubAppInstallationID</code>. This will later be mapped to the variable <code>github_app.installation_id</code> in the tenant definition. </p> </li> <li> <p>Click on app settings and extract the App ID, which will later be mapped to the tenant variable <code>github_app.id</code></p> </li> </ol> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#using-your-github-app-for-authentication","title":"Using Your GitHub App for Authentication","text":"<p>There are two options availible for authentication for GitHub App in both teams and tenants. One by using External Secrets utilizing a ClusterSecretStore or using Sealed Secrets.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#external-secrets-option-a","title":"External Secrets - Option A","text":"<p>Warning</p> <p>To use external secrets for defining authentication methodes, you need to have set up a ClusterSecretStore in your team to pull down the credentials from your Key Vault provider.</p> <p>When using the External Secrets option you input the non sensitive values in plain-text. for the sensitive values you refrence the name of the secret stored in your Azure key Vault connected with your ClusterSecretStore.</p> <ul> <li><code>url</code> --&gt; <code>repo_url</code> </li> <li><code>githubAppId</code> --&gt; <code>id</code></li> <li><code>githubAppInstallationID</code> --&gt; <code>installation_id</code></li> <li><code>githubAppPrivateKey</code> --&gt; <code>private_key</code> --&gt; Name of secret stored in Azure Key Vault</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#sealed-secrets-option-b","title":"Sealed Secrets - Option B","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#create-a-secret-for-github-app","title":"Create a secret for Github App","text":"<p>Create a secret for your Github App. Fill out the yaml below.</p> SecretDemo secret.yml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;Name of your tenant&gt;\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: &lt;HTTPS url of git repository&gt;\n  githubAppId: &lt;GitHub App Id&gt;\n  githubAppInstallationID: &lt;GitHub Installation Id&gt;\n  githubAppPrivateKey: |\n    -----BEGIN RSA PRIVATE KEY-----\n    ** secret key here **\n    -----END RSA PRIVATE KEY-----\ntype: Opaque\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: poseidon1\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/TeamPoseidon/poseidon1_fake_main_repo.git\n  githubAppId: 124145\n  githubAppInstallationID: 5214652346\n  githubAppPrivateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn\n    NhAAAAAwEAAQAAAYEAucA6uzBvuWiyA4quhSFXHr1ypmvOBHwccLVDSq44OnvJyiDu/Vw3\n    wvoGmXWk15AP+FXH+S+sA+vNbEkUcuj5CLQAzsNMGLtWvAMDzXpkGWME1AkrB9VV6kobww\n    FTxh6tQEU5jTAj8F6ttUjuYlNpZr1NrmlBbI4JjrqoPdDTJql0VUA7LNGPYwe6/17Z6bJx\n    lgnGWueP+GGJBOBPLfB7i3mKZ9xle7YbYocRO9/PMb3pWaemy/4Y22QPoir9zWhzinjDlh\n    ppVqeMr5G0EADv5nktgbuKErQ8V27UYcr6Ffs2AAKw47CFSDyMoLXjGGBGxRBBWQe7naPe\n    X64k9vFP/Eh29cjmRE+JEeJlwEVGoN66vFb3dGWR2aFcSQXri+BmdQRvwr0pQkk57MLqPX\n    gwOaggDEmUAxWa7k5zmdF2D9useHx3MxF6YICGKRtEAR0XJt0UzVgdVy4QDjUnsJZAM5No\n    4nQNb7SLMa6TiEAt385Ot662bOa4z/jva4ez4gkpAAAFkBGCcWgRgnFoAAAAB3NzaC1yc2\n    EAAAGBALnAOrswb7losgOKroUhVx69cqZrzgR8HHC1Q0quODp7ycog7v1cN8L6Bpl1pNeQ\n    D/hVx/kvrAPrzWxJFHLo+Qi0AM7DTBi7VrwDA816ZBljBNQJKwfVVepKG8MBU8YerUBFOY\n    0wI/BerbVI7mJTaWa9Ta5pQWyOCY66qD3Q0yapdFVAOyzRj2MHuv9e2emycZYJxlrnj/hh\n    iQTgTy3we4t5imfcZXu2G2KHETvfzzG96Vmnpsv+GNtkD6Iq/c1oc4p4w5YaaVanjK+RtB\n    AA7+Z5LYG7ihK0PFdu1GHK+hX7NgACsOOwhUg8jKC14xhgRsUQQVkHu52j3l+uJPbxT/xI\n    dvXI5kRPiRHiZcBFRqDeurxW93RlkdmhXEkF64vgZnUEb8K9KUJJOezC6j14MDmoIAxJlA\n    MVmu5Oc5nRdg/brHh8dzMRemCAhikbRAEdFybdFM1YHVcuEA41J7CWQDOTaOJ0DW+0izGu\n    k4hALd/OTreutmzmuM/472uHs+IJKQAAAAMBAAEAAAGAO4UlTKYQptCtegUONwqf5/G8sy\n    cINNewJU1v6pY43kScPHChI/5Qv+FuC+5ui8RV2bVjBq4H6Jm+kVw5eTi909QaDib2U1Z0\n    -----END OPENSSH PRIVATE KEY-----\ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#encrypt-the-secret-with-kubeseal","title":"Encrypt the Secret with Kubeseal","text":"<p>To seal the secret with <code>kubeseal</code>, you can use the following command:</p> <p><pre><code>kubeseal --cert /path/to/pub.cert --scope namespace-wide -f secret.yaml -o yaml &gt; seald_secret.yaml\n</code></pre> Replace <code>/path/to/pub.cert</code> with the path to your public certificate provided by Sopra Steria. <code>secret.yaml</code> is the path to the Secret YAML file you created in the first step. The sealed secret will be outputted to <code>sealed_secret.yaml</code>.</p> sealed_secret.yaml<pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/namespace-wide: \"true\"\n  creationTimestamp: null\n  name: poseidon1\n  namespace: gitops-developers\nspec:\n  encryptedData:\n    githubAppId: AgCfjVQbZOkLwnY4Dpwit0W0LOh90SU1sf40s2hkrqJHOWRJ3fElO51sc6thSX/P9v9AgMnETpxi7KQamrSldwJISDp8XLFtXrnCxLc6C7gfcQL3I6Nld1NjSfMsvNy7Wd1gsyaQmIJV0u3UVAHm4VP/48A1ss+J7yXJoGVnI2ujlBy3n9SX7T82RWbhAt\n    githubAppInstallationID: AgAafCX0Xln0LymsikcxK35f10hLm3XLhu35kmvLfltwJ2odxEP58Iq2Wrjl8qe0/1N2tSWbdhxnrVXZ5YEi48EiAUSlGfZ7zYAg1wIPSN7svJAY1mNzHDbJdRLZtpWGzrhYf5mS8SyjMIktbVnwYE9Mgapzm7kE6OiYeuw3ZLIuvohkcqzwgem9cVVKpf+PLgibaMaar3NDEYWbf6Iq5S5EC3Cv7gBqE8OoPjU7U/ZaTGOV8GAojn982fT4pkeqRzfAk63Swu9v\n    githubAppPrivateKey: AgCLwhDTUapl/m7IsJkRUbdV47QQ9SZjFUkZ3hC2cvNQjmknBuNwJMHF7y6Et0osXXRLvgFduMTflBp80YjW6G/0RmUqHe+rT14ThemBwWnSd57H3krZmY4KtHoACG4bHvqnzs7yt0BGItWul0dqI4Ys0mJVF7jXmtkdRlDx142K3yufAEDQ+cpR+jjZerycWCETL1S78WQJf97w4uPvT7+59K5GPSX59/f+ZMP3tIAzG1hc1x36DHk4Dh+C+uaH1ozForHSzW2luV6/gVtmMMKEh5y8nernWB9jWJo6n/4A4buqhDpT8MEOaZELvPZTnf/tNSNREPOVdsyc3qvxcvqfp3zlv+fgaY9dN/GQY+Vx1DOhoPwkoVk8wk4WPAWMZ9kNY0wbIUJPHfiMFRpjJYIh5qHrJcK1Dzzjt+lyS+ZPFcX9R/Rr4wE8atMB2GYly2AYi5VIc7Kyw5PDYfjlQAdg7I\n    type: AgBnObKmkh/cUKMHUAvWKcrBbcz9mgy7NC9mNF4YDM4RIVTNDRu+1UNa7KTjbIopOSzExq3UgdU75/5RVfV40oCPsO7mItdYTFFx6/zbBfldoZO135rkOddMZL9fPMiB6vvJLMMXNMaCncbLqJI2fj/YB7hXOE4dzupEuWweos+jtKMeYHyY+rVaG3Nfq8RxqDbbOyb/uxgdq5pgcC8j+ekrElIINCC6EdXrtwEtOnSrjd\n    url: AgAu8s4oBkIR0yKQ9jvWI7amKa9C56cHnvb8z19fN6Ctv7M50WCBCbgIRGIq2sU0rmc94EpQ/L3cvgQ1ehzDNz/BbZ8ap18ceJXRydM8NktQ5Kp1ZJBcOmjYWA987d7b1R1xtHPmCK/ZAWpRHInfy6CA9mQeSmWBVhILKD+kj2+9nkjuRgXzOzsQ/IgumlC37Xg5stcm61MXE9OWbajDzcNiIpnotkycIEtDsx/W9/zQXra0t3nQzIwOB9YhcPnCd+EVoqBP5Mt9JBKMTp8b7hso/Up5hVOwdSZnUCG7nLpMkkVKQl6iwGIqpDV/Y58NzD45NdW8kBcFwO1zAj9ZUXlL1lP3s99vSWpNQC1nSo6BdLopOAP3e/AD4v2REyrOHc6qv/\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/namespace-wide: \"true\"\n      creationTimestamp: null\n      labels:\n        argocd.argoproj.io/secret-type: repository\n      name: poseidon1\n      namespace: gitops-developers\n    type: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#fill-out-the-argo-specific-section-in-your-team-or-tenant-definition","title":"Fill out the argo-specific section in your team or tenant definition","text":"<p>Fill out the argo-specific section by copying the following fields in the Sealed secret to the <code>gitops.authentication.sealed_secrets.github_app</code> feature inside your team or tenant definition:</p> <ul> <li><code>url</code> -&gt; <code>repo_url</code></li> <li><code>type</code> -&gt; <code>type</code></li> <li><code>githubAppId</code> -&gt; <code>id</code></li> <li><code>githubAppInstallationID</code> -&gt; <code>installation_id</code></li> <li><code>githubAppPrivateKey</code> -&gt; <code>private_key</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-github-app/#useful-links","title":"Useful Links","text":"<p>See Install Kubeseal for instructions on how to install Kubeseal</p> <p>See GitOps - Authentication (Team) for the feature template for teams</p> <p>See GitOps - Authentication (Tenant) for the feature template for tenants</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/","title":"Authentication Methods for ArgoCD with GitHub","text":"<p>For the authentication methods, you must add the GitHub repository URL and the basepath for your applications:</p> <ul> <li><code>argocd.main_git_repository.repourl</code>: Plaintext HTTPS Git URL for repository where your applications are stored </li> <li><code>argocd.main_git_repository.basepath</code>: Basepath is where argoCD will look to find your application definions. ArgoCD will look in your git repository for path: <code>$basepath/$environment/applications/</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#authenticate-with-personal-access-token-pat","title":"Authenticate with Personal Access Token (PAT)","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#create-a-pat","title":"Create a PAT","text":"<p>PAT's are a token that's personal for you, and is a way to access your github account. This guide help you create a PAT with a minimum access for the intended purpose.</p> <p>Go to your profile settings, and then choose Developer settings: </p> <p></p> <p>Under personal access token choose generate new token: </p> <p>Fill out the required fields and choose a sensible Expiration, select the correct repository and set the correct permissions. You only need read access for content and metadata: </p> <p></p> <p>And then press \"Generate token\". You now have a PAT to use for the next steps.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#using-your-pat-for-authentication","title":"Using Your PAT for Authentication","text":"<p>There are two options availible for authentication for PAT tokens in both teams and tenants. One by using External Secrets utilizing a ClusterSecretStore or using Sealed Secrets.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#external-secrets-option-a","title":"External Secrets - Option A","text":"<p>Warning</p> <p>To use external secrets for defining authentication methodes, you need to have set up a ClusterSecretStore in your team to pull down the credentials from your Key Vault provider.</p> <p>When using the External Secrets option you input the non sensitive values in plain-text. for the sensitive values you refrence the name of the secret stored in your Azure key Vault connected with your ClusterSecretStore.</p> <ul> <li><code>url</code> --&gt; <code>repo_url</code></li> <li><code>username</code> --&gt; <code>username</code></li> <li><code>password</code>--&gt; <code>password</code> --&gt; Name of secret stored in Azure Key Vault</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#sealed-secrets-option-b","title":"Sealed Secrets - Option B","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#create-a-secret-for-pat","title":"Create a secret for PAT","text":"<p>When using the External Secrets option you need to first create a secret for your git repository. Fill out the yaml below. </p> SecretDemo <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;Name of your tenant&gt;\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ntype: Opaque\nstringData:\n  url: &lt;HTTPS url of git repository&gt;\n  type: git\n  password: &lt;Personal access token&gt;\n  username: &lt;Git username associated with personal access token&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: poseidon1\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ntype: Opaque\nstringData:\n  url: https://github.com/TeamPoseidon/poseidon1_fake_main_repo.git\n  type: git\n  password: pat_8VUndBHIJQCWIJUkhHWKWMFDAGAGMAGJ202ZIaT\n  username: team.poseidon\n</code></pre> <p>Note</p> <p>For the git url use the HTTPS url of the git repository.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#encrypt-the-secret-with-kubeseal","title":"Encrypt the Secret with Kubeseal","text":"<p>To seal the secret with <code>kubeseal</code>, you can use the following command:</p> <p><pre><code>kubeseal --cert /path/to/pub.cert --scope namespace-wide -f secret.yaml -o yaml &gt; seald_secret.yaml\n</code></pre> Replace <code>/path/to/pub.cert</code> with the path to your public certificate. <code>secret.yaml</code> is the path to the Secret YAML file you created in the first step. The sealed secret will be outputted to <code>sealed_secret.yaml</code>.</p> sealed_secret.yaml<pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/namespace-wide: \"true\"\n  creationTimestamp: null\n  name: poseidon1\n  namespace: gitops-developers\nspec:\n  encryptedData:\n    password: AgAKW3TQw5l51HOdRbuDjXpJvv+cm6AdE5E95kNn9yr9sifywF6VMSkToJO/YVs9w5NsG3Qd8CXR1dSB+klb3kJe9lIucY1gVML82XOZL+nRpVrDivyKNpxHfQOI0BOOg+wbQlZpzdhYaUUAkNN9vukAWoeb67BmDClMrhdzaVH19Htvc0GkoCNM/2HPbuKM8BTUn7xeUut6PVMmcNCKyb3uae3w26i8ElmQI10OSOMoFjVm37HNj1QnjbEmUmsvS6H0sVKo3/NiYC6T75FsesG/jp/FyNBDyjRlqpdSlJ0YYv15oEwI46ByxDFHyFA\n    type: AgAjVZbAT7lXfEwsRC25vNqceFiDoiCLM/lUx4I7ti1ngfRG50MC8c5v3ZiEaCxTICBqkBJpGKtXcgih8jddhXsldWyKlwVFKP5z43LLv2nqNc5Hb/tKZFCP+OVXa/zeWbVREKm/+dH170t0RlTd9PtUJ947i3I5/uJSlayG+D+zg4XBxj2QfKmKS5hntfE\n    url: AgCTo2lVv2zzReuviCxUlkfSF8BTNYX4iQAlQMP7D52AMpLa4fHJ/Ml+N3vzrmdsjeuOp2Wa51q9mz57tVKmYLHA7UQ7Jr2kwwTBNaiz2ZC7oeI2pLPtYuvhkKdz5vMBQxNzAmSavq7jdFulx9Q5k1BY3WNH3TYiQgwhXwHmIJKQS8gZH0JHU62UIZ7RzSNSxmqoejhurDIuzxdpm+Llm21U+VdHGBY+D7FnonEZ5xe5Hx9WayqYgFE\n    username: AgCrjo7uFTS2TWJlE7z6bi2bU1Z2GHI8ycF9Ktg7ZSHMJUemC5Zo9kBD393ixQkElda+UnkU5vYT5LaGf6KiggI1y0Ww3LOjunc60VXy3C7jDH4qmlydR/J7qALCrxvqoB1aPWVEadFWLyMLhEqWhtqtV+AugZIzFt4BjL5y5jjl9IgvOlINp8/2HbNWqvn2E02wsMn\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/namespace-wide: \"true\"\n      creationTimestamp: null\n      labels:\n        argocd.argoproj.io/secret-type: repository\n      name: poseidon1\n      namespace: gitops-developers\n    type: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#fill-out-the-argo-specific-section-in-your-team-or-tenant-definition","title":"Fill out the argo-specific section in your team or tenant definition","text":"<p>Fill out the argo-specific section by copying the following fields in the Sealed secret to the <code>gitops.authentication.sealed_secrets.pat</code> feature inside your team or tenant definition:</p> <ul> <li><code>url</code> --&gt; <code>repo_url</code></li> <li><code>type</code> --&gt; <code>type</code></li> <li><code>username</code> --&gt; <code>username</code></li> <li><code>password</code>--&gt; <code>password</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-personal-access-token/#useful-links","title":"Useful Links","text":"<p>See Install Kubeseal for instructions on how to install Kubeseal</p> <p>See GitOps - Authentication (Team) for the feature template for teams</p> <p>See GitOps - Authentication (Tenant) for the feature template for tenants</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/","title":"Authentication Methods for ArgoCD with GitHub","text":"<p>For the authentication methods, you must add the GitHub repository URL and the basepath for your applications:</p> <ul> <li><code>argocd.main_git_repository.repourl</code>: SSH Git URL for repository where your applications are stored </li> <li><code>argocd.main_git_repository.basepath</code>: Basepath is where argoCD will look to find your application definions. ArgoCD will look in your git repository for path: <code>$basepath/$environment/applications/</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#authenticate-with-ssh","title":"Authenticate with SSH","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#create-a-ssh-key-pair","title":"Create a SSH key-pair","text":"<ol> <li> <p>On your local computer navigate to the .ssh folder</p> <pre><code>cd  ~/.ssh\n</code></pre> </li> <li> <p>Create the SSH key-pair and give it a name of your choice</p> <pre><code>ssh-keygen -t rsa -f &lt;NAME&gt;\n</code></pre> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#add-ssh-public-key-to-github","title":"Add SSH public key to GitHub","text":"<p>Log in to your GitHub profile that has access to the repository you want access to and follow the steps below:</p> <ol> <li>Go to Settings --&gt; SSH and GPG keys</li> <li>Click on the New SSH key button on the top right corner</li> <li>Add the newly created public key starting with <code>ssh-rsa ...</code> and give it an informative name.</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#using-your-ssh-key-for-authentication","title":"Using Your SSH Key for Authentication","text":"<p>There are two options availible for authentication for SSH keys in both teams and tenants. One by using External Secrets utilizing a ClusterSecretStore or using Sealed Secrets.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#external-secrets-option-a","title":"External Secrets - Option A","text":"<p>Warning</p> <p>To use external secrets for defining authentication methodes, you need to have set up a ClusterSecretStore in your team to pull down the credentials from your Key Vault provider.</p> <p>When using the External Secrets option you input the non sensitive values in plain-text. for the sensitive values you refrence the name of the secret stored in your Azure key Vault connected with your ClusterSecretStore.</p> <ul> <li><code>url</code> --&gt; <code>repo_url</code></li> <li><code>sshPrivateKey</code>--&gt; <code>private_key</code> --&gt; Name of secret stored in Azure Key Vault</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#sealed-secrets-option-b","title":"Sealed Secrets - Option B","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#create-a-secret-for-ssh","title":"Create a secret for SSH","text":"<p>Create a secret for your Github SSH.  Fill out the yaml below.</p> <p>Warning</p> <p>For the git url use the SSH url of the git repository. This should look something like this: git@github.com:TeamPoseidon/poseidon1_fake_main_repo.git</p> SecretDemo <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;Name of your tenant&gt;\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ntype: Opaque\nstringData:\n  url: &lt;SSH url of git repository&gt;\n  type: git\n  sshPrivateKey: |\n    -----BEGIN RSA PRIVATE KEY-----\n    ** secret key here **\n    -----END RSA PRIVATE KEY-----\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: poseidon1\n  namespace: gitops-developers\nlabels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  url: git@github.com:TeamPoseidon/poseidon1_fake_main_repo.git\n  type: git\n  sshPrivateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn\n    NhAAAAAwEAAQAAAYEAucA6uzBvuWiyA4quhSFXHr1ypmvOBHwccLVDSq44OnvJyiDu/Vw3\n    wvoGmXWk15AP+FXH+S+sA+vNbEkUcuj5CLQAzsNMGLtWvAMDzXpkGWME1AkrB9VV6kobww\n    FTxh6tQEU5jTAj8F6ttUjuYlNpZr1NrmlBbI4JjrqoPdDTJql0VUA7LNGPYwe6/17Z6bJx\n    lgnGWueP+GGJBOBPLfB7i3mKZ9xle7YbYocRO9/PMb3pWaemy/4Y22QPoir9zWhzinjDlh\n    ppVqeMr5G0EADv5nktgbuKErQ8V27UYcr6Ffs2AAKw47CFSDyMoLXjGGBGxRBBWQe7naPe\n    X64k9vFP/Eh29cjmRE+JEeJlwEVGoN66vFb3dGWR2aFcSQXri+BmdQRvwr0pQkk57MLqPX\n    gwOaggDEmUAxWa7k5zmdF2D9useHx3MxF6YICGKRtEAR0XJt0UzVgdVy4QDjUnsJZAM5No\n    4nQNb7SLMa6TiEAt385Ot662bOa4z/jva4ez4gkpAAAFkBGCcWgRgnFoAAAAB3NzaC1yc2\n    EAAAGBALnAOrswb7losgOKroUhVx69cqZrzgR8HHC1Q0quODp7ycog7v1cN8L6Bpl1pNeQ\n    D/hVx/kvrAPrzWxJFHLo+Qi0AM7DTBi7VrwDA816ZBljBNQJKwfVVepKG8MBU8YerUBFOY\n    0wI/BerbVI7mJTaWa9Ta5pQWyOCY66qD3Q0yapdFVAOyzRj2MHuv9e2emycZYJxlrnj/hh\n    iQTgTy3we4t5imfcZXu2G2KHETvfzzG96Vmnpsv+GNtkD6Iq/c1oc4p4w5YaaVanjK+RtB\n    AA7+Z5LYG7ihK0PFdu1GHK+hX7NgACsOOwhUg8jKC14xhgRsUQQVkHu52j3l+uJPbxT/xI\n    dvXI5kRPiRHiZcBFRqDeurxW93RlkdmhXEkF64vgZnUEb8K9KUJJOezC6j14MDmoIAxJlA\n    MVmu5Oc5nRdg/brHh8dzMRemCAhikbRAEdFybdFM1YHVcuEA41J7CWQDOTaOJ0DW+0izGu\n    k4hALd/OTreutmzmuM/472uHs+IJKQAAAAMBAAEAAAGAO4UlTKYQptCtegUONwqf5/G8sy\n    cINNewJU1v6pY43kScPHChI/5Qv+FuC+5ui8RV2bVjBq4H6Jm+kVw5eTi909QaDib2U1Z0\n    -----END OPENSSH PRIVATE KEY-----\ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#encrypt-the-secret-with-kubeseal","title":"Encrypt the Secret with Kubeseal","text":"<p>To seal the secret with <code>kubeseal</code>, you can use the following command:</p> <p><pre><code>kubeseal --cert /path/to/pub.cert --scope namespace-wide -f secret.yaml -o yaml &gt; sealed_secret.yaml\n</code></pre> Replace <code>/path/to/pub.cert</code> with the path to your public certificate. <code>secret.yaml</code> is the path to the Secret YAML file you created in the first step. The sealed secret will be outputted to <code>sealed_secret.yaml</code>.</p> sealed_secret.yaml<pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/namespace-wide: \"true\"\n  creationTimestamp: null\n  name: poseidon1\n  namespace: gitops-developers\nspec:\n  encryptedData:\n    sshPrivateKey: AgBokMbk9urpdte9hxrpGQoOma367oA41G3yiLaP8Btc4Aqr4TIcxdKbFjcbZu6W1ytU4MMgyM2m9c7vTPqH/61AnF6pA6BKkNcrViuXziLNWzesDtR8Egj3yNEcqp0la6wHzdmVavYhjcnqhJ9O/2KavLIP/BqEoKx7/vhbf9c8z7J/lgqp5Siq4xadgasgdgaX5dbHpZap+SyCz4GyYpUOhWWYLuoAhCpuY/QzGu0elzOC4BeCmj2bNCLV3lmeHme7skam12n3g0IAU9dnhK+L3lC5BdsmkKWswYuvhLAvEWzF7VKOHqkQEFqlMoOD8f+bBjkaFK3omGf4ir0C4RAq\n    type: AgAV02zTTM7hLij9q2cKRs/cXNnS5EWEMS52vZe2b0E2N2i1twr1SuSNnvXGQB4upV1QO++Viwm150UmT+beWIP2QcC+0nqtfqujSzP670ZZvTVKuu7gOa3b5RVg0UHHK0lv+yUDrydK+UZjHDP2MWVmXsc5YanMknwjseo8Q4ad69adgasgSWAzfBRKlZPh\n    url: AgBjm9LUY7SC3WXcgBtMBXb6CAZ4GV3/x3NEZ1GY6oVHPDSBifoATPh3jKtn5GUBktw1YsqFRREyuO9A1VNyi4DQu/H86vjo4pwo0G/egWkr4HbQKS6VpbUbwq2U2B3Cnp/A9H+giNuQ3Oj7UWnbOWPndpOTXL6oxFETTpnFDH5asgasgOgrTe6YM2XTNuizyDd\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/namespace-wide: \"true\"\n      creationTimestamp: null\n      labels:\n        argocd.argoproj.io/secret-type: repository\n      name: poseidon1\n      namespace: gitops-developers\n    type: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#fill-out-the-argo-specific-section-in-your-team-or-tenant-definition","title":"Fill out the argo-specific section in your team or tenant definition","text":"<p>Fill out the argo-specific section by copying the following fields in the Sealed secret to the <code>gitops.authentication.sealed_secrets.ssh_key</code> feature inside your team or tenant definition:</p> <ul> <li><code>url</code> --&gt; <code>repo_url</code></li> <li><code>type</code> --&gt; <code>type</code></li> <li><code>sshPrivateKey</code> --&gt; <code>private_key</code></li> </ul> <p>Note</p> <p>It is very important that you also change the <code>argocd.main_git_repository.repourl</code> to the SSH url of the git repository</p> <p><code>argocd.main_git_repository.repourl</code> --&gt; <code>git@github.com:TeamPoseidon/poseidon1_fake_main_repo.git</code></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Authentication%20Methods/authenticate-with-ssh/#useful-links","title":"Useful Links","text":"<p>See Install Kubeseal for instructions on how to install Kubeseal</p> <p>See GitOps - Authentication (Team) for the feature template for teams</p> <p>See GitOps - Authentication (Tenant) for the feature template for tenants</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/introduction/","title":"Private Helm Registry for GitOps","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/introduction/#why-private-helm-registeries","title":"Why Private Helm Registeries","text":"<p>As a developer, you may need to use a private Helm chart to efficiently and securely deploy Kubernetes infrastructure. Private Helm registries store charts securely, ensuring version control, access management, and consistency across deployments. On an OpenShift platform, they enable reusable and parameterized templates for managing tenant environments. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/introduction/#how-to-setup-a-private-helm-registry","title":"How to setup a Private Helm Registry","text":"<p>For connecting to a Private Helm Registery you have to do the following:</p> <ol> <li>Azure Configuration - Setup Private container registry and make it accesiable for the GitOps controller (Argo CD)<ol> <li>In this setup we will use a Azure Container Registery (ACR) and show how to configure it and to make it available for Argo CD</li> </ol> </li> <li>OpenShift Configuration - Add the ACR credentials to the OpenShift Tenant (or Team overlay)<ol> <li>The credentials to access the Private helm registery are stored as a sealed secret through Git. The secret is encrypted and only accessible by the developer GitOps controller (Argo CD).</li> </ol> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/","title":"Using a Private Helm Registry","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/#prerequsites","title":"Prerequsites","text":"<ul> <li>Completed the steps in: Azure Configuration</li> <li>Completed the steps in: OpenShift Tenant Configuration</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/#different-methodes-using-a-private-helm-registry-with-argo-cd","title":"Different methodes using a Private Helm Registry with Argo CD","text":"<p>There are different methodes availible to utilize a private helm registry with Argo CD. Below is a list containing the methodes we will cover in this section:</p> <ol> <li>User-Defined Method</li> <li>Auto-Defined Method</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/#user-defined-method","title":"User-Defined Method","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/#example","title":"Example","text":"<p>The user-defined method allows you to define Argo applications in a single file. This file contains all the nessesary information to create your app, such as name of the helm chart, url to the remote helm registry, and the version you want to utilize. In addition to this, you can modify and define specific fields in the helm chart outside of the default values. Below is an example of a user-defined argo application:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: &lt;tenant&gt;-&lt;applikasjon&gt;-&lt;milj\u00f8&gt;\n  namespace: gitops-developers # REMEMBER TO USE THE APPROPRIATE NAMESPACE HERE\nspec:\n  project: &lt;virksomhet&gt;-&lt;tenant&gt;\n  source:\n    chart: &lt;name_private_helm_chart&gt; # CHART NAME\n    repoURL: &lt;url_private_helm_registry&gt; # YOUR ACR URL HERE\n    targetRevision: 1.0.1 # CHART VERSION\n    helm:\n      releaseName: &lt;name_private_helm_chart&gt; # CHART NAME\n  destination:\n    server: \"https://kubernetes.default.svc\"\n    namespace: &lt;tenant&gt;-&lt;milj\u00f8&gt;\n  syncPolicy:\n    automated:\n      selfHeal: true\n      prune: true\n      allowEmpty: true\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/#multiple-values-files","title":"Multiple Values Files","text":"<p>This example, we will do almost the same as in the example above. However, in this example we will utilize multiple values files which allows you to easily define or overwrite the default or common values. </p> <p>Below is an example of how your file structure could look:</p> <p></p> <p>Below is an example of the user-defined argo application:</p> helm-app.yml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: &lt;tenant&gt;-&lt;applikasjon&gt;-&lt;milj\u00f8&gt;\n  namespace: gitops-developers # REMEMBER TO USE THE APPROPRIATE NAMESPACE HERE\nspec:\n  project: &lt;virksomhet&gt;-&lt;tenant&gt; \n  sources:\n    # value file repo\n    - repoURL: &lt;git-repo&gt; # For SSH: git@github.com:&lt;git-repo&gt; --- For GitHub token: https://github.com/&lt;git-repo&gt;\n      targetRevision: HEAD\n      ref: valuesfile\n    # private helm regsitry url\n    - repoURL: &lt;url_private_helm_registry&gt; # YOUR ACR URL HERE\n      targetRevision: 1.0.1 # CHART VERSION\n      chart: &lt;name_private_helm_chart&gt; # CHART NAME\n      helm:\n        releaseName: &lt;name_private_helm_chart&gt; # CHART NAME\n        valueFiles:\n          - $valuesfile/applications/common_values/values_COMMON.yaml       # Default values for your environments\n          - $valuesfile/applications/dev/environment_values-helm-app/values_ENVIRONMENT.yaml  # Values you want to overwrite\n  destination:\n    server: \"https://kubernetes.default.svc\"\n    namespace: &lt;tenant&gt;-&lt;milj\u00f8&gt;\n  syncPolicy:\n    automated:\n      selfHeal: true\n      prune: true\n      allowEmpty: true\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/#auto-defined-method","title":"Auto-Defined Method","text":"<p>We will cover an options that can be used to deploy Private Helm charts with ArgoCD ApplicationSets when choosing the auto-defined method. In our example we will be referencing the Helm chart directly from its Chart.yaml.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-reg-usage/#example_1","title":"Example","text":"<p>This is a Chart.yaml file defining the private Helm registry. To use this, you need the registry name, OCI link, and version. Below is an example:</p> Chart.yaml<pre><code>apiVersion: v2\nname: helm-testing\ndescription: A Helm chart for testing\ndependencies:\n- name: &lt;name_private_helm_chart&gt; # CHART NAME\n  version: 1.0.1 # CHART VERSION\n  repository: oci://&lt;url_private_helm_registry&gt; # OCI link to private helm registry\nversion: 1.0.0\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-registry-azure/","title":"Azure Configuration - Private Helm Registry","text":"<p>The following steps must be done to setup a Private Helm Registery in Azure with authentication for Argo CD:</p> <ol> <li>Configure Azure Container Registry</li> <li>Configure Authentication for Argo CD <ol> <li>Method 1 - Configuring ACR Access Token Authenticatio</li> <li>Method 2 - Configuring App Registration Authentication</li> </ol> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-registry-azure/#step-1-configuring-an-azure-container-registry","title":"Step 1 - Configuring an Azure Container Registry","text":"<p>Configuring Azure Container Registry:</p> <ol> <li>Select Create a resource -&gt; Containers -&gt; Container Registry </li> <li>In the Basics tab, enter values for Resource group, Registry name, Location and SKU type </li> <li>Accept default values for the remaining settings. Then select Review + create. After reviewing the settings, select Create</li> </ol> <p>Note down the ACR Login Server and the ACR Name.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-registry-azure/#step-2-configure-authentication-for-argo-cd","title":"Step 2 - Configure authentication for Argo CD","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-registry-azure/#method-1-configure-acr-access-token","title":"Method 1 - Configure ACR access token:","text":"<ol> <li>In the ACR navigate to Access control Settings -&gt; Access Keys</li> <li>Create a access token</li> <li>Note down the username and access token</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-registry-azure/#method-2-configuring-app-registration-authentication","title":"Method 2 - Configuring App Registration Authentication","text":"<ol> <li>In the Azure portal, select Microsoft Entra ID</li> <li>Select App registrations </li> <li>Select New registration</li> <li>For Supported account types, select Accounts in this organization directory only. Leave the other options as is </li> <li>Select Register</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-registry-azure/#configure-client-secret-for-the-app-registration","title":"Configure Client Secret for the App Registration:","text":"<ol> <li>In the App Registartion navigate to Manage -&gt; Certificates &amp; secrets</li> <li>Select 'New client secret'</li> <li>Choose an appropriate name and expiary date of the client secret </li> <li>Select Add</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/GitOps/Private%20Helm%20Registry/private-helm-registry-azure/#configure-app-registration-permissions-with-arcpull-permission","title":"Configure App Registration permissions  with  ArcPull permission","text":"<ol> <li>In the ACR navigate to Access control (IAM) -&gt; Add -&gt; Add role assignment</li> <li>In the Role tab search for and select the ArcPull role. Then select Members</li> <li>In the Members tab, search for and select the App Registration. Then select Review and assign</li> <li>After reviewing the assignment, select Review and assign again</li> </ol> <p>Note down the Application ID (client ID) and the Application Secret (Client Secret).</p> <p>The next step is to configure a sealed Secret for your OpenShift Tenant for integration with Argo CD. See the OpenShift Tenant configuration for Private Helm Registry</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/External-IP-with-MetalLB/","title":"External IP with MetalLB","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/External-IP-with-MetalLB/#introduction","title":"Introduction","text":"<p>If you need to expose service from Openshift on a unique IP, MetalLB provides the possibility to create services with spesific IP adresses. </p> <p>The networking in and around Openshift is limited and restricted, so for practical use, discuss and plan the changes with Sopra Steria.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/External-IP-with-MetalLB/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Openshift cluster with MetalLB set up</li> <li>An IP address pool provided by Sopra Steria Platform Team</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/External-IP-with-MetalLB/#plan","title":"Plan","text":"<p>When adding a new external IP in Openshift there's some planning needed.</p> <ul> <li>Know where you need to reach your service from, and if it is exposed on to the Internet.</li> <li>Order the required network changes from Sopra Steria.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/External-IP-with-MetalLB/#user-guide","title":"User guide","text":"<p>Use a kubernetes Service object and add:</p> <ul> <li>spec.loadBalancerIP </li> <li>metadata.annotations:   metallb.universe.tf/address-pool:"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/External-IP-with-MetalLB/#example","title":"Example","text":"<p>The addresspool provided from Sopra Steria looks like this:</p> <pre><code>apiVersion: metallb.io/v1beta1\nkind: AddressPool\nmetadata:\n  labels:\n    app.kubernetes.io/instance: metallb\n  name: dmz-allocated-network\n  namespace: metallb-system\nspec:\n  addresses:\n  - 10.0.0.200/29\n  autoAssign: false\n  protocol: layer2\n</code></pre> <p>Create a service that looks like this:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  annotations:\n    metallb.universe.tf/address-pool: dmz-allocated-network\nspec:\n  selector:\n    app: my-app\n  ports:\n    - port: 8080\n      targetPort: 8080\n      protocol: TCP\n  type: LoadBalancer\n  loadBalancerIP: 10.0.0.201\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/How-to-use-EgressIP/","title":"How to use EgressIP","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/How-to-use-EgressIP/#routine","title":"Routine","text":"<p>When using EgressIP there are some infrastructure changes needed, and these changes needs to be ordered by sending a request/change to Sopra Steria.</p> <p>After the changes are in effect a EgressIP can be created. The EgressIP object looks something like this:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: EgressIP\nmetadata:\n  name: egress-&lt;tenant-name&gt;-&lt;app&gt;\nspec:\n  egressIPs:\n  - xxx.xxx.xxx.230\n  podSelector:\n    matchLabels:\n      &lt;foo&gt;: &lt;bar&gt;\n  namespaceSelector:\n    matchLabels:\n      soprasteria/tenant: &lt;tenant-name&gt;\n</code></pre> <p>Creating a EgressIP reqiures knowledge of the IP-range/Subnet and elevated cluster priviledges. State your needs and details in an order to Sopra Steria, and an administrator will process the order. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/How-to-use-EgressIP/#firewall","title":"Firewall","text":"<p>Remember to update firewall according to the needs of you application(s).</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/connect-openshift-application-cross-namespaces/","title":"Connect OpenShift applications cross namespaces","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/connect-openshift-application-cross-namespaces/#introduction","title":"Introduction","text":"<p>This user guide provides a guide for how to connect your OpenShift Application cross namespaces. This can be achieved be creating a network policy that permits incoming traffic from a specific external namespace to certain services within the target namespace. The policy is constructed so that all pods in the external namespace are granted access to the given service. Within the target namespace, the service that can be accessed through this policy has the selector <code>app=label</code>. In essence it's a access control allowing only specific pods from an external namespace to communicate with a given service in the current namespace.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/connect-openshift-application-cross-namespaces/#prerequisites","title":"Prerequisites","text":"<ul> <li>A pod and a service running in the target namespace with target ports set</li> <li>A second external namespace with a running application or pod</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/connect-openshift-application-cross-namespaces/#user-guide","title":"User guide","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/connect-openshift-application-cross-namespaces/#how-to-use","title":"How to use","text":"<ol> <li>Replace <code>&lt;namespace&gt;</code> in the metadata section with the name of the namespace you want apply the policy to.</li> <li>Replace <code>&lt;namespace&gt;</code> in the ingress rule with the namespace that you want to allow inbound traffic from.</li> <li>Replace <code>&lt;Label&gt;</code> in the podSelector section with the label of the pods/service in the current namespace that you want the other namespace to access. E.g <code>app=httpd</code></li> <li>Apply the policy by running:  <pre><code>oc apply -f networkpolicy.yml\n</code></pre></li> <li>Confirm your new networkpolicy by running  <pre><code>oc get networkpolicy\n</code></pre></li> <li>From your external application/pod, try to curl the service in the target namespace on <code>IP:Port</code></li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/connect-openshift-application-cross-namespaces/#example","title":"Example","text":"networkpolicy.yml<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-other-namespace\n  namespace: &lt;namespace&gt; # Target namespace\nspec:\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: &lt;namespace&gt; # External namespace\n  podSelector:\n    matchLabels:\n      app: &lt;Label&gt; # The label your service/pod have that the external namespace can access\n  policyTypes:\n  - Ingress\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/egress-firewall/","title":"Egress Firewall","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/egress-firewall/#introduction","title":"Introduction","text":"<p>The Egress Firewall in OpenShift allows you to control outbound traffic from Pods to external networks at the namespace level. It defines rules to explicitly allow or block egress connections to specific IPs, CIDRs, or ports. This enables more fine-grained firewall configurations, where certain openings can be scoped to specific namespaces rather than being applied cluster-wide.</p> <p>This is particularly useful for securing workloads by limiting what external services they can access (e.g., blocking internet access or allowing only specific APIs).</p> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/egress-firewall/#how-to-use-egress-firewall","title":"How to use Egress Firewall","text":"<p>Egress firewall can be enabled in each tenant by enabling the <code>enable_egress_firewall</code> field under <code>namespace</code> in the tenant chart. By enabling this field an egress firewall for each environment in the tenant gets defined based on the following fields: <code>openshift_dhcp_range_main</code>, <code>openshift_dhcp_range_dr</code>, <code>externalURLs</code>, <code>externalIPs</code>, <code>enable_global_egress_IPs</code>, <code>udp_IPs</code>, <code>udpURLs</code> and <code>allow_to_internet</code>.</p> <p>How to enable egress firewall:</p> enable_firewall.yml<pre><code>namespace:\n  name: \"\"\n  use_egress_firewall: true # If true - Egress Firewall Resource will get created for each namespace in the tenant\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/egress-firewall/#global-egress-firewall-openings","title":"Global Egress Firewall Openings","text":"<p>If there are some firewall openings that persist over multiple tenants  and environments in a cluster, these can be defined as global variables to reduce configuration overhead. These firewall openings are stored in a global variables file and is enabled in a tenant when the 'enable_global_egress_IPs' variable under 'namespace' is set to True. To enable this it is also required that the 'use_egress_firewall' variable is set to True. Below is an example of enabling global egress firewall openings:</p> tenant.yml<pre><code>  namespace:\n    name: \"\"\n    description: \"\"\n    displayName: \"\"\n    use_egress_firewall: true # Enables egress firewall\n    enable_global_egress_IPs: true # If set to true - global egress firewall openings are included in the egress firewall of the environments in the tenant\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/exposing-an-openshift-service/","title":"Exposing an OpenShift service","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/exposing-an-openshift-service/#purpose","title":"Purpose","text":"<p>Expose services running in OpenShift outside the scope of the cluster. For documentation on how to create OpenShift routes, please refer to the OpenShift documentation</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/exposing-an-openshift-service/#exposing-a-service-using-the-wildcard-ingress-certificate","title":"Exposing a service using the wildcard ingress certificate","text":"<p>To expose a service running in OpenShift you can create an object of type Ingress. </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n\u00a0 name: minimal-ingress (1)\n\u00a0 namespace: my-namespace (2)\n\u00a0 annotations:\n\u00a0 \u00a0 nginx.ingress.kubernetes.io/rewrite-target: /\n\u00a0 \u00a0 haproxy.router.openshift.io/ip_whitelist: '0.0.0.0/0' (3)\nspec:\n\u00a0 tls:\u00a0(4)\n\u00a0 - {}\n\u00a0 ingressClassName: openshift-default (5)\n\u00a0 rules:\n\u00a0 - http:\n\u00a0 \u00a0 \u00a0 paths:\n\u00a0 \u00a0 \u00a0 - path: /\n\u00a0 \u00a0 \u00a0 \u00a0 pathType: Prefix\n\u00a0 \u00a0 \u00a0 \u00a0 backend:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 service:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: test (6)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number: 80 (7)\n</code></pre> <ol> <li>Name of your ingress object.</li> <li>Namespace where your ingress object is built.</li> <li>The IP address range allowed you to reach your service. <code>0.0.0.0/0</code>will allow all source IPs.</li> <li>TLS configuration. The configuration in the example will use the wildcard ingress certificate to expose your service. </li> <li>Select the ingress class that should be used to expose your service.</li> <li>Name of your service.</li> <li>Port exposed by your service.</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/exposing-an-openshift-service/#expose-services-on-a-different-ingresscontroller","title":"Expose services on a different ingressController","text":"<p>While exposing services on a specific ingressController you have to set two additional fields as seen below. </p> <pre><code>kind: Ingress\napiVersion: networking.k8s.io/v1 \nmetadata: \n  name: minimal-ingress\n  namespace: my-namespace \n  labels: \n    mydeveloperplatform/exposetointernet: \"true\" (1) \nspec:\n  ingressClassName: openshift-ingress-external (2)\n</code></pre> <ol> <li>The label used to define that your route should be exposed on a specific ingress. Which label should be used for each customer is agreed upon once self-service of exposing services on a specific ingress is approved.</li> <li>The name of the ingress class you are using. Also agreed upon when exposing services on a specific ingress is set up.</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/exposing-an-openshift-service/#requirements-for-all-ingress-objects","title":"Requirements for all ingress objects","text":"<p>All ingress objects are required to specify: <pre><code>...\nmetadata:\n  annotations:\n    haproxy.router.openshift.io/ip_whitelist: \n...\n</code></pre></p> <p>Reason: Developers can expose their services wherever they like, but they need to actively decide which IP range should be allowed to access their service.</p> <pre><code>...\nspec:\n  tls:\n...\n</code></pre> <p>Reason: Services should be exposed using HTTPS, not HTTP. </p> <pre><code>...\nspec:\n  ingressClassName:\n...\n</code></pre> <p>Reason: While the ingresses will default to openshift-default while ingressClassName is not set, it is considered a good practice to actively decide which ingressClass is used.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/introduction/","title":"Network Communication","text":"<p>Network traffic management and communication between OpenShift applications.</p> <p>Official Documentation: OpenShift Networking</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/introduction/#cross-namespace-communication","title":"Cross-Namespace Communication","text":"<p>Applications may need to communicate across namespaces. Use secure communication techniques to connect applications in different namespaces.</p> <p>Guide: Connect OpenShift Applications Cross Namespaces</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Network/introduction/#exposing-services-to-internet","title":"Exposing Services to Internet","text":"<p>Requirements:</p> <ul> <li>Organization approval for internet exposure</li> <li>Specific label for external access</li> <li>Wildcard domain for exposed services</li> <li>OpenShift tenant</li> </ul> <p>Apply the provided label to your ingress or route to enable external access.</p> Example<pre><code>kind: Ingress\napiVersion: networking.k8s.io/v1 (1)\nmetadata: \n  name: mynewroute (2)\n  namespace: mytenant-dev (3)\n  labels: \n    mydeveloperplatform/exposetointernet: \"true\" (4) \nspec:\n  ingressClassName: openshift-ingress-external (5)\n  rules:\n  - host: grafana.external-test.cp.oslo.kommune.no (6) \n    http:\n      paths:\n      - backend:\n          service:\n            name: my-service (7)\n            port:\n              number: 3000 (8)\n        path: /\n        pathType: ImplementationSpecific\n</code></pre> <ol> <li>In the above example ingresses are used. You can also use the api for ingress if you prefer. If you wish to use the ingress API specify <code>apiVersion: route.openshift.io/v1</code> and <code>kind: Route</code>.</li> <li>Name of your ingress object.</li> <li>The namespace where your ingress recides.</li> <li>The label used to define that your route should be exposed on the internet. Which label should be used for each customer is agreed upon once self-service of exposing services on the internet is approved.</li> <li>The name of the ingress class you are using. Should be openshift-ingress-external. </li> <li>The URL of your exposed service.</li> <li>The service you want to expose.</li> <li>The port on which your service is exposed.</li> </ol> <p>Once you have created your route/ingress object you may access your service on \"your-subdomain\".\"Wildcard domain for exposed ingress\".</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/","title":"Observability Documentation","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#available-observability-features","title":"Available Observability Features","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#1-built-in-openshift-observability","title":"1. Built-in OpenShift Observability","text":"<p>OpenShift provides native observability capabilities through the developer console, including: - Real-time performance dashboards - Application metrics and monitoring - Log aggregation and viewing - Alert management</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#2-team-monitoring-stack","title":"2. Team Monitoring Stack","text":"<p>Deploy a dedicated Prometheus-based monitoring stack for your team: - Custom metrics collection - Configurable retention periods - High availability options - Alertmanager integration - Note: When routes are exposed, the metrics from Prometheus are available to anyone with network access to the OpenShift environment.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#3-team-grafana-instances","title":"3. Team Grafana Instances","text":"<p>Access powerful visualization capabilities: - Custom dashboards - Multi-tenant data sources - Role-based access control (configurable) - Integration with team monitoring stacks</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#4-network-security","title":"4. Network Security","text":"<p>Team namespaces are secured using network policies: - Default deny-all network policies (enabled by default) - Automatic allow rules for essential OpenShift services - Intra-namespace communication allowed - Team-to-team communication configurable - Prometheus federation with OpenShift monitoring enabled</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#5-application-monitoring","title":"5. Application Monitoring","text":"<p>Configure monitoring for your specific applications: - Custom Prometheus alerts - Service level indicators (SLIs) - Performance tracking - Health checks and uptime monitoring</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized into the following sections:</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#core-observability-features","title":"Core Observability Features","text":"<ul> <li>Using OpenShift Observability: Native OpenShift monitoring capabilities</li> <li>Configure Grafana and Thanos Query: Setting up advanced visualization and querying</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#network-security","title":"Network Security","text":"<ul> <li>Team Network Policies: Understanding network security in team namespaces</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#application-monitoring","title":"Application Monitoring","text":"<ul> <li>Configuring Monitoring for a Tenant: Custom alerts and monitoring rules for your applications</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#best-practices","title":"Best Practices","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#for-development-teams","title":"For Development Teams","text":"<ul> <li>Start with built-in OpenShift observability before implementing custom solutions</li> <li>Use team monitoring stacks for production environments</li> <li>Configure appropriate retention periods based on your needs</li> <li>Grafana Security: Configure RBAC and authentication for Grafana instances</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#for-production-environments","title":"For Production Environments","text":"<ul> <li>Enable high availability for critical monitoring components</li> <li>Set up comprehensive alerting for system and application health</li> <li>Regular backup and disaster recovery for observability data</li> <li>Monitor resource usage of observability components</li> <li>Configure Grafana Authentication: Set up proper RBAC for Grafana instances</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#security-considerations","title":"Security Considerations","text":"<ul> <li>Network Security: Team namespaces use default deny-all network policies</li> <li>Grafana RBAC: Configure authentication and role-based access for Grafana</li> <li>Prometheus/Thanos Routes: Monitoring stack routes are publicly accessible when enabled</li> <li>No Prometheus Authentication: Prometheus and Thanos lack built-in authentication</li> <li>Secure sensitive metrics and logs through network policies</li> <li>Regular security updates for observability components</li> <li>Monitor access to exposed observability endpoints</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#support-and-troubleshooting","title":"Support and Troubleshooting","text":"<p>If you encounter issues with observability features:</p> <ol> <li>Check Prerequisites: Ensure your tenant has the necessary permissions and configurations</li> <li>Review Documentation: Each feature has detailed setup and troubleshooting guides</li> <li>Contact Support: Reach out to Sopra Steria for tenant-specific issues</li> <li>Community Resources: Leverage OpenShift and Prometheus community documentation</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/Introduction/#next-steps","title":"Next Steps","text":"<p>Begin your observability journey by: 1. Exploring the built-in OpenShift observability features 2. Setting up team monitoring capabilities 3. Configuring application-specific monitoring</p> <p>Remember that effective observability is an iterative process. Start with basic monitoring and gradually add more sophisticated features as your applications and understanding grow.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/","title":"Configure grafana and Thanos Query","text":"<p>This user guide provides a process of configuring and setting up Grafana with Thanos Query using the provided Tenant service account and Grafana datasource template. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/#introduction","title":"Introduction","text":"<p>This Bash script is intended to create a data source for a Grafana instance set up on an OpenShift cluster. The script will check that specific prerequisites are met before running and provide feedback on any issues.</p> <p>The script will check that all prerequisites are met before creating a data source per namespace in the tenant for the Grafana instance. It will retrieve a token for the \"serviceaccount\" that the script uses and then render a template file to new files containing the correct variables for your namespaces in the cluster. Finally, the script will create the datasources in the OpenShift cluster and provide feedback on whether the creation was successful or not. The script will loop through all your namespaces in the provided tenant and set a custom query parameter in the datasource like this <code>customQueryParameter=\"namespace=${namespace}\"</code>.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/#prerequisites-for-the-script","title":"Prerequisites for the script","text":"<ul> <li>In the tenant definition ensure that the <code>enable_tooling</code> is set to <code>true</code> so that the tooling-namespace and service account is deployed to your tenant. </li> <li>In the tenant definition ensure that the <code>deploy_grfana</code>is set to <code>true</code>, so that a Grafana instance is deployed in the tooling-namespace.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/#user-guide","title":"User guide","text":"<ol> <li>In your chosen directory create the files <code>create_datasource.sh</code> and <code>grafana-datasource-template.yml</code> provided at the end of this user guide here.</li> <li>Run the command to give the script execute permission.     <pre><code>sudo chmod +x create_datasource.sh\n</code></pre></li> <li>Run the script with the command <code>./create_datasource.sh &lt;tenant&gt;</code>, where <code>&lt;tenant&gt;</code> is the name of the OpenShift tenant you are using.     <pre><code>./create_datasource.sh &lt;tenant&gt;\n</code></pre></li> <li>If the script returns an error, please follow the instructions provided in the output.</li> <li>If the script runs successfully, you'll see a couple of new files called <code>grafana-datasource-&lt;namespace&gt;.yml</code> in your working directory. You should have a datasource per namespace in your tenant.</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/#login-to-grafana-dashboard","title":"Login to Grafana dashboard","text":"<ol> <li> <p>Make sure you are in the correct OpenShift project. <pre><code>oc project &lt;namespace&gt;-tooling\n</code></pre></p> </li> <li> <p>Run the following command to get the route to Grafana. <pre><code>oc get route\n</code></pre></p> </li> <li>Copy the route from the HOST/PORT field and paste the route into your desired web browser. Remember to use <code>https://</code>. It should look similar to the example below. <pre><code>https://grafana-instance-service-&lt;NAMESPACE&gt;-tooling.apps.&lt;CLUSTER&gt;.cp.&lt;COMPANY.DOMAIN&gt;\n</code></pre></li> <li>Login using Username admin and extract the password using the following command: <pre><code>oc extract secret/grafana-instance-admin-credentials --to=-\n</code></pre></li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/#create-a-simple-dashboard-that-query-multiple-datasources","title":"Create a simple dashboard that query multiple datasources","text":"<ol> <li>Inside Grafana choose Dashboards</li> <li>Click on New and then Add a new panel</li> <li>In the Query window, select Mixed as the data source. </li> <li>In query A selecet the first data sources you will like to query </li> <li>Write a query and clik on Run queries. A simple query to start with could be: <pre><code>pod:container_cpu_usage:sum{}\n</code></pre></li> <li>To query another datasource click on + Query and repeat steps 4. and 5. </li> <li>To apply these queries to your new dashboard click on Apply in the top right corner.</li> <li>The end results should look similar to this, if you have any pods running that are consuming CPU. </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/#create_datasourcesh","title":"create_datasource.sh","text":"<pre><code>#! /bin/bash\n\n#Instructions of use\n#\n# 1. Open a terminal and navigate to the directory where the script and \"grafana-datasource-template.yml\" files are located.\n# 2. Run the command \"chmod +x create_datasource.sh\" to give the script execute permission.\n#       sudo chmod +x create_datasource.sh\n# 3. Run the script with the command \"./create_datasource.sh &lt;tenant&gt;\" (where \"&lt;tenant&gt;\" is the name of the OpenShift tenant you are using).\n#       ./create_datasource.sh &lt;tenant&gt;\n# 4. Follow the feedback from the script to see if any issues arise.\n# 5. If the script runs sucsessfully you'll see some new files called \"_grafana-datasource-&lt;namespace&gt;.yml_\" in your working directory.\"\n#\n#\n\n######### SCRIPT PREREQUISITES #########\necho \"Verify script prerequisites\"\n\nif [ -z \"$1\" ]\n  then\n    echo \"No argument supplied. Example usage: ./create_datasource.sh MYTENANTNAME\"\n    exit\nfi\n\noc whoami &gt; /dev/null 2&gt;/dev/null\nexit_code=$?\nif [ $exit_code -ne 0 ]\nthen\n  echo \"Please log in to cluster before running this script.\"\n  exit\nfi\n\noc get namespace $1-tooling &gt; /dev/null 2&gt;/dev/null\nexit_code=$?\nif [ $exit_code -ne 0 ]\nthen\n  echo \"Make sure your OpenShift tenant is set up with namespace.enable_tooling: true. Contact Sopra Steria to fix this.\"\n  exit\nfi\n\nls grafana-datasource-template.yml &gt; /dev/null 2&gt;/dev/null\nexit_code=$?\nif [ $exit_code -ne 0 ]\nthen\n  echo \"Could not find file: grafana-datasource-template.yml in folder $(pwd). Make sure you follow the user guide from Sopra Steria when setting up the Grafana datasource.\"\n  exit\nfi\n\noc get sa $1-tooling -n $1-tooling &gt; /dev/null 2&gt;/dev/null\nexit_code=$?\nif [ $exit_code -ne 0 ]\nthen\n  echo \"Make sure serviceaccount exists. Contact Sopra Steria to fix this.\"\n  exit\nfi\n\noc get grafana grafana-instance -n $1-tooling &gt; /dev/null 2&gt;/dev/null\nexit_code=$?\nif [ $exit_code -ne 0 ]\nthen\n  echo \"WARNING: Could not find grafana grafana-instance. Make sure grafana is running to make use of datasource.\"\nfi\n\necho \"Prerequisites verified - OK\"\n\n\n######### START SCRIPT #########\nexport dashboard_name=$1-tooling\nexport tooling_namespace=$1-tooling\nexport datasource_query=$1-tooling\nexport datasource_namespaces=$(oc get project -l soprasteria/tenant=$1 --no-headers | cut -f1 -d \" \")\n\nfor ns in $datasource_namespaces; do \n  export namespace=$ns    \n\n  echo \"Fetching secret for serviceaccount $1-tooling\"\n  export mysecret=$(oc get secret -n $1-tooling | grep $1-tooling-token | cut -f1 -d ' ')\n  # Throw error if mysecret is not correctly found\n  echo \"Secret fetched\"\n\n  echo \"Extracting token for serviceaccount $1-tooling\"\n  export mytoken=$(oc get secret $mysecret -n $1-tooling -o jsonpath={.data.token} | base64 -d)\n  # Throw error if token is not extracted\n  echo \"Token extracted\"\n\n  echo \"Rendering template grafana-datasource-template.yml to file grafana-datasoruce-$namespace.yml\"\n  cat grafana-datasource-template.yml | envsubst &gt; grafana-datasource-$namespace.yml\n  echo \"Template rendered to grafana-datasource.yml\"\n\n  echo \"Creating datasource in OpenShift-cluster in namespace $1-tooling\"\n  oc apply -f grafana-datasource-$namespace.yml\n  echo \"Created datasource $namespace-datasource in namespace $1-tooling\"\ndone\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configure-grafana-and-thanos-query/#grafana-datasource-templateyml","title":"grafana-datasource-template.yml","text":"grafana-datasource-template.yml<pre><code>apiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDatasource\nmetadata:\n  name: \"${namespace}-datasource\" \n  namespace: \"${tooling_namespace}\" \nspec:\n  instanceSelector:\n    matchLabels:\n      dashboards: \"${dashboard_name}\"\n  datasource:\n    name: prometheus-${namespace}\n    type: prometheus\n    access: proxy\n    basicAuth: true\n    url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9092'\n    isDefault: true\n    jsonData:\n      \"tlsSkipVerify\": true\n      \"timeInterval\": \"5s\"\n      \"httpHeaderName1\": \"Authorization\"\n      customQueryParameters: \"namespace=${namespace}\"\n    secureJsonData:\n      httpHeaderValue1: 'Bearer ${mytoken}' \n    editable: true\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/","title":"Configuring Monitoring for A Tenant","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#prerequisites","title":"Prerequisites","text":"<ul> <li>An OpenShift tenant created by Sopra Steria</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#introduction","title":"Introduction","text":"<p>Prometheus alerts help ensure the health and reliability of your systems by allowing you to define rules and respond quickly to issues. By understanding the alerting workflow and using Alertmanager for notifications, you can build an effective alerting setup in OpenShift.</p> <p>This user guide will provide you with further insights and detailed instructions on managing and creating alerts in Openshift.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#overview-of-alerts","title":"Overview of Alerts","text":"<p>We have created a set of alerts tailored to enhance your monitoring and alerting capabilities within your Prometheus environment. These alerts are designed to help you proactively detect issues. Below, we provide an overview of the various alert groups we've developed for you to use. Each group is carefully crafted to address specific aspects of your infrastructure and applications, empowering you to take action swiftly when needed. Explore the following alert groups to gain deeper insights into your monitoring and make informed decisions.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#job-cronjob-alerts","title":"Job &amp; CronJob Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes jobs and cron jobs. These alerts help you keep track of job and cron job executions and potential issues.</li> <li><code>KubernetesJobFailed</code>: This alert triggers when a Kubernetes job fails to complete.</li> <li><code>KubernetesCronjobSuspended</code>: This alert triggers when a Kubernetes CronJob is suspended.</li> <li><code>KubernetesCronjobTooLong</code>: This alert triggers when a Kubernetes CronJob takes too long to complete.</li> <li><code>KubernetesJobSlowCompletion</code>: This alert triggers when Kubernetes Job has not completed all expected tasks (completions) within 12 hours.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#storage-alerts","title":"Storage Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes storage and persistent volume claims. These alerts help you maintain storage reliability.</li> <li><code>KubernetesPersistentvolumeclaimPending</code>: This alert triggers when a PersistentVolumeClaim is in a pending state.</li> <li><code>KubernetesVolumeOutOfDiskSpace</code>: This alert triggers when a volume is almost full.</li> <li><code>KubernetesVolumeFullInFourDays</code>: This alert triggers when a volume is expected to fill up within four days.</li> <li><code>KubernetesPersistentvolumeError</code>: This alert triggers when a Persistent Volume is in a bad state.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#replicasets-alerts","title":"ReplicaSets Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes ReplicaSets. These alerts help you maintain the desired number of replicas.</li> <li><code>KubernetesReplicasetReplicasMismatch</code>: This alert triggers when a Kubernetes ReplicaSet's desired replicas do not match the ready replicas.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#deployments-alerts","title":"Deployments Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes Deployments. These alerts help you ensure that your deployments are running smoothly.</li> <li><code>KubernetesDeploymentReplicasMismatch</code>: This alert triggers when a Kubernetes Deployment's desired replicas do not match the available replicas.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#statefulsets-alerts","title":"StatefulSets Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes StatefulSets. These alerts help you ensure the correct behavior of your StatefulSets.</li> <li><code>KubernetesStatefulsetReplicasMismatch</code>: This alert triggers when a Kubernetes StatefulSet's ready replicas do not match the total replicas.</li> <li><code>KubernetesStatefulsetGenerationMismatch</code>: This alert triggers when a Kubernetes StatefulSet's observed generation does not match its metadata generation for more than 10 minutes.</li> <li><code>KubernetesStatefulsetUpdateNotRolledOut</code>: this alert trigger when a Kubernetes StatefulSet update has not been fully rolled out for more than 10 minutes.</li> <li><code>KubernetesStatefulsetDown</code>: This alert triggers when a Kubernetes StatefulSet goes down.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#resource-quota-alerts","title":"Resource Quota Alerts","text":"<ul> <li>This group of alerts focuses on monitoring resource quota usage within namespaces. These alerts help ensure that resource limits are respected to maintain the stability of the cluster.</li> <li><code>MemoryUsageExceeded</code>: This alert triggers when memory usage in a namespace exceeds 95% of the defined hard limit for 15 minutes.</li> <li><code>CpuUsageExceeded</code>: This alert triggers when CPU usage in a namespace exceeds 95% of the defined hard limit for 15 minutes.</li> <li><code>MemoryRequestsExceeded</code>: This alert triggers when the actual memory usage in the namespace exceeds 95% of the defined hard requests for 15 minutes.</li> <li><code>CpuRequestsExceeded</code>: This alert triggers when the actual CPU usage in the namespace exceeds 95% of the defined hard requests for 15 minutes.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#pod-alerts","title":"Pod Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes containers. These alerts help you stay informed about container-related issues that may affect your applications.</li> <li><code>KubernetesContainerOomKiller</code>: This alert triggers when a Kubernetes container is OOM-killed.</li> <li><code>KubernetesPodNotHealthy</code>: This alert triggers when a pod is not healthy.</li> <li><code>KubernetesPodCrashLooping</code>: This alert triggers when a pod is in a crash-loop.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#hpa-alerts","title":"HPA Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes workloads, including StatefulSets, Horizontal Pod Autoscalers (HPAs), and pods. These alerts help you maintain the health and performance of your workloads.</li> <li><code>KubernetesHpaScaleInability</code>: This alert triggers when an HPA is unable to scale.</li> <li><code>KubernetesHpaMetricsUnavailability</code>: This alert triggers when an HPA is unable to collect metrics.</li> <li><code>KubernetesHpaScaleMaximum</code>: This alert triggers when an HPA reaches its maximum scaling limit.</li> <li><code>KubernetesHpaUnderutilized</code>: This alert triggers when an HPA is underutilized.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#how-to-disable-group-alarms","title":"How to disable group alarms","text":"<p>To disable a group alert you need to set a group alert to false in your tenant configuration. </p> <pre><code>monitoring:\n  jobCronJobAlertsEnabled: false # Set to false to disable the JobCronJobAlerts group\n  storageAlertsEnabled: true\n  replicasSetsAlertsEnabled: true\n  deploymentsAlertsEnabled: true\n  statefulSetsAlertsEnabled: true\n  resourceQuotaAlertsEnabled: true\n  podAlertsEnabled: true\n  hpaAlertsEnabled: true\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#configure-your-own-alerts","title":"Configure your own alerts","text":"<p>If you want to make your own alerts you can do this by following the steps below. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#configure-alerting-rules-for-user-defined-projects","title":"Configure alerting rules for user-defined projects","text":"<p>Before you start configuring alerts for your project, please familiarize yourself with the best practices for alerting as described in best-practices. For a guide on how to create Prometheus rules using PromQL, please refer to the prometheus documentation.</p> <p>Note</p> <p>You will not be allowed to create resources directly in openshift-user-workload-monitoring, as this namespace is used across independent teams.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#steps","title":"Steps","text":"<ol> <li> <p>Create a YAML definition for the alerting rules you want to create providing the namespace of your choice.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: example-alert\n  namespace: &lt;Tenant&gt;\nspec:\n  groups:\n  - name: example\n    rules:\n    - alert: VersionAlert\n      expr: version{job=\"prometheus-example-app\"} == 0\n</code></pre> </li> <li> <p>Apply the alerting rule to the cluster. This should be done using GitOps to ensure tracability and transparency of configuration, as well as securing an automated setup if the tenant would have to be rebuilt at some point.</p> </li> <li> <p>Verify that the alerting rules have been created by navigating to Observe --&gt; Alerts for your project in the Developer view of the OpenShift console, as seen below. </p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#sending-notifications-to-external-systems","title":"Sending notifications to external systems","text":"<p>You have access to configuring your Alertmanager configuration specific to your project. As for the alerting rules, all objects are project-scoped, so configure all your projects with the desired notification rules. </p> <p>Warning</p> <p>Never store secrets in git. In the AlertmanagerConfig object below, you should reference secrets created using a secure secret handling system like sealed-secrets or external secrets. You should not type in authentication tokens or passwords directly in the AlertmanagerConfig object.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#steps_1","title":"Steps","text":"<ol> <li> <p>Create a YAML definition for the Alertmanager configuration you want to create. An example is found below:</p> <pre><code>apiVersion: monitoring.coreos.com/v1beta1\nkind: AlertmanagerConfig\nmetadata:\n  name: example-routing\n  namespace: &lt;Tenant&gt;\nspec:\n  route:\n    receiver: default\n    groupBy: [job]\n  receivers:\n  - name: default\n    webhookConfigs:\n    - url: https://example.org/post\n</code></pre> </li> <li> <p>Apply the alerting rule to the cluster. This should be done using GitOps to ensure traceability and transparency of configuration, as well as securing an automated setup if the tenant has to be rebuilt at some point.</p> </li> <li>Verify that notifications are sent to the target when alerts are created. A routine for this will be documented shortly.</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#alert-code-examples","title":"Alert code examples","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#job-cronjob-alerts_1","title":"Job &amp; CronJob Alerts","text":"KubernetesJobFailedKubernetesCronjobSuspendedKubernetesCronjobTooLongKubernetesJobSlowCompletion <pre><code>alert: KubernetesJobFailed\nexpr: kube_job_status_failed &gt; 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes Job failed (instance {{`{{ $labels.instance }})`}}'\n  description: \"Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesCronjobSuspended\nexpr: kube_cronjob_spec_suspend != 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes CronJob suspended (instance {{`{{ $labels.instance }}`}})'\n  description: \"CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is suspended\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesCronjobTooLong\nexpr: time() - kube_cronjob_next_schedule_time &gt; 3600\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes CronJob too long (instance {{`{{ $labels.instance}}`}})'\n  description: \"CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is taking more than 1h to complete.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{     $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesJobSlowCompletion\nexpr: kube_job_spec_completions - kube_job_status_succeeded - kube_job_status_failed &gt; 0\nfor: 12h\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: \"Kubernetes Job failed (instance {{`{{ $labels.instance }}`}})\"\n  description: \"Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete\\n  VALUE = {{`{{ $value }}`}}\\n  LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#storage-alerts_1","title":"Storage Alerts","text":"KubernetesPersistentvolumeclaimPendingKubernetesVolumeOutOfDiskSpaceKubernetesVolumeFullInFourDaysKubernetesPersistentvolumeError <pre><code>alert: KubernetesPersistentvolumeclaimPending\nexpr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes PersistentVolumeClaim pending (instance {{`{{ $labels.instance }}`}})'\n  description: \"PersistentVolumeClaim {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}} is pending\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesVolumeOutOfDiskSpace\nexpr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) * 100 &lt; 10\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes Volume out of disk space (instance {{`{{ $labels.instance }}`}})'\n  description: \"Volume is almost full (&lt; 10% left)\\n VALUE = {{`{{  $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesVolumeFullInFourDays\nexpr: predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) &lt; 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes Volume full in four days (instance {{`{{ $labels.instance }})`}}'\n  description: \"Volume under {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}} is expected to fill up within four days. Currently {{`{{ $value humanize }}`}}% is available.\\n VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesPersistentvolumeError\nexpr: kube_persistentvolume_status_phase{phase=~\"Failed|Pending\", job=\"kube-state-metrics\"} &gt; 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes PersistentVolume error (instance {{`{{ $labels.instance }})`}}'\n  description: \"Persistent volume {{`{{ $labels.persistentvolume }}`}} is in a bad state\\n VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#replicasets-alerts_1","title":"ReplicaSets Alerts","text":"KubernetesReplicasetReplicasMismatch <pre><code>alert: KubernetesReplicasetReplicasMismatch\nexpr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes ReplicaSet replicas mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"ReplicaSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.replicaset }}`}} replicas mismatch\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#deployments-alerts_1","title":"Deployments Alerts","text":"KubernetesDeploymentReplicasMismatch <pre><code>alert: KubernetesDeploymentReplicasMismatch\nexpr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes Deployment replicas mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} replicas mismatch\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#statefulsets-alerts_1","title":"StatefulSets Alerts","text":"KubernetesStatefulsetReplicasMismatchKubernetesStatefulsetGenerationMismatchKubernetesStatefulsetUpdateNotRolledOutKubernetesStatefulsetDown <pre><code>alert: KubernetesStatefulsetReplicasMismatch\nexpr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes StatefulSet replicas mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet does not match the expected number of replicas.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesStatefulsetGenerationMismatch\nexpr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes StatefulSet generation mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} has failed but has not been rolled back.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesStatefulsetUpdateNotRolledOut\nexpr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas !=     kube_statefulset_status_replicas_updated)\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes StatefulSet update not rolled out (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} update has not been rolled out.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesStatefulsetDown\nexpr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready &gt; 0\nfor: 1m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes StatefulSet down (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} went down\\n  VALUE = {{`{{ $value }}`}}\\n  LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#resource-quota-alerts_1","title":"Resource Quota Alerts","text":"MemoryUsageExceededCpuUsageExceededMemoryRequestsExceededCpuRequestsExceeded <pre><code>alert: MemoryUsageExceeded\nexpr: (sum by (name)(openshift_clusterresourcequota_usage{type=\"used\", resource=\"limits.memory\", name=\"{{ $.Values.namespace.name }}\"}) / sum by (name)(openshift_clusterresourcequota_usage{type=\"hard\", resource=\"limits.memory\", name=\"{{ $.Values.namespace.name }}\"})) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: Memory usage for {{ $.Values.namespace.name }} exceeded 95%\n  description: Memory usage for {{ $.Values.namespace.name }} has reached or exceeded 80% of its hard limit.\n</code></pre> <pre><code>alert: CpuUsageExceeded\nexpr: (sum by (name)(openshift_clusterresourcequota_usage{type=\"used\", resource=\"limits.cpu\", name=\"{{$.Values.namespace.name }}\"}) / sum by (name)(openshift_clusterresourcequota_usage{type=\"hard\", resource=\"limits.cpu\", name=\"{{ $.Values.namespace.name }}\"})) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: ClusterResourceQuota CPU Usage Exceeded for instance = {{`{{ $labels.name }}`}}\n  description: Resource {{ $.Values.namespace.name }} is using more than 95% of its hard limit for CPU. VALUE = {{`{{ $value }}`}}, LABELS = {{`{{ $labels.name }}`}}\n</code></pre> <pre><code>alert: MemoryRequestsExceeded\nexpr: sum(sum(container_memory_working_set_bytes{job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", cluster=\"\", namespace=~\"{{ $.Values.namespace.name }}.*\", container!=\"\", image!=\"\"}) by (pod))/sum(openshift_clusterresourcequota_usage{resource=\"requests.memory\", type=\"hard\",name=\"{{ $.Values.namespace.name }}\"}) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: Memory requests for {{ $.Values.namespace.name }} exceeded 95%\n  description: Memory requests for {{ $.Values.namespace.name }} has reached or exceeded 95% of its hard limit.\n</code></pre> <pre><code>alert: CpuRequestsExceeded\nexpr: sum(sum by (namespace)(avg_over_time(pod:container_cpu_usage:sum{namespace=~\"{{ $.Values.namespace.name }}.*\"}[1h])))/sum(openshift_clusterresourcequota_usage{resource=\"requests.cpu\", type=\"hard\",name=\"{{ $.Values.namespace.name }}\"}) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: ClusterResourceQuota CPU Requests Exceeded for instance = {{`{{ $labels.name }}`}}\n  description: Resource {{ $.Values.namespace.name }} is requesting more than 95% of its hard limit for CPU. VALUE = {{`{{ $value }}`}}, LABELS = {{`{{ $labels.name }}`}}\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#pod-alerts_1","title":"Pod Alerts","text":"KubernetesContainerOomKillerKubernetesPodNotHealthyKubernetesPodCrashLooping <pre><code>alert: KubernetesContainerOomKiller\nexpr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m &gt;= 1) and ignoring(reason) min_over_time    (kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: \"Kubernetes Container oom killer (instance {{`{{ $labels.instance }})`}}\"\n  description: \"Container {{`{{ $labels.container }}`}} in pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been OOMKilled {{`{{ $value }}`}} times in the     last 10 minutes.\\n VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesPodNotHealthy\nexpr: sum by (namespace, pod)(kube_pod_status_phase{phase=~\"Pending|Unknown|Failed\"}) &gt; 0\nfor: 15m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes Pod not healthy (instance {{`{{ $labels.instance }})`}}'\n  description: \"Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-running state for longer than 15 minutes.\\n  VALUE = {{`{{ $value }}`}}\\n  LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesPodCrashLooping\nexpr: increase(kube_pod_container_status_restarts_total[1m]) &gt; 3\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes pod crash looping (instance {{`{{ $labels.instance }})`}}'\n  description: \"Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is crash looping\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/configuring-monitoring-for-a-tenant/#hpa-alerts_1","title":"HPA Alerts","text":"KubernetesHpaScaleInabilityKubernetesHpaMetricsUnavailabilityKubernetesHpaScaleMaximumKubernetesHpaUnderutilized <pre><code>alert: KubernetesHpaScaleInability\nexpr: kube_horizontalpodautoscaler_status_condition{status=\"false\", condition=\"AbleToScale\"} == 1\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes HPA scale inability (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{$labels.horizontalpodautoscaler }}`}} is unable to scale\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>- alert: KubernetesHpaMetricsUnavailability\nexpr: kube_horizontalpodautoscaler_status_condition{status=\"false\", condition=\"ScalingActive\"} == 1\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes HPA metrics unavailability (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} is unable to collect metrics\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesHpaScaleMaximum\nexpr: kube_horizontalpodautoscaler_status_desired_replicas &gt;= kube_horizontalpodautoscaler_spec_max_replicas\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: info\nannotations:\n  summary: 'Kubernetes HPA scale maximum (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} has hit the maximum number of desired pods\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesHpaUnderutilized\nexpr: max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) &gt; 3\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: info\nannotations:\n  summary: 'Kubernetes HPA underutilized (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} has hit the maximum number of desired pods\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/","title":"Slack Alert Integration with OpenShift Alerts","text":"<p>This user guide describes how to set up a Slack integration for OpenShift alerts and it also provides a design overview of the process. Follow the steps below to set up a Slack app and how to configure your tenant order to start receiving alerts to your desired Slack channel. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#design-overview","title":"Design overview","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#alertmanagerconfig-for-slack","title":"alertmanagerConfig for Slack","text":"<p>This diagram represents a monitoring setup for an OpenShift environment using Prometheus and Slack for notifications. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have a Slack channel ready for receiving alerts.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#creating-a-slack-webhook-via-a-slack-app","title":"Creating a Slack Webhook via a Slack App","text":"<p>To send alerts to a Slack channel, you first need to create a Slack app and configure a webhook. Follow these steps:</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#1-create-a-new-slack-app","title":"1. Create a New Slack App:","text":"<ul> <li>Go to the Slack API page and click on \"Create New App\".</li> <li>Choose \"From scratch\", enter your App Name, and select the Slack Workspace where the app will be installed.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#2-activate-incoming-webhooks","title":"2. Activate Incoming Webhooks:","text":"<ul> <li>In the \"Features\" section on the sidebar, click on \"Incoming Webhooks\".</li> <li>Toggle the switch to activate incoming webhooks.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#3-create-a-webhook-url-for-your-slack-channel","title":"3. Create a Webhook URL for Your Slack Channel:","text":"<ul> <li>Click on \"Add New Webhook to Workspace\".</li> <li>Choose the channel where you want to receive alerts and click \"Allow\".</li> <li>Slack will generate a Webhook URL; this URL will be used in your alerting configuration.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#4-secure-your-webhook-url","title":"4. Secure Your Webhook URL:","text":"<ul> <li>You then have to encrypt your Webhook URL using SealedSecrets for Kubernetes. Follow the instructions provided below.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#encrypt-secret-with-kubeseal","title":"Encrypt secret with kubeseal","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#1-base64-encode-webhook","title":"1. Base64 encode webhook","text":"<ul> <li>The webhook-URL you have generated from your Slack app needs to be stored in a secret, which you apply to your desired namespace. The first step is to base64 encode the webhook URL:</li> </ul> <pre><code>echo -n &lt;webhook-url&gt; | base64\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#2-encrypt-the-secret-with-kubeseal","title":"2. Encrypt the Secret with Kubeseal","text":"<ul> <li>Paste the encoded URL in the secret and change the name of the secret. This secret name is not important as we only need the encrypted url.</li> </ul> secret.ymlDemo <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret name&gt; # not important as you only need the encrypted url\ntype: Opaque\ndata:\n  url: &lt;base64 encoded webhook&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: webhook-secret\ntype: Opaque\ndata:\n  url: d3d3Lm15c2VjcmV0d2ViaG9vay5jb20K\n</code></pre> <p>To seal the secret with <code>kubeseal</code>, you can use the following command:</p> <pre><code>kubeseal --cert /path/to/pub.cert --scope cluster-wide -f secret.yaml -o yaml &gt; sealed_secret.yaml\n</code></pre> <p>Replace <code>/path/to/pub.cert</code> with the path to your public certificate. <code>secret.yaml</code> is the path to the Secret YAML file you created in the first step. The sealed secret will be outputted to <code>sealed_secret.yaml</code>.</p> <p>Read more about encrypting a secret with kubeseal from this guide</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#helm-chart-configuration-for-slack-alerts","title":"Helm Chart Configuration for Slack Alerts","text":"<p>After setting up your Slack webhook, you need to configure your Helm chart to use this webhook for alerting.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Description <code>slack_alert_integration.enable</code> Set to <code>True</code> to enable Slack alert integration. <code>slack_alert_integration.alert_severity</code> Define the severity of alerts to be sent (e.g., <code>critical</code>, <code>warning</code>, <code>info</code>). For multiple severities, separate them with a pipe (<code>\\|</code>). <code>slack_alert_integration.webhook_secret.encrypted_webhookURL</code> Encrypted Slack webhook URL."},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#1-update-your-tenant-definition-eg-valuesyml","title":"1. Update your tenant definition, e.g <code>values.yml</code>:","text":"<p>Define your Slack alerting configurations in for your Helm chart.</p> values.ymlDemo <pre><code>slack_alert_integration:\n  enable: true\n  alert_severity: critical  # Can be one or multiple from: critical|warning|info\n  webhook_secret:\n    encrypted_webhookURL: \"&lt;your-encrypted-webhook-url&gt;\"\n</code></pre> <pre><code>slack_alert_integration:\n  enable: true\n  alert_severity: critical|warning|info\n  webhook_secret:\n    encrypted_webhookURL: Mohlnf5T512UsdGSDG$W$F#VE6WLDzYo8hTe/ssPKlbZFUypY1rD74NpTN0Arrw4lP48Wxwln7RkPbkhMONFI5lHuzxSuu7iaN8CeUYaxMD6o1V+xX2Rgy56AjNvhevSqrdBU40qU8TRS4LfH/VZ4ueq4kDzhCjh4POXLpCwwG4tod420COFtQY6vxe8VdbDxUbxoP8b3/Q+vJAd5XOVOKZS7/DFGfi10w4vJfKuQ3OX/y2moFpopN8iTYxCoI9a9/wjKq2kA7D0Y9ySUW\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#2-deploy-your-helm-chart","title":"2. Deploy Your Helm Chart:","text":"<p>With the <code>values.yml</code> configured, deploy your Helm chart to apply the Slack alert integration settings. The Helm chart will use the configurations from <code>values.yml</code> to set up <code>AlertmanagerConfig</code> and <code>SealedSecret</code> resources as necessary.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/slack-integration-with-openshift-alerts/#example-test-alert","title":"Example test alert","text":"<p>Change the needed variables and apply the prometheus rule in your environment to send a alert to Slack. </p> <p>Note</p> <p>Edit the <code>metadata.namespace</code> and <code>spec.groups.rules.labels.namespace</code> to a namespace in your cluster</p> test-alert.yml<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    app.kubernetes.io/name: custom-monitoring-rules\n    app.kubernetes.io/part-of: openshift-monitoring\n    prometheus: k8s\n    role: alert-rules\n  name: test-alarm\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  groups:\n  - name: Test for sending an immediate alarm\n    rules:\n    - alert: TestAlert\n      expr: vector(1) &gt; 0 \n      labels:\n        severity: critical\n        namespace: &lt;NAMESPACE&gt;\n      annotations:\n        summary: \"This is a test alert\"\n        description: \"This alert is for testing purposes\"\n</code></pre> <p>Alert is firing </p> <p>Alert is resolved </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/","title":"Team Network Policies","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#overview","title":"Overview","text":"<p>Team namespaces are secured by default using Kubernetes Network Policies that implement a \"default deny-all\" approach with specific allow rules for essential services. This ensures that only authorized traffic can flow into and out of your team namespace.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#default-security-posture","title":"Default Security Posture","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#secure-by-default","title":"Secure by Default","text":"<ul> <li>Default deny-all: All ingress and egress traffic is blocked by default</li> <li>Automatic allow rules: Essential OpenShift services are automatically permitted</li> <li>Configurable: Network policies can be disabled if needed (not recommended for production)</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#configuration","title":"Configuration","text":"<p>Network policies are controlled by a single setting in your team configuration: <pre><code>networkPolicy:\n  default_deny_all: true  # Default: enabled for security\n</code></pre></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#network-policy-rules","title":"Network Policy Rules","text":"<p>When <code>default_deny_all: true</code>, the following network policies are automatically created:</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#1-default-deny-all-policy","title":"1. Default Deny-All Policy","text":"<pre><code># Blocks ALL ingress and egress traffic by default\npolicyTypes:\n- Ingress  \n- Egress\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#2-same-namespace-communication","title":"2. Same Namespace Communication","text":"<ul> <li>Purpose: Allow pods within the same namespace to communicate</li> <li>Scope: Bidirectional communication (ingress + egress)</li> <li>Use Case: Application components talking to each other</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#3-essential-egress-rules","title":"3. Essential Egress Rules","text":"<p>Automatically allows critical outbound connections: - DNS Resolution: Communication with OpenShift DNS (ports 5353 UDP/TCP) - Kubernetes API: Access to cluster API server (ports 443, 6443) - Purpose: Ensures pods can function properly</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#4-openshift-monitoring-access","title":"4. OpenShift Monitoring Access","text":"<ul> <li>Purpose: Allow OpenShift monitoring to scrape metrics</li> <li>Source: <code>network.openshift.io/policy-group: monitoring</code> namespaces</li> <li>Direction: Ingress only</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#5-openshift-ingress-access","title":"5. OpenShift Ingress Access","text":"<ul> <li>Purpose: Allow ingress controllers to route traffic to your applications</li> <li>Source: <code>policy-group.network.openshift.io/ingress</code> namespaces</li> <li>Direction: Ingress only</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#6-kubernetes-api-server-operator","title":"6. Kubernetes API Server Operator","text":"<ul> <li>Purpose: Allow API server operator to manage resources</li> <li>Source: <code>openshift-kube-apiserver-operator</code> namespace</li> <li>Direction: Ingress only</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#7-grafana-operator-access","title":"7. Grafana Operator Access","text":"<ul> <li>Purpose: Allow Grafana operator to manage Grafana instances</li> <li>Source: <code>grafana-operator</code> namespace</li> <li>Direction: Ingress only</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#8-prometheus-federation","title":"8. Prometheus Federation","text":"<ul> <li>Purpose: Allow team Prometheus to federate metrics from OpenShift monitoring</li> <li>Target: <code>openshift-monitoring</code> namespace</li> <li>Port: 9091 (federation endpoint)</li> <li>Direction: Egress only</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#9-team-namespace-communication","title":"9. Team Namespace Communication","text":"<ul> <li>Purpose: Allow communication with other namespaces belonging to the same team</li> <li>Selector: <code>soprasteria/team: &lt;team-name&gt;</code> label</li> <li>Direction: Bidirectional (ingress + egress)</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#security-benefits","title":"Security Benefits","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#traffic-isolation","title":"Traffic Isolation","text":"<ul> <li>East-West Security: Prevents lateral movement between different teams</li> <li>Ingress Control: Only authorized sources can reach your applications</li> <li>Egress Control: Prevents data exfiltration and unauthorized outbound connections</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#default-secure","title":"Default Secure","text":"<ul> <li>Zero Trust: No traffic is allowed unless explicitly permitted</li> <li>Operator-Friendly: Essential OpenShift services automatically allowed</li> <li>Application-Ready: Same-namespace communication enabled</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#compliance-ready","title":"Compliance Ready","text":"<ul> <li>Audit Trail: All network policies are declaratively defined</li> <li>Consistent Security: Same security model across all team namespaces</li> <li>Documentation: Clear documentation of allowed traffic flows</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#what-traffic-is-blocked","title":"What Traffic is Blocked","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#blocked-ingress","title":"Blocked Ingress","text":"<ul> <li>Traffic from other team namespaces (unless same team)</li> <li>Direct external traffic (must go through ingress controllers)</li> <li>Inter-cluster communication from non-whitelisted sources</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#blocked-egress","title":"Blocked Egress","text":"<ul> <li>Communication to other team namespaces (unless same team)</li> <li>Direct internet access (unless through allowed endpoints)</li> <li>Communication to cluster services not explicitly allowed</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#still-allowed","title":"Still Allowed","text":"<ul> <li>\u2705 Same namespace communication</li> <li>\u2705 OpenShift system services (monitoring, ingress, API)</li> <li>\u2705 DNS resolution and Kubernetes API access</li> <li>\u2705 Team-to-team communication (same team label)</li> <li>\u2705 Prometheus federation with OpenShift monitoring</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#troubleshooting-network-issues","title":"Troubleshooting Network Issues","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#common-issues","title":"Common Issues","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#pods-cant-communicate","title":"Pods Can't Communicate","text":"<p>Symptom: Applications in same namespace can't reach each other Solution: Check if <code>default_deny_all: true</code> - same namespace communication should be automatically allowed</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#monitoring-not-working","title":"Monitoring Not Working","text":"<p>Symptom: No metrics showing in OpenShift console Solution: Verify network policies allow OpenShift monitoring access</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#external-traffic-blocked","title":"External Traffic Blocked","text":"<p>Symptom: Applications can't reach external services Solution: This is expected behavior - add specific egress rules if needed</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#checking-network-policies","title":"Checking Network Policies","text":"<pre><code># List all network policies in your namespace\noc get networkpolicy -n &lt;team-namespace&gt;\n\n# Describe a specific policy\noc describe networkpolicy &lt;policy-name&gt; -n &lt;team-namespace&gt;\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#verifying-connectivity","title":"Verifying Connectivity","text":"<pre><code># Test DNS resolution\noc exec &lt;pod-name&gt; -n &lt;team-namespace&gt; -- nslookup kubernetes.default\n\n# Test API server access  \noc exec &lt;pod-name&gt; -n &lt;team-namespace&gt; -- curl -k https://kubernetes.default:443/api\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#disabling-network-policies","title":"Disabling Network Policies","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#when-to-disable","title":"When to Disable","text":"<ul> <li>Development/Testing: Temporary troubleshooting</li> <li>Legacy Applications: Applications requiring unrestricted network access</li> <li>Migration Period: Gradual adoption of network security</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#how-to-disable","title":"How to Disable","text":"<pre><code>networkPolicy:\n  default_deny_all: false  # Disables all network policies\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#security-warning","title":"\u26a0\ufe0f Security Warning","text":"<p>Disabling network policies removes all network security controls: - Any pod can communicate with your applications - No protection against lateral movement - Compliance and security audit failures possible</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#best-practices","title":"Best Practices","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#for-development","title":"For Development","text":"<ul> <li>Start Secure: Keep network policies enabled even in development</li> <li>Test Thoroughly: Verify application functionality with policies enabled</li> <li>Document Dependencies: Note any external services your applications need</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#for-production","title":"For Production","text":"<ul> <li>Always Enable: Network policies should always be enabled in production</li> <li>Monitor Access: Regularly review network policy effectiveness</li> <li>Document Exceptions: Any custom network policies should be documented</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#for-troubleshooting","title":"For Troubleshooting","text":"<ul> <li>Check Policies First: Network issues often relate to policy restrictions</li> <li>Use Logging: Enable network policy logging for debugging</li> <li>Test Incrementally: Temporarily disable policies for testing (non-production only)</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#custom-network-policies","title":"Custom Network Policies","text":"<p>If you need additional network rules beyond the defaults, you can:</p> <ol> <li>Contact Platform Team: Request additional standard policies</li> <li>Add Custom Policies: Deploy additional NetworkPolicy resources in your namespace</li> <li>Document Changes: Ensure custom policies are documented and reviewed</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/team-network-policies/#example-custom-policy","title":"Example Custom Policy","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-database\n  namespace: my-team-namespace\nspec:\n  podSelector:\n    matchLabels:\n      app: my-application\n  policyTypes:\n  - Egress\n  egress:\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 5432  # PostgreSQL\n</code></pre> <p>Remember: Custom policies work alongside the default policies - they don't replace them.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-network-observability/","title":"Configure to use Network Observability","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-network-observability/#introduction","title":"Introduction","text":"<p>The Network Observability Operator enables administrators to observe and analyze network traffic flows for OpenShift Container Platform clusters. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-network-observability/#getting-started","title":"Getting Started","text":"<p>To begin using the Network Observability Operator:</p> <ol> <li>Install the Operator: Follow the instructions provided in the OpenShift documentation to install the Network Observability Operator.</li> <li>Create a FlowCollector: Set up a FlowCollector resource to start gathering network flow data.</li> <li>Observe Network Traffic: Navigate to the 'Network Traffic' menu under 'Observe' to access the dashboard.</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-network-observability/#network-observability-operator-overview","title":"Network Observability Operator Overview","text":"<p>The Network Observability Operator is a critical component for developers looking to understand and monitor their network infrastructure within Kubernetes and OpenShift environments. This operator provides insights into network flows and helps maintain network visibility.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-network-observability/#dashboard-overview","title":"Dashboard Overview","text":"<p>The Network Observability dashboard is your command center for monitoring network traffic. It provides a comprehensive view of traffic flows, topology, and detailed metrics.</p> <p> Figure 1: Dashboard Overview</p> <p>Features include:</p> <ul> <li>Top 5 Average and Latest Rates: Visualize the most active network flows.</li> <li>Flow Rates Over Time: Observe the trends and fluctuations in network traffic.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-network-observability/#analyzing-traffic-flows","title":"Analyzing Traffic Flows","text":"<p>The 'Traffic Flows' panel offers a granular look at the individual network packets and bytes transmitted over the network. Use this to identify the top contributors to network traffic and pinpoint any unusual activity.</p> <p> Figure 2: Analyzing Traffic Flows</p> <p>Here you can find:</p> <ul> <li>Query Options: Customize what network flows are shown based on source, destination, or both.</li> <li>Quick Filters: Quickly exclude infrastructure traffic or focus on specific namespaces.</li> <li>Time Range and Refresh: Set the period for data display and the refresh rate for real-time updates.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-network-observability/#topology-view","title":"Topology View","text":"<p>The Topology view provides a dynamic graphical representation of network traffic, allowing you to see how traffic is flowing between nodes and services within your cluster.</p> <p> Figure 3: Topology View</p> <p>Highlights:</p> <ul> <li>Edge Labels: Choose to display the latest rate, average rate, bytes, or packets.</li> <li>Graph Vertices: Represents the network entities, such as pods and services.</li> <li>Grouping and Layout: Organize and display your network topology.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-openshift-observability/","title":"Using OpenShift Observability","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-openshift-observability/#using-openshift-observability","title":"Using OpenShift Observability","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-openshift-observability/#prerequisites","title":"Prerequisites","text":"<ul> <li>An OpenShift tenant created by Sopra Steria</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-openshift-observability/#introduction","title":"Introduction","text":"<p>In the OpenShift console, developers can access an observability hub for their applications. This hub gives real-time access to performance dashboards, triggered alerts and system logs in a user-friendly format.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-openshift-observability/#accessing-the-observability-hub","title":"Accessing the observability hub","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Observability/using-openshift-observability/#steps","title":"Steps","text":"<ol> <li>Navigate to the openshift-console.</li> <li>Select \"Developer View\", and navigate to \"Observe\"</li> <li>You should see performance metrics from your application, as seen below.</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/RBAC/rbac/","title":"RBAC","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/","title":"Resource Management","text":"<p>This page explains how to manage CPU and memory resources in OpenShift using requests, limits, quotas, and scaling strategies to ensure stable and efficient workloads.</p> <p>Official Documentation: Managing Resources </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#resource-requests-and-limits","title":"Resource Requests and Limits","text":"<p>Each container can specify CPU and memory requirements:</p> <ul> <li>Resource Request: Amount OpenShift reserves on the node. Used by scheduler for pod placement</li> <li>Resource Limit: Maximum amount a container can consume</li> <li>Memory limit exceeded \u2192 pod terminated (OOMKilled)</li> <li>CPU limit exceeded \u2192 process throttled (not terminated)</li> </ul> <p>Properly defined requests and limits prevent resource contention and ensure fair allocation.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#tenant-resource-quotas","title":"Tenant Resource Quotas","text":"<p>Each tenant has a ClusterResourceQuota defining maximum CPU and memory that can be requested across all tenant namespaces.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#request-vs-limit-in-quotas","title":"Request vs. Limit in Quotas","text":"<ul> <li>Request quota: Hard cap on total requested resources across all pods in tenant</li> <li>Limits: Should be set at pod level, not quota level, for flexibility</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#best-practices","title":"Best Practices","text":"<ul> <li>Set requests based on actual resource needs under normal conditions</li> <li>Define limits at pod level to prevent overconsumption</li> <li>Monitor and adjust quotas regularly</li> <li>Requests don't restrict actual usage\u2014pods can consume more if available</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#scaling-workloads","title":"Scaling Workloads","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#horizontal-pod-autoscaling-hpa","title":"Horizontal Pod Autoscaling (HPA)","text":"<p>Automatically adjusts number of pod replicas based on CPU utilization or other metrics.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#vertical-pod-autoscaling-vpa","title":"Vertical Pod Autoscaling (VPA)","text":"<p>Automatically adjusts CPU and memory requests based on actual usage. Restarts pods when necessary.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#hpa-and-vpa-in-openshift-tenants","title":"HPA and VPA in OpenShift Tenants","text":"<p>Not recommended in environments with ClusterResourceQuotas due to:</p> <ul> <li>Quota Exhaustion: VPA may increase requests beyond quota limits</li> <li>Unpredictable Scaling: Difficult to plan quota allocation</li> <li>Service Disruptions: VPA requires pod restarts</li> <li>Resource Starvation: Uneven resource allocation</li> </ul> <p>Recommendation: Manually define and regularly review resource requests/limits for predictable resource usage.</p> <p>Because of these risks, we do not necessarily recommend using HPA or VPA in our environment, particularly where ClusterResourceQuotas are in place. Instead, resource requests and limits should be manually defined and regularly reviewed to ensure a predictable and balanced use of resources across tenants.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/resource-management/#resource-monitoring-and-optimization","title":"Resource Monitoring and Optimization","text":"<ul> <li>Track usage patterns and detect anomalies</li> <li>Fine-tune requests/limits based on actual behavior</li> <li>Prevent overprovisioning and quota exhaustion</li> <li>Maintain stable, predictable, and efficient environments</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/team-resource-dashboards/","title":"Team Resource Dashboards","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/team-resource-dashboards/#introduction","title":"Introduction","text":"<p>This section provides an overview of the default Team Resource Dashboards that comes with your team\u2019s Grafana instance. </p> <p>These dashboards serve as both a resource management and FinOps overview, helping you visualize utilization, optimize costs, and right-size your workloads for improved efficiency.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/team-resource-dashboards/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Setting up Grafana for teams</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/team-resource-dashboards/#dashboards","title":"Dashboards","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/team-resource-dashboards/#team-overview","title":"Team Overview","text":"<p>This dashboard shows your team\u2019s CPU and memory usage and the resulting costs across all tenants. You can adjust the \u201cvCPU price\u201d variable at the top to update the per-core rate and see the impact on total CPU cost. It also includes a breakdown of quota usage and waste percentage for each tenant.</p> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Resource%20Management/team-resource-dashboards/#resource-optimization","title":"Resource Optimization","text":"<p>This dashboard gives a historical view of resource use and cost savings for a single tenant. It shows the number of namespaces, hard vs. used CPU quota (and what\u2019s left), the resulting total CPU cost, and estimated monthly savings from right-sizing. Below, a table lists each workload\u2019s current requests alongside optimized values and the CPU/memory savings (with NOK savings calculated).</p> <p></p> <p>The four dashboard variables at the top work as follows:</p> <ul> <li>Tenant: pick which tenant to analyze</li> <li>vCPU price: set the per-core rate to recalculate cost</li> <li>CPU Percentile: choose the usage percentile (e.g. 95%) for sizing recommendations. Meaning, in 95% of the cases, your CPU request will be sufficient. </li> <li>Overhead percentage: add a buffer to your CPU percentile. Meaning you get the CPU Percentile + X% overhead.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/","title":"Secret Management Introduction","text":"<p>OpenShift Container Platform-as-a-Service integrates secret management with the Teams and Tenants model for centralized credentials and secure, isolated access.</p> <p>Both External Secrets and Sealed Secrets provide Kubernetes/OpenShift secrets, covering all secret types and use cases described in Creating and Managing OpenShift Secrets.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#what-we-deliver","title":"What We Deliver","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#external-secrets","title":"External Secrets","text":"<p>Integrates with external Key Management System (KMS) (e.g., Azure Key Vault) to fetch and sync secrets:</p> <ul> <li>Dynamic retrieval and automatic rotation  </li> <li>GitOps-friendly, no sensitive data in Git  </li> <li>Centralized provider management  </li> </ul> <p>More information about External Secret and how it is integrated into OpenShift Tenant and Teams can be found here.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#sealed-secrets","title":"Sealed Secrets","text":"<p>Encrypts secrets for safe Git storage without external dependencies:</p> <ul> <li>One-way encryption, cluster-only decryption  </li> <li>Git-safe storage  </li> <li>Simple GitOps workflow integration</li> <li>Suitable if you can't use KMS</li> </ul> <p>More information about External Secret and how it is integrated into OpenShift Tenant and Teams can be found here.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#kubernetes-secrets","title":"Kubernetes Secrets","text":"<p>Both External Secrets and SealedSecrets creates a Kubernetes Secret in the end, so it is important to understand how these objects work and how you can fit them into your application.  OpenShift accommodates various secret types, each designed to meet specific application requirements. All secret values in OpenShift are base64 encoded, adding a layer of data protection. You can encode your secrets using the Linux CLI command:  <pre><code>echo -n 'mysecret' | base64 \n</code></pre></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#types-of-secrets","title":"Types of Secrets","text":"<ol> <li> <p>Generic Secrets (Opaque)    These are used for storing general sensitive data. Users can define multiple key-value pairs.</p> </li> <li> <p>Container Registry Secrets (Docker Config)    It is essential for pulling images from private container registries.</p> </li> <li> <p>TLS Secrets    Stores SSL certificates and keys for secure communications.</p> </li> <li> <p>Key Vault Credentials Secrets    Used for integrating with Azure KeyVault, holding the necessary access credentials.</p> </li> <li> <p>ArgoCD Repository secret    ArgoCD Repository secrets store the credentials needed to access the source repository for an ArgoCD project</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#sample-secret-definitions","title":"Sample Secret Definitions","text":"<p>Each section below provides a sample definition of different OpenShift secrets, explaining their structure and usage.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#generic-secret-opaque","title":"Generic Secret (Opaque)","text":"<p>A generic OpenShift secret uses the type opaque. Here, you define as many key-value pairs as you would like. Below is a sample of the syntax:</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret_name&gt;\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ndata:\n  &lt;key1&gt;: &lt;value1&gt;          # Substitute key1 and vaule1 with your key-value pair. Encode vaule1 with base64\n  &lt;key2&gt;: &lt;value2&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: demo-dev\ndata:\n  username: dXNlcm5hbWU=    # Decoded value: username\n  password: cGFzc3dvcmQ=    # Decoded value: password\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#container-registry-secret-docker-config","title":"Container Registry Secret (Docker Config)","text":"<p>To pull images from a container registry, the namespace needs an <code>imagePullSecret</code>. This secret must be defined with the type <code>kubernetes.io/dockerconfigjson</code>. The key should be named <code>.dockerconfigjson</code>, and the value should follow this format: </p> <pre><code>{\"auths\":{\"&lt;repoURL&gt;\":{\"username\":\"&lt;ClientID&gt;\",\"password\":\"&lt;ClientSecret&gt;\"}}}\n</code></pre> <p>Note</p> <p>Omitting the newline when encoding the value with base64 is crucial. Since this value includes the character <code>\"</code>, use <code>'</code> when encoding the value with base64, e.g., <code>echo -n '&lt;.dockerconfigjson&gt;' | base64</code>.</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret_name&gt;\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;json string containing repoURL, ClientID and ClientSecret&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: demo-docker-pull-secret\n  namespace: demo-dev\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: eyJhdXRocyI6eyJkZW1vLmF6dXJlY3IuaW8iOnsidXNlcm5hbWUiOiIzOTBmdWc5NC05M2o1LTNlcjYtOTI2My1kc2Y4OTJoYWtqZmUiLCJwYXNzd29yZCI6IlQ4ajhRfnFXcklhdndocHNIUmFSenMyYUpkSkphdnB1TGVKbHRkQkoifX19\n\n# Decoded value: {\"auths\":{\"demo.azurecr.io\":{\"username\":\"390fug94-93j5-3er6-9263-dsf892hakjfe\",\"password\":\"T8j8Q~qWrIavwhpsHRaRzs2aJdJJavpuLeJltdBJ\"}}}\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#tls-certificate-secrets","title":"TLS certificate secrets","text":"<p>SSL/TLS secrets in OpenShift are used for storing certificates and keys to enable secure communications over networks. </p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret_name&gt;\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ntype: kubernetes.io/tls\ndata:\n  tls.crt: &lt;TLS_public_crt&gt;\n  tls.key: &lt;tlS_private_key&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ingress-certificate\n  namespace: demo-dev\ntype: kubernetes.io/tls\ndata:\n  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCg==...\n  tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQ==...\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#keyvault-credentials-azure-keyvault","title":"Keyvault-credentials (Azure KeyVault)","text":"<p>KeyVault-credentials is a regular generic secret, but to standardize we use a default setup. Below is a sample of the syntax:</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: keyvault-credentials\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ndata:\n  ClientID: &lt;App_Registration_Object_ID&gt;\n  ClientSecret: &lt;Service_Principal_value&gt;\ntype: Opaque\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: keyvault-credentials\n  namespace: demo-dev\ndata:\n  ClientID: MzkwZnVnOTQtOTNqNS0zZXI2LTkyNjMtZHNmODkyaGFramZl               # Decoded value: 390fug94-93j5-3er6-9263-dsf892hakjfe\n  ClientSecret: VDhqOFF+cVdySWF2d2hwc0hSYVJ6czJhSmRKSmF2cHVMZUpsdGRCSg==   # Decoded value: T8j8Q~qWrIavwhpsHRaRzs2aJdJJavpuLeJltdBJ\ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Introduction/#repository-credentials-argocd-repository","title":"Repository credentials (ArgoCD repository)","text":"<p>To use a private repository as source repository for an ArgoCD application, it requires repository credentials. It is a generic secret, but for ArgoCD to notice it as a repository secret, it requires a specific label:</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;tenant&gt;-git-repository\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ndata:\n  username: &lt;username&gt;\n  password: &lt;password&gt;\n  project: &lt;ArgoCD_project, e.g. tenant_name&gt;\n  type: &lt;type, e.g. git&gt;\n  url: &lt;repoURL&gt;\ntype: Opaque\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: demo-dev-git-repository\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ndata:\n  username: dXNlcm5hbWU=    # Decoded value: username\n  password: cGFzc3dvcmQ=    # Decoded value: password\n  project: ZGVtbw==         # Decoded value: demo\n  type: Z2l0                # Decoded value: git\n  url: \ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/argocd-repository-secret/","title":"Configuring Argo CD Repository Secret with Your Team\u2019s ClusterSecretStore","text":"<p>This guide explains how to use your team's configured ClusterSecretStore to deploy an Argo CD repository secret. This secret allows Argo CD to authenticate and fetch application code from source repositories or Helm registries.</p> <p>Note</p> <p>For more details and advanced use cases, refer to our extended documentation.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/argocd-repository-secret/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure the following requirements are met:</p> <ul> <li>A ClusterSecretStore has been configured for your team. See this guide for setup instructions.</li> <li>The necessary credentials have been added to team's Key Vault.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/argocd-repository-secret/#argo-cd-repository-authentication-using-externalsecrets","title":"Argo CD Repository Authentication Using ExternalSecrets","text":"<p>To authenticate Argo CD with source repositories or Helm registries, you must reference credentials stored in your team\u2019s <code>ClusterSecretStore</code>. To reference the <code>ClusterSecretStore</code>, the name defined in the <code>gitops.authentication.external_scerts.secretstore</code> must match what you named the clustersecretstore under <code>secret_management.external_secrets.cluster_secret_stores.name</code>.</p> <p>In the secret definition for </p> <p>Sensitive values (e.g. tokens, private keys) should be stored as secrets in Azure Key Vault. These are referenced by key name. Non-sensitive values like usernames or URLs are written directly as plain text.</p> Example: GitHub App Authentication Using ClusterSecretStore <pre><code>secret_management:\n  external_secrets:\n    cluster_secret_stores: \n    - name: gitops\n      tenant_id: 8f3c5b3a-12d4-4e9f-9b92-7f04d2c44abc\n      keyvault_url: https://keyvault-team-poseidon-test.vault.azure.net/\n      client_id: AgAlMksI3FZJmIl3McPD[...]\n      client_secret: AgBsdfhaL1Aw0HbX[...]\n\ngitops:\n  authentication:\n    external_secrets:\n      secretstore: gitops\n      github_app:\n      - id: 123456\n        installation_id: 654321\n        private_key: GitHub-App-private-key\n        repo_url: https://github.com/soprasteria/team-poseidon\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/argocd-repository-secret/#github-app-authentication","title":"GitHub App Authentication","text":"<p>To authenticate using a GitHub App, provide the GitHub App credentials in your configuration as shown below:</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/argocd-repository-secret/#github-app-authentication_1","title":"GitHub App authentication","text":"<p>To authenticate with GitHub App, you are required to add your private key to the team's <code>ClusterSecretStore</code>. Fill in the template as follows:</p> GitHub AppExample <pre><code>gitops:\n  authentication:\n    external_secrets:\n      secretstore: \"\"\n      github_app:\n      - id: \"\"\n        installation_id: \"\"\n        private_key: \"\"\n        repo_url: \"\"\n      ssh_key:\n</code></pre> <pre><code>gitops:\n  authentication:\n    external_secrets:\n      secretstore: \"\"\n      github_app:\n      - id: \"\"\n        installation_id: \"\"\n        private_key: \"\"\n        repo_url: \"\"\n      ssh_key:\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/configure-cluster-secret-store/","title":"Configure ClusterSecretStore for Managing Team Secrets","text":"<p>This guide provides step-by-step instructions for configuring a ClusterSecretStore to enable secure, team-level secret management in OpenShift. Once configured, your team can use ExternalSecret to deploy sensitive data to your team\u2019s tenants and namespaces in a GitOps-friendly way.</p> <p>Note</p> <p>This guide uses Azure Key Vault as the external secret management provider.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/configure-cluster-secret-store/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following:</p> <ul> <li>An Azure Key Vault instance dedicated to your team  </li> <li>An Azure App Registration with associated ClientID and ClientSecret</li> <li>The App Registration must be assigned the Key Vault Secrets User role for the relevant Key Vault</li> <li>Your Azure Tenant ID</li> <li>The <code>kubeseal</code> and <code>yq</code> CLI tools installed locally  </li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/configure-cluster-secret-store/#encrypting-clustersecretstore-credentials","title":"Encrypting ClusterSecretStore Credentials","text":"<p>To authenticate with Azure Key Vault, OpenShift requires credentials from an Azure App Registration. Since these credentials are stored in a Git repository, they must be encrypted to ensure confidentiality. This is achieved using SealedSecrets, which provides one-way encryption such that only the SealedSecrets controller running within the OpenShift cluster can decrypt the data. </p> <p>You can encrypt the credentials using the <code>scripts/encrypt_client_credentials.sh</code> script in your tenant repository, or follow this guide to do it manually.</p> <p>Run the script as follows:</p> ScriptExample <pre><code>$ ./scripts/encrypt_client_credentials.sh\n\nEnter Client ID: &lt;Client_ID&gt;\nEnter Client Secret: &lt;Client_Secret&gt;\nEnter Team Name (namespace): &lt;Team_Name&gt;\nWhich cluster will the secret be deployed to?\n1. test\n2. prod\nEnter 1 or 2: &lt;1_or_2&gt;\n</code></pre> <pre><code>$ ./scripts/encrypt_client_credentials.sh\n\nEnter Client ID: 3b9f8a4e-1d2c-4567-9e8f-abc123def456\nEnter Client Secret: vN7~eFgL9qT#8wzX2!aMkH4JtPsD0rXeKpC7v\nEnter Team Name (namespace): team-poseidon\nWhich cluster will the secret be deployed to?\n1. test\n2. prod\nEnter 1 or 2: 1\n</code></pre> <p>The script will seal your credentials and print the encrypted values to standard output. These values will be used in the next step to configure the <code>ClusterSecretStore</code> as part of your team\u2019s secret management setup.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/configure-cluster-secret-store/#configuring-clustersecretstore-in-your-team-setup","title":"Configuring ClusterSecretStore in Your Team Setup","text":"<p>After sealing your credentials, add them to your team\u2019s configuration file under <code>secret_management</code>, together with the required values. Refer to the example below for guidance.</p> Secret Management ConfigurationExample <pre><code>secret_management:\n  external_secrets:\n    cluster_secret_stores: \n    - name: &lt;cluster_secret_stores_name&gt;\n      tenant_id: &lt;azure_tenant_id&gt; \n      keyvault_url: &lt;keyvault_url&gt; \n      client_id: &lt;encrypted_client_id&gt; \n      client_secret: &lt;encrypted_client_secret&gt; \n</code></pre> <pre><code>secret_management:\n  external_secrets:\n    cluster_secret_stores: \n    - name: gitops\n      tenant_id: 8f3c5b3a-12d4-4e9f-9b92-7f04d2c44abc\n      keyvault_url: https://keyvault-team-poseidon-test.vault.azure.net/\n      client_id: AgAlMkJ1FkdAaFFebrbwMsadZTdlz3BgP2dtsI3FZJmIl3McPD[...]\n      client_secret: AgB4MfXJu6oX4I3F+5JG1hSFHCnTtq9IdgdfhaL1Aw0HbX[...]\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/configure-cluster-secret-store/#final-step-submit-your-changes","title":"Final Step: Submit Your Changes","text":"<p>To complete the setup, commit your changes and create a pull request in your team\u2019s repository. Once the pull request is reviewed and merged, the ClusterSecretStore configuration will be active, and your team can begin using ExternalSecrets to manage secrets across tenants and namespaces.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/","title":"Creating and managing openshift secrets","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#introduction","title":"Introduction","text":"<p>This user guide offers insights and templates for creating and managing various types of OpenShift secrets. In OpenShift, secrets are essential for storing sensitive data, such as passwords, API keys, and tokens, securely. They ensure this critical information is accessible only to authorized users and systems.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>OpenShift Permissions: Users need permissions to create and manage resources within specific OpenShift tenants.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#openshift-secrets","title":"OpenShift Secrets","text":"<p>OpenShift accommodates various secret types, each designed to meet specific application requirements. All secret values in OpenShift are base64 encoded, adding a layer of data protection. You can encode your secrets using the Linux CLI command:  <pre><code>echo -n 'mysecret' | base64 \n</code></pre></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#types-of-secrets","title":"Types of Secrets","text":"<ol> <li> <p>Generic Secrets (Opaque)    These are used for storing general sensitive data. Users can define multiple key-value pairs.</p> </li> <li> <p>Container Registry Secrets (Docker Config)    It is essential for pulling images from private container registries.</p> </li> <li> <p>TLS Secrets    Stores SSL certificates and keys for secure communications.</p> </li> <li> <p>Key Vault Credentials Secrets    Used for integrating with Azure KeyVault, holding the necessary access credentials.</p> </li> <li> <p>ArgoCD Repository secret    ArgoCD Repository secrets store the credentials needed to access the source repository for an ArgoCD project</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#sample-secret-definitions","title":"Sample Secret Definitions","text":"<p>Each section below provides a sample definition of different OpenShift secrets, explaining their structure and usage.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#generic-secret-opaque","title":"Generic Secret (Opaque)","text":"<p>A generic OpenShift secret uses the type opaque. Here, you define as many key-value pairs as you would like. Below is a sample of the syntax:</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret_name&gt;\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ndata:\n  &lt;key1&gt;: &lt;value1&gt;          # Substitute key1 and vaule1 with your key-value pair. Encode vaule1 with base64\n  &lt;key2&gt;: &lt;value2&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\n  namespace: demo-dev\ndata:\n  username: dXNlcm5hbWU=    # Decoded value: username\n  password: cGFzc3dvcmQ=    # Decoded value: password\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#container-registry-secret-docker-config","title":"Container Registry Secret (Docker Config)","text":"<p>To pull images from a container registry, the namespace needs an <code>imagePullSecret</code>. This secret must be defined with the type <code>kubernetes.io/dockerconfigjson</code>. The key should be named <code>.dockerconfigjson</code>, and the value should follow this format: </p> <pre><code>{\"auths\":{\"&lt;repoURL&gt;\":{\"username\":\"&lt;ClientID&gt;\",\"password\":\"&lt;ClientSecret&gt;\"}}}\n</code></pre> <p>Note</p> <p>Omitting the newline when encoding the value with base64 is crucial. Since this value includes the character <code>\"</code>, use <code>'</code> when encoding the value with base64, e.g., <code>echo -n '&lt;.dockerconfigjson&gt;' | base64</code>.</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret_name&gt;\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;json string containing repoURL, ClientID and ClientSecret&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: demo-docker-pull-secret\n  namespace: demo-dev\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: eyJhdXRocyI6eyJkZW1vLmF6dXJlY3IuaW8iOnsidXNlcm5hbWUiOiIzOTBmdWc5NC05M2o1LTNlcjYtOTI2My1kc2Y4OTJoYWtqZmUiLCJwYXNzd29yZCI6IlQ4ajhRfnFXcklhdndocHNIUmFSenMyYUpkSkphdnB1TGVKbHRkQkoifX19\n\n# Decoded value: {\"auths\":{\"demo.azurecr.io\":{\"username\":\"390fug94-93j5-3er6-9263-dsf892hakjfe\",\"password\":\"T8j8Q~qWrIavwhpsHRaRzs2aJdJJavpuLeJltdBJ\"}}}\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#tls-certificate-secrets","title":"TLS certificate secrets","text":"<p>SSL/TLS secrets in OpenShift are used for storing certificates and keys to enable secure communications over networks. </p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret_name&gt;\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ntype: kubernetes.io/tls\ndata:\n  tls.crt: &lt;TLS_public_crt&gt;\n  tls.key: &lt;tlS_private_key&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ingress-certificate\n  namespace: demo-dev\ntype: kubernetes.io/tls\ndata:\n  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCg==...\n  tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQ==...\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#keyvault-credentials-azure-keyvault","title":"Keyvault-credentials (Azure KeyVault)","text":"<p>KeyVault-credentials is a regular generic secret, but to standardize we use a default setup. Below is a sample of the syntax:</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: keyvault-credentials\n  namespace: &lt;tenant&gt;-&lt;env&gt;\ndata:\n  ClientID: &lt;App_Registration_Object_ID&gt;\n  ClientSecret: &lt;Service_Principal_value&gt;\ntype: Opaque\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: keyvault-credentials\n  namespace: demo-dev\ndata:\n  ClientID: MzkwZnVnOTQtOTNqNS0zZXI2LTkyNjMtZHNmODkyaGFramZl               # Decoded value: 390fug94-93j5-3er6-9263-dsf892hakjfe\n  ClientSecret: VDhqOFF+cVdySWF2d2hwc0hSYVJ6czJhSmRKSmF2cHVMZUpsdGRCSg==   # Decoded value: T8j8Q~qWrIavwhpsHRaRzs2aJdJJavpuLeJltdBJ\ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/creating-and-managing-openshift-secrets/#repository-credentials-argocd-repository","title":"Repository credentials (ArgoCD repository)","text":"<p>To use a private repository as source repository for an ArgoCD application, it requires repository credentials. It is a generic secret, but for ArgoCD to notice it as a repository secret, it requires a specific label:</p> SecretDemo example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;tenant&gt;-git-repository\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ndata:\n  username: &lt;username&gt;\n  password: &lt;password&gt;\n  project: &lt;ArgoCD_project, e.g. tenant_name&gt;\n  type: &lt;type, e.g. git&gt;\n  url: &lt;repoURL&gt;\ntype: Opaque\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: demo-dev-git-repository\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ndata:\n  username: dXNlcm5hbWU=    # Decoded value: username\n  password: cGFzc3dvcmQ=    # Decoded value: password\n  project: ZGVtbw==         # Decoded value: demo\n  type: Z2l0                # Decoded value: git\n  url: \ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/Introduction/","title":"External Secrets Introduction","text":"<p>This page explains how to use External Secrets in OpenShift to securely sync secrets from Azure Key Vault and other providers into your cluster. It covers how to configure SecretStore and ClusterSecretStore resources for scalable and secure secret management across teams and tenants.</p> <p>Official Documentation: External Secrets Operator</p> <p>Below is a diagram to illustrate how the external secrets and cluster secret store works: </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/Introduction/#cluster-secret-store-with-team-integration","title":"Cluster Secret Store with team integration","text":"<p>With team integration, a <code>ClusterSecretStore</code> can be created at the team level and made accessible to all tenants managed by that team. This setup allows the team to centrally manage the credentials used by individual <code>SecretStore</code> resources across multiple tenant namespaces.</p> <p>As illustrated below, the blue boxes represent secrets and credentials associated with the <code>ClusterSecretStore</code>.</p> <p></p> <p>Info</p> <p><code>ExternalSecret 1\u20133</code> are defined in each tenant's namespace and reference the shared <code>ClusterSecretStore</code> to retrieve credentials from Azure Key Vault. </p> <p>This model ensures a consistent and scalable approach to secret management across environments while reducing duplication and simplifying credential lifecycle management.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/Introduction/#external-secrets-resource","title":"External Secrets Resource","text":"<p>In OpenShift, External Secrets allow applications to use secrets stored in external systems without exposing sensitive data in cluster configuration. They address several limitations of native OpenShift Secrets:</p> <ul> <li>Secure Storage: Secrets remain stored securely in an external backend. Gaining access requires compromising both the OpenShift cluster and the external provider, making it Git-friendly and more secure.</li> <li>Automated Rotation: Many supported providers offer automatic secret rotation, reducing the need for manual updates and improving security posture.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/Introduction/#how-it-works","title":"How it works","text":"<ol> <li>A <code>SecretStore</code> resource defines the connection to external KMS (e.g., Azure Key Vault)</li> <li>Developer creates an <code>ExternalSecret</code> resource referencing the <code>SecretStore</code></li> <li>ExternalSecret fetches secrets and creates Kubernetes Secret objects</li> <li>Resources can be stored in Git for GitOps workflows</li> </ol> <p>Info</p> <p>You can choose between creating a SecretStore or a ClusterSecretStore:</p> <ul> <li>ClusterSecretStore: Available cluster-wide across multiple namespaces</li> <li>SecretStore: Scoped to a single namespace</li> </ul> <p>This documentation focuses on Azure Key Vault, but External Secrets supports many providers. See the official documentation for more details.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/Introduction/#configure-secret-store-with-azure-key-vault-for-your-tenant","title":"Configure secret store with Azure Key Vault for your tenant","text":"<p>For configuring a secret store with Azure Key Vault you have to do the following:</p> <ul> <li>SecretStore Configuration - Setup App Registartion and Azure Key Vault and give the App Registration the 'Key Vault Secrets User' role</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/Introduction/#creating-external-secrets-with-azure-key-vault","title":"Creating external secrets with Azure Key Vault","text":"<ul> <li>External Secret Creation - Add the App Registration credentials and Azure Key Vault url to the OpenShift Tenant (or Team Overlay) and create your external secret</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/cluster-external-secrets/","title":"Creating a ClusterExternalSecret","text":"<p>A <code>ClusterExternalSecret</code> is a type of secret that can be deployed across multiple environments and shared between multiple tenants in OpenShift. It enables centralized secret management using Azure Key Vault.</p> <p>Customers do not have direct access to create <code>ClusterExternalSecret</code> objects in OpenShift, but Sopra Steria can create it on their behalf by following the steps below.</p> <p>The customer will be responsible for managing the secret after it has been created by Sopra Steria as requested.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/cluster-external-secrets/#prerequisites","title":"Prerequisites","text":"<ul> <li>A team or tenant must be defined in the cluster, with an associated ClusterSecretStore or SecretStore configured.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/cluster-external-secrets/#procedure","title":"Procedure","text":"<ol> <li> <p>Store the secret in Azure Key Vault </p> <ul> <li>The secret must be stored in the team's Key Vault that is linked to their <code>SecretStore</code> or <code>ClusterSecretStore</code>.</li> </ul> </li> <li> <p>Provide the following information to Sopra Steria:</p> <ul> <li>Name of the <code>SecretStore</code>/<code>ClusterSecretStore</code> to be used</li> <li>Desired name of the OpenShift secret to be created in the team's namespace</li> <li>Name of the key in the resulting OpenShift secret</li> <li>Name of the corresponding key in Azure Key Vault</li> </ul> </li> </ol> <p>Once this information is received, Sopra Steria will create the <code>ClusterExternalSecret</code> object for the customer.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/configuration-secretstore/","title":"Deploying Secret Store with Azure Key Vault for your tenant","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/configuration-secretstore/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following:</p> <ul> <li>An Azure Key Vault instance</li> <li>An Azure App Registration with associated ClientID and ClientSecret</li> <li>The App Registration must be assigned the <code>Key Vault Secrets User</code> role for the relevant Key Vault</li> <li>Your Azure Tenant ID</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/configuration-secretstore/#deploying-a-custom-secret-store","title":"Deploying a custom Secret Store","text":"<p>If there is need to create a custom Secret Store this can be done by creating it yourself. The secret store resource definition needs to be created and added to the <code>resources</code> section of your application's <code>kustomization.yml</code> file. This action will deploy the secret store directly to the application's environment.</p> <ul> <li>Below is a sample syntax for defining a SecretStore resource:</li> </ul> secret store resource layout<pre><code>apiVersion: external-secrets.io/v1alpha1\nkind: SecretStore\nmetadata:\n  name: &lt;tenant_name&gt;-secret-store\n  namespace: &lt;tenant_name&gt;-&lt;env&gt;\nspec:\n  provider:\n    azurekv:\n      authSecretRef:\n        clientId:\n          key: ClientID\n          name: keyvault-credentials\n        clientSecret:\n          key: ClientSecret\n          name: keyvault-credentials\n      authType: ServicePrincipal\n      tenantId: &lt;Azure-Tenant-ID&gt;\n      vaultUrl: &lt;Azure_KeyVault_Vault_URL&gt;\n</code></pre> <ul> <li>Below is a sample syntax for defining a ClusterSecretStore resource (If you want to have your secret store available cluster wide over multiple namespaces):</li> </ul> cluster secret store resource layout<pre><code>apiVersion: external-secrets.io/v1alpha1\nkind: ClusterSecretStore\nmetadata:\n  name: &lt;tenant_name&gt;-secret-store\n  namespace: &lt;tenant_name&gt;-&lt;first_env&gt;\nspec:\n  provider:\n    azurekv:\n      tenantId: &lt;AZURE_TENANT_ID&gt;\n      vaultUrl: &lt;AZURE_KEY_VAULT_URL&gt;\n      authSecretRef:\n        clientId:\n          name: keyvault-credentials\n          key: ClientID\n          namespace: &lt;namespace&gt;-&lt;first_env&gt;\n        clientSecret:\n          name: keyvault-credentials\n          key: ClientSecret\n          namespace: &lt;namespace&gt;-&lt;first_env&gt; # &lt;first_env&gt; value should be the first element in the environments list in the tenant\n</code></pre> <p>Ensure to replace placeholders like <code>&lt;tenant_name&gt;</code>, <code>&lt;env&gt;</code>, <code>&lt;AZURE_TENANT_ID&gt;</code>, <code>&lt;AZURE_KEY_VAULT_URL&gt;</code>, <code>&lt;namespace&gt;</code>, and <code>&lt;first_env&gt;</code> with your specific values. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/","title":"Creating the External Secret with Azure Key Vault","text":"<p>To sync a secret from Azure Key Vault into OpenShift, you must create a custom ExternalSecret resource. This resource interacts with a SecretStore to access the external secret, then creates an equivalent Kubernetes Secret within the specified namespace\u2014making the sensitive data available to applications in a secure and GitOps-friendly manner.</p> Defining an external secret<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: &lt;name_of_external_secret&gt;\n  namespace: &lt;tenant_name&gt;-&lt;env&gt;\nspec:\n  refreshInterval: 10s\n  secretStoreRef:\n    kind: SecretStore # Use ClusterSecretStore to share across namespaces\n    name: &lt;tenant_name&gt;-secret-store\n  target:\n    name: &lt;secret_name_in_ocp&gt;\n    creationPolicy: Owner\n  data:\n    - secretKey: &lt;key_name_in_ocp_secret&gt;\n      remoteRef:\n        key: &lt;key_name_in_azure_keyvault&gt;\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#field-descriptions","title":"Field Descriptions","text":"<ul> <li><code>&lt;name_of_external_secret&gt;</code>: A unique name for the ExternalSecret resource in OpenShift.</li> <li><code>&lt;tenant_name&gt;-&lt;env&gt;</code>: The namespace where the secret will be created (e.g., team1-preprod).</li> <li><code>refreshInterval</code>: How frequently the external secret is refreshed from Azure Key Vault.</li> <li><code>&lt;tenant_name&gt;-secret-store</code>: The name of the SecretStore that defines how to authenticate to Azure Key Vault.</li> <li><code>&lt;secret_name_in_ocp&gt;</code>: The name of the OpenShift Secret that will be created.</li> <li><code>&lt;key_name_in_ocp_secret&gt;</code>: The key under which the secret value will be stored in the OpenShift secret.</li> <li><code>&lt;key_name_in_azure_keyvault&gt;</code>: The name of the secret in Azure Key Vault.</li> </ul> <p>When <code>creationPolicy: Owner</code> is used, the lifecycle of the OpenShift Secret is bound to the ExternalSecret. If the ExternalSecret is deleted, the OpenShift Secret will be removed automatically. Alternatively, setting <code>creationPolicy: Merge</code> allows the Secret to persist after deletion of the ExternalSecret.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#example-of-a-completed-external-secret-configuration","title":"Example of a Completed External Secret Configuration","text":"<p>Below is a comprehensive example that includes an Azure secret, a SecretStore object, and an ExternalSecret object. This real-world example demonstrates how these elements interconnect.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#azure-secret","title":"Azure Secret","text":"<p>A secret is stored in Azure KeyVault with the following attributes:</p> <ul> <li>Vault URL: <code>https://demo-dev.vault.azure.net/</code></li> <li>Secret Name: <code>azure-secret</code></li> <li>Secret Value: <code>mysecret</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#secretstore-object","title":"SecretStore Object","text":"<p>The SecretStore object is defined within OpenShift to reference the Azure secret. Here is its configuration:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore # This value can also be ClusterSecretStore if you want multiple namespaces\nmetadata:\n  name: demo-secret-store\n  namespace: demo-dev\nspec:\n  provider:\n    azurekv:\n      authSecretRef:\n        clientId:\n          key: ClientID\n          name: keyvault-credentials\n        clientSecret:\n          key: ClientSecret\n          name: keyvault-credentials\n      authType: ServicePrincipal\n      tenantId: dne39jdh-slik-f9wj-bcdd-sdofjs5dlkfj\n      vaultUrl: https://demo-dev.vault.azure.net\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#externalsecret-object","title":"ExternalSecret Object","text":"<p>An ExternalSecret object is used to fetch the secret from Azure using the defined SecretStore. Its configuration is as follows:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: openshift-external-secret\n  namespace: demo-dev\nspec:\n  refreshInterval: 10s\n  secretStoreRef:\n    kind: SecretStore # This value can also be ClusterSecretStore if you want multiple namespaces\n    name: demo-secret-store\n  target:\n    name: openshift-secret\n    creationPolicy: Owner\n  data:\n  - secretKey: secret-key\n    remoteRef:\n      key: azure-secret\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#resulting-openshift-secret","title":"Resulting OpenShift Secret","text":"<p>The configuration above will result in the creation of the following OpenShift secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-secret\n  namespace: demo-dev\n  [...]\ndata:\n  secret-key: bXlzZWNyZXQ=     # Decoded value: mysecret\ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#summary","title":"Summary","text":"<p>In this configuration:</p> <ul> <li>The Azure secret named <code>azure-secret</code> has a value of <code>mysecret</code>.</li> <li>The SecretStore object is configured to access this secret.</li> <li>The ExternalSecret object fetches the secret from Azure and creates a corresponding secret in OpenShift named <code>openshift-secret</code>.dddd</li> </ul> Defining an external secret<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: &lt;name_of_external_secret&gt;\n  namespace: &lt;tenant_name&gt;-&lt;env&gt;\nspec:\n  refreshInterval: 10s\n  secretStoreRef:\n    kind: SecretStore # This value can also be ClusterSecretStore if you want multiple namespaces\n    name: &lt;tenant_name&gt;-secret-store\n  target:\n    name: &lt;secret_name_in_ocp&gt;\n    creationPolicy: Owner\n  data:\n  - secretKey: &lt;key_definition_in_secret&gt;\n    remoteRef:\n      key: &lt;key_ref_in_azure&gt;\n</code></pre> <p>Below is the explanation of the different variables:</p> <ul> <li>Name of External Secret (<code>&lt;name_of_external_secret&gt;</code>): This is the unique identifier for the ExternalSecret object within OpenShift.</li> <li>Namespace (<code>&lt;tenant_name&gt;-&lt;env&gt;</code>): Replace with the specific tenant and environment where the secret is to be deployed.</li> <li>Refresh Interval: This determines how often the external secret syncs with Azure KeyVault to ensure updated data accessibility.</li> <li>Secret Name in OCP (<code>&lt;secret_name_in_ocp&gt;</code>): The designated name for the secret within OpenShift post-import.</li> <li>Key Definition in Secret (<code>&lt;key_definition_in_secret&gt;</code>): This key is used within OpenShift for referencing the secret\u2019s value.</li> <li>Key Reference in Azure (<code>&lt;key_ref_in_azure&gt;</code>): The name of the secret as stored in Azure KeyVault.</li> </ul> <p>This ExternalSecret object connects to Azure KeyVault through the specified SecretStore, fetching the secret identified by <code>&lt;key_ref_in_azure&gt;</code>. It then creates or updates a secret named <code>&lt;secret_name_in_ocp&gt;</code> in OpenShift, storing the fetched value under <code>&lt;key_definition_in_secret&gt;</code>. </p> <p>When <code>creationPolicy: Owner</code> is set, the ExternalSecret object \"owns\" the resulting secret within OpenShift. In this ownership model, if the ExternalSecret object is deleted, the associated secret within OpenShift is also automatically deleted. On the other hand, a <code>creationPolicy: Merge</code> would imply that the secret remains even after deleting the ExternalSecret object, enhancing data persistence.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#example-of-a-completed-external-secret-configuration_1","title":"Example of a Completed External Secret Configuration","text":"<p>Below is a comprehensive example that includes an Azure secret, a SecretStore object, and an ExternalSecret object. This real-world example demonstrates how these elements interconnect.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#azure-secret_1","title":"Azure Secret","text":"<p>A secret is stored in Azure KeyVault with the following attributes:</p> <ul> <li>Vault URL: <code>https://demo-dev.vault.azure.net/</code></li> <li>Secret Name: <code>azure-secret</code></li> <li>Secret Value: <code>mysecret</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#secretstore-object_1","title":"SecretStore Object","text":"<p>The SecretStore object is defined within OpenShift to reference the Azure secret. Here is its configuration:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore # This value can also be ClusterSecretStore if you want multiple namespaces\nmetadata:\n  name: demo-secret-store\n  namespace: demo-dev\nspec:\n  provider:\n    azurekv:\n      authSecretRef:\n        clientId:\n          key: ClientID\n          name: keyvault-credentials\n        clientSecret:\n          key: ClientSecret\n          name: keyvault-credentials\n      authType: ServicePrincipal\n      tenantId: dne39jdh-slik-f9wj-bcdd-sdofjs5dlkfj\n      vaultUrl: https://demo-dev.vault.azure.net\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#externalsecret-object_1","title":"ExternalSecret Object","text":"<p>An ExternalSecret object is used to fetch the secret from Azure using the defined SecretStore. Its configuration is as follows:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: openshift-external-secret\n  namespace: demo-dev\nspec:\n  refreshInterval: 10s\n  secretStoreRef:\n    kind: SecretStore # This value can also be ClusterSecretStore if you want multiple namespaces\n    name: demo-secret-store\n  target:\n    name: openshift-secret\n    creationPolicy: Owner\n  data:\n  - secretKey: secret-key\n    remoteRef:\n      key: azure-secret\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/External%20Secrets/creating-external-secrets/#resulting-openshift-secret_1","title":"Resulting OpenShift Secret","text":"<p>The configuration above will result in the creation of the following OpenShift secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-secret\n  namespace: demo-dev  \ndata:\n  secret-key: bXlzZWNyZXQ=     # Decoded value: mysecret\ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/Introduction/","title":"Introduction Sealed Secrets","text":"<p>Sealed Secrets provide a secure and GitOps-friendly way to manage Kubernetes secrets in OpenShift by encrypting sensitive data so that it can safely be stored in Git. Unlike External Secrets, which fetch data from external providers like Azure Key Vault at runtime, Sealed Secrets rely on pre-encrypted content that is decrypted inside the cluster by the Sealed Secrets controller.</p> <p>This method is useful when you don\u2019t have access to a Key Management Service (KMS), or when you prefer to manage secret material directly within Git without integrating external backends.</p> <p>The Sealed Secret controller runs inside the cluster and is the only component capable of decrypting sealed content into native Kubernetes <code>Secret</code> objects, ensuring your sensitive data is protected, even in version control.</p> <p>Official Documentation: Sealed Secrets</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/Introduction/#key-benefits","title":"Key Benefits","text":"<ul> <li>GitOps-compatible \u2013 Safe to store encrypted secrets in your Git repository alongside the rest of your manifests.</li> <li>Cluster-side decryption \u2013 Only the controller inside the OpenShift cluster can decrypt the data.</li> <li>No external dependency \u2013 Doesn\u2019t require Key Vaults or cloud integrations.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/Introduction/#how-it-works","title":"How It Works","text":"<ol> <li>Generate or obtain Kubernetes secret (Docker config, password, API key)</li> <li>Encrypt secret using public certificate from Sealed Secrets controller</li> <li>Commit encrypted SealedSecret resource to Git</li> <li>Controller decrypts and creates standard Kubernetes Secret when applied</li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/Introduction/#encryption-scope","title":"Encryption Scope","text":"<p>When encrypting secrets, you must define the encryption scope, which determines how specific the decrypted Secret can be:</p> <ul> <li>Strict (default): Only decrypted in specific namespace with specific name</li> <li>Namespace-wide: Specific namespace, any name</li> <li>Cluster-wide: Any namespace and name (use with caution)</li> </ul> <p>Info</p> <p>Specified when using <code>kubeseal</code> or helper script from tenant repository.</p> <p>Warning</p> <p>Cluster-wide scope allows decryption in any namespace, reducing isolation and increasing exposure risk.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/Introduction/#tooling-and-workflow","title":"Tooling and Workflow","text":"<p>We recommend using the provided helper script to simplify the encryption process when encrypting credentials for Key Vaults:</p> <ul> <li><code>scripts/encrypt_client_credentials.sh</code></li> </ul> <p>Alternatively, you can follow the manual steps in our guide:</p> <ul> <li>Manually Encrypting with Sealed Secrets</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/Introduction/#when-to-use-sealed-secrets","title":"When to Use Sealed Secrets","text":"<p>Use Sealed Secrets if:</p> <ul> <li>You don\u2019t use Azure Key Vault or another supported KMS</li> <li>You prefer full control of secret content in Git</li> <li>You want to manage secrets without external dependencies</li> </ul> <p>For use cases involving shared credentials, automatic rotation, or large-scale secret reuse, we recommend External Secrets instead.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/encrypting-secret-with-sealed-secrets/","title":"Encrypt secrets with kubeseal","text":"<p>This guide explains how to create a <code>SealedSecret</code> by encrypting an OpenShift Secret using Kubeseal. SealedSecrets are safe to store in version control and can only be decrypted by the controller running in the target OpenShift cluster.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/encrypting-secret-with-sealed-secrets/#pre-requisites","title":"Pre-requisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li><code>Kubeseal</code> installed on your local machine. Follow this guide: Install Kubeseal</li> <li>The public certificate for your OpenShift cluster, available in the tenant repository under <code>/certificates</code></li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/encrypting-secret-with-sealed-secrets/#1-encode-your-secret","title":"1. Encode Your Secret","text":"<p>OpenShift secrets require base64-encoded values. To encode a string:</p> <pre><code>echo -n 'my-password' | base64\n</code></pre> <p>Tip</p> <p>Use the <code>-n</code> flag with echo to avoid appending a newline character, which would alter the result.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/encrypting-secret-with-sealed-secrets/#2-create-an-openshift-secret-manifest","title":"2. Create an OpenShift Secret Manifest","text":"<p>Construct a standard Secret manifest using your base64-encoded values. For example, if you're storing Azure credentials:</p> Configuration TemplateExample <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;secret-name&gt;\n  namespace: &lt;namespace&gt;\ndata:      \n  ClientID: &lt;client_id&gt;\n  ClientSecret: &lt;client_secret&gt;\ntype: Opaque\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: team-poseidon-gitops\n  namespace: team-poseidon\ndata:      \n  ClientID: YWFkMmM4ZDItMTIzNC00YWJjLTllOGQtYWJjZGVmMTIzNDU2\n  ClientSecret: ejN4OFF+QTdQOXRMa3FMSmpzOE4yYkMuM2NHfnJEOXVHUHVCbGJ6aw==\ntype: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/encrypting-secret-with-sealed-secrets/#3-encrypt-secret-using-kubeseal","title":"3. Encrypt Secret using Kubeseal","text":"<p>To generate a SealedSecret, run the following command:</p> <pre><code>kubeseal --cert /path/to/pub.crt --scope namespace-wide -o yaml &lt; secret.yaml\n</code></pre> <p>Replace <code>/path/to/pub.crt</code> with the path to your cluster's public certificate.</p> <p>This will output a SealedSecret manifest that is safe to commit to a Git repository.</p> <p>Note</p> <p>The <code>--scope namespace-wide</code> flag restricts the sealed secret to the specified namespace, meaning it can only be decrypted within that namespace.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/install-kubeseal/","title":"Kubeseal","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/install-kubeseal/#introduction","title":"Introduction","text":"<p>This user guide provides detailed instructions and templates for setting up encrypted variables for the ArgoCD feature within the OpenShift Tenant framework. It is divided into Two main sections:</p> <ol> <li> <p>Installation of Kubeseal: This section guides you through installing Kubeseal, a tool essential for encrypting the variables.</p> </li> <li> <p>Encrypt Secret with Kubeseal: This part explains how to use Kubeseal to seal a secret</p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/install-kubeseal/#install-kubeseal","title":"Install KubeSeal","text":"<p>If not already installed, you can install <code>kubeseal</code> on your machine with the following commands:</p> <p><pre><code>wget https://github.com/bitnami-labs/sealed-secrets/releases/download/&lt;release-tag&gt;/kubeseal-&lt;version&gt;-linux-amd64.tar.gz\n</code></pre> <pre><code>tar -xvzf kubeseal-&lt;version&gt;-linux-amd64.tar.gz kubeseal\n</code></pre> <pre><code>sudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre></p> <p>Remember to replace <code>&lt;release-tag&gt;</code> and <code>&lt;version&gt;</code> with the latest release version. For example:</p> <pre><code>wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.22.0/kubeseal-0.22.0-linux-amd64.tar.gz\ntar -xvzf kubeseal-0.22.0-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\n</code></pre> <p>For Mac users you can use Homebrew package manager:</p> <pre><code>brew install kubeseal\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Secret%20Management/Sealed%20Secrets/install-kubeseal/#encrypt-secret-with-kubeseal","title":"Encrypt Secret with Kubeseal","text":"<p>To seal the secret with <code>kubeseal</code>, you can use the following command:</p> <p><pre><code>kubeseal --cert /path/to/pub.cert --scope namespace-wide -f secret.yaml -o yaml &gt; seald_secret.yaml\n</code></pre> Replace <code>/path/to/pub.cert</code> with the path to your public certificate provided by Sopra Steria. <code>secret.yaml</code> is the path to the Secret YAML file you created in the first step. The sealed secret will be outputted to <code>sealed_secret.yaml</code>.</p> SecretSealed Secret <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: poseidon1\n  namespace: gitops-developers\n  labels:\n    argocd.argoproj.io/secret-type: repository\ntype: Opaque\nstringData:\n  type: git\n  url: https://github.com/TeamPoseidon/poseidon1_fake_main_repo.git\n  username: team.poseidon\n  password: pat_8VUndBHIJQCWIJUkhHWKWMFDAGAGMAGJ202ZIaT\n</code></pre> <pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  annotations:\n    sealedsecrets.bitnami.com/namespace-wide: \"true\"\n  creationTimestamp: null\n  name: poseidon1\n  namespace: gitops-developers\nspec:\n  encryptedData:\n    password: AgAKW3TQw5l51HOdRbuDjXpJvv+cm6AdE5E95kNn9yr9sifywF6VMSkToJO/YVs9w5NsG3Qd8CXR1dSB+klb3kJe9lIucY1gVML82XOZL+nRpVrDivyKNpxHfQOI0BOOg+wbQlZpzdhYaUUAkNN9vukAWoeb67BmDClMrhdzaVH19Htvc0GkoCNM/2HPbuKM8BTUn7xeUut6PVMmcNCKyb3uae3w26i8ElmQI10OSOMoFjVm37HNj1QnjbEmUmsvS6H0sVKo3/NiYC6T75FsesG/jp/FyNBDyjRlqpdSlJ0YYv15oEwI46ByxDFHyFA\n    type: AgAjVZbAT7lXfEwsRC25vNqceFiDoiCLM/lUx4I7ti1ngfRG50MC8c5v3ZiEaCxTICBqkBJpGKtXcgih8jddhXsldWyKlwVFKP5z43LLv2nqNc5Hb/tKZFCP+OVXa/zeWbVREKm/+dH170t0RlTd9PtUJ947i3I5/uJSlayG+D+zg4XBxj2QfKmKS5hntfE\n    url: AgCTo2lVv2zzReuviCxUlkfSF8BTNYX4iQAlQMP7D52AMpLa4fHJ/Ml+N3vzrmdsjeuOp2Wa51q9mz57tVKmYLHA7UQ7Jr2kwwTBNaiz2ZC7oeI2pLPtYuvhkKdz5vMBQxNzAmSavq7jdFulx9Q5k1BY3WNH3TYiQgwhXwHmIJKQS8gZH0JHU62UIZ7RzSNSxmqoejhurDIuzxdpm+Llm21U+VdHGBY+D7FnonEZ5xe5Hx9WayqYgFE\n    username: AgCrjo7uFTS2TWJlE7z6bi2bU1Z2GHI8ycF9Ktg7ZSHMJUemC5Zo9kBD393ixQkElda+UnkU5vYT5LaGf6KiggI1y0Ww3LOjunc60VXy3C7jDH4qmlydR/J7qALCrxvqoB1aPWVEadFWLyMLhEqWhtqtV+AugZIzFt4BjL5y5jjl9IgvOlINp8/2HbNWqvn2E02wsMn\n  template:\n    metadata:\n      annotations:\n        sealedsecrets.bitnami.com/namespace-wide: \"true\"\n      creationTimestamp: null\n      labels:\n        argocd.argoproj.io/secret-type: repository\n      name: poseidon1\n      namespace: gitops-developers\n    type: Opaque\n</code></pre>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Security/advanced-cluster-security/","title":"OpenShift Advanced Cluster Security (ACS)","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Security/advanced-cluster-security/#introduction","title":"Introduction","text":"<p>OpenShift Advanced Cluster Security (ACS) is an essential add-on for Container Platform as a Service (CaaS), purpose-built to enhance the security of your cloud-native applications. This guide is designed to help developers and DevOps professionals effectively use ACS to secure their deployments.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Security/advanced-cluster-security/#purpose-and-benefits","title":"Purpose and Benefits","text":"<p>This guide is intended for developers and DevOps professionals responsible for building, deploying, and running cloud-native applications on OpenShift. ACS gives a range of benefits, including:</p> <ul> <li> <p>Enhanced Security and productivity: ACS actively identifies and mitigates security threats throughout the development lifecycle. ACS provides integrated security guardrails that support developer velocity, allowing them to focus on innovation while maintaining a high level of security.</p> </li> <li> <p>Policy Enforcement: ACS enforces security policies seamlessly, ensuring developers adhere to best practices and compliance standards without disrupting their workflow.</p> </li> <li> <p>Continuous Scanning: Developers benefit from continuous scanning of container images and workloads, addressing vulnerabilities early in the development process.</p> </li> <li> <p>CI/CD Integration: ACS integrates smoothly into CI/CD pipelines, automating security checks and preventing vulnerabilities from reaching production environments.</p> </li> <li> <p>Reduced Operational Risk: By leveraging Kubernetes-native controls and automation, developers can minimize operational risk, reducing potential impacts on applications and infrastructure.</p> </li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Security/advanced-cluster-security/#getting-started-with-acs","title":"Getting Started with ACS","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Security/advanced-cluster-security/#prerequisites","title":"Prerequisites","text":"<p>The following steps must be done before you can use ACS to enhance the security of your organisation's applications:</p> <ul> <li>Order the ACS add-on for your OCP from Sopra Steria:</li> <li>Before using ACS, the product has to be enabled for your OpenShift Container Platform (OCP). ACS will run as an operator on the cluster and has to be enabled by OpenShift Administrators.</li> <li>Order access for your specific Tenant to ACS:</li> <li>Access must be ordered from Sopra Steria for each tenant.</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Security/advanced-cluster-security/#acs-dashboard-overview","title":"ACS Dashboard Overview","text":"<p>The ACS dashboard gives a quick overview of the security standards of your namespace. You can see policy violations by severity and by category. Which images and deployments are at most risk and compliance by different compliance standards.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Security/advanced-cluster-security/#checking-vulnerable-deployments","title":"Checking vulnerable deployments","text":"<p>Steps:</p> <ol> <li>Navigate to Vulnerability Management (1.0) --&gt; Dashboard.</li> <li>In the Top riskiest images section you will find all your images sorted from most vulnerable to least vulnerable. </li> <li>Click an image to get an in-dept analysis of the security of a given image, including CVE's, the containerfile and an analysis of vulnerabilities by image component. </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Storage/openshift-storage/","title":"OpenShift Storage","text":"<p>This page covers how to provision and manage persistent storage in OpenShift using OpenShift Data Foundation (ODF).</p> <p>It includes how to choose the right StorageClass, create PersistentVolumeClaims (PVCs), and use snapshots for backup and restore.</p> <p>Official Documentation: OpenShift Storage Overview</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Storage/openshift-storage/#storageclasses","title":"StorageClasses","text":"<p>A StorageClass defines how storage is provisioned in OpenShift, enabling dynamic creation of persistent volumes with specific performance and access settings.</p> <p>In a typical setup with ODF, the following StorageClasses are available:</p> <ul> <li><code>ocs-storagecluster-ceph-rbd</code> (default): RWO filesystem volumes and RWO/RWX block volumes</li> <li><code>ocs-storagecluster-cephfs</code>: RWO and RWX filesystem volumes for multi-pod access</li> <li><code>ocs-storagecluster-ceph-rgw</code>: Object Bucket Claims (OBCs) with Ceph</li> <li><code>openshift-storage.noobaa.io</code>: Object Bucket Claims (OBCs) with NooBaa</li> </ul> <p>Recommendation: Use filesystem storage classes for most workloads. Store object data outside the cluster when possible.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Storage/openshift-storage/#persistent-volumes","title":"Persistent Volumes","text":"<p>Create storage by defining a PVC and binding it to your application. The PV is automatically provisioned.</p> <ol> <li> <p>Create PVC:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-app-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: ocs-storagecluster-ceph-rbd\n</code></pre> </li> <li> <p>Bind to Pod:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app-pod\nspec:\n  containers:\n    - name: my-app-container\n      image: my-app-image:latest\n      volumeMounts:\n        - name: my-app-volume\n          mountPath: /path/to/mount\n  volumes:\n    - name: my-app-volume\n      persistentVolumeClaim:\n        claimName: my-app-pvc\n</code></pre> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/Storage/openshift-storage/#backup-and-restore","title":"Backup and Restore","text":"<p>Create point-in-time snapshots of PVCs and restore to new PVCs.</p> <ol> <li> <p>Navigate to Storage -&gt; PersistentVolumeClaims in the OpenShift console. </p> </li> <li> <p>Click the three dots to the right of the pvc you want to snapshot. </p> </li> <li> <p>Proceed with default settings, and click \"Create\" </p> </li> <li> <p>Navigate to Storage -&gt; VolumeSnapshots to find your recently created snapshot. </p> </li> <li> <p>To restore a PVC from the snapshot click the three dots next to the right of the VolumeSnapshot. </p> </li> <li> <p>Proceed with default settings to create a new pvc. </p> </li> <li> <p>Find your restored snapshot under Storage -&gt; PersistentVolumeClaims. </p> </li> </ol>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-teams/","title":"OpenShift Teams","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-teams/#what-is-a-team","title":"What is a Team?","text":"<p>To simplify the organisation and create a landing zone for developer teams we have created OpenShift Teams.</p> <p>Teams are organizational units that enable centralized management of multiple related runtime environments - OpenShft Tenants Teams provide shared resources and configurations that simplify administration for development teams managing multiple applications or microservices.</p> <p>When a team is created in OpenShift, a dedicated namespace is established where all common secrets, service accounts, and shared resources are managed centrally.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-teams/#team-tenant-relationship","title":"Team-Tenant Relationship","text":"<p>Teams act as a management layer above tenants:</p> <p>Below is an illustration of teams. </p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-teams/#team-features","title":"Team Features","text":""},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-teams/#centralized-resource-management","title":"Centralized Resource Management","text":"<ul> <li>Observability Dashboards: Grafana instance with datasources for all team tenants</li> <li>Secret Management: ClusterSecretStores accessible by all team tenants  </li> <li>GitOps Credentials: Shared repository access credentials for ArgoCD</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-teams/#benefits-for-development-teams","title":"Benefits for Development Teams","text":"<ul> <li>Reduced Duplication: Share credentials and configurations across tenants</li> <li>Unified Monitoring: Single Grafana instance for all team applications</li> <li>Simplified Administration: Manage multiple tenants from one central location</li> <li>Consistent Policies: Apply team-level policies to all member tenants</li> </ul>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-teams/#getting-started","title":"Getting Started","text":"<p>Quick Start: OpenShift Teams</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-tenants/","title":"OpenShift Tenants","text":"<p>OpenShift tenants provide isolated runtime environments for applications and microservices, each with its own resource quota for CPU and memory. This tenant model enables granular access control and efficient resource distribution among development teams, allowing teams to manage their allocated resources within defined limits.</p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-tenants/#features-and-benefits","title":"Features and Benefits","text":"<ul> <li> <p>Backup Scheduling: Teams can define backup schedules specifying which resources to back up and how long backups are retained, ensuring data protection and recovery readiness.</p> </li> <li> <p>Isolated Runtime Environments: Each tenant consists of a set of OpenShift namespaces serving as isolated environments for application deployment, enabling secure and independent operation of applications.</p> </li> <li> <p>Resource Management: A combined resource quota for memory and CPU capacity is allocated per tenant, ensuring efficient resource utilization and preventing resource contention.</p> </li> <li> <p>Role-Based Access Control (RBAC): Access to tenant resources is managed through standardized roles linked to the customer\u2019s Azure AD/Entra ID, following a least privilege approach to enhance security.</p> </li> <li> <p>Network Policies: Rules for incoming and outgoing traffic are defined via lists of URLs and IP sets, controlling service connectivity to and from tenant applications and improving network security.</p> </li> <li> <p>Secrets Management: Application secrets are securely stored and dynamically retrieved from Azure KeyVault, with Bitnami sealed secrets as a fallback, providing robust and version-controlled secret management.</p> </li> <li> <p>Grafana Integration: A dedicated Grafana instance can be set up for tenants to build custom dashboards, enabling tenant-specific monitoring and visualization.</p> </li> <li> <p>GitOps Integration: Git repositories can be used to define and version control the resources deployed within the tenant, streamlining deployment workflows and ensuring consistency.</p> </li> </ul> <p></p>"},{"location":"About%20Container-Platform-as-a-Service/Service%20Breakdown/old-maybe-remove/openshift-tenants/#getting-started","title":"Getting Started","text":"<p>Quick Start: OpenShift Tenants</p>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/","title":"OpenShift Pipelines with Tekton","text":""},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#introduction","title":"Introduction","text":"<p>This document describes the team's approach to delivering Continuous Integration and Continuous Delivery (CI/CD) workflow using Tekton Pipelines within an OpenShift environment. By integrating these powerful tools, we've established a streamlined workflow that automates the path from source code to production, ensuring consistency, reliability, and efficiency in our deployment processes.</p> <p>This CI/CD pipeline serves as a template to showcase our streamlined development and deployment capabilities to potential customers. It's a glimpse into how we can enhance their project's efficiency and reliability with our automated solutions.</p>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#design-overview","title":"Design overview","text":"<p>This is a general overview of the steps involved in the process. </p> <p></p> <p>This workflow is a high-level representation of a CI/CD process where code changes are automatically fetched, built, and deployed, demonstrating the concept of a pipeline that progresses from code to deployment in a series of automated steps.</p> <p>The <code>Pipeline</code> described in this workflow is our public Sopra Steria/Team Poseidon pipeline, here by called public-2S-pipeline, which is located in a publicly accessible GitHub Repository. The repository defines the different pipelines our customer can choose from to build and deploy their source code. The repository can be found here: OpenShift Pipelines</p> <ol> <li>Source Code: The process begins with the source code, which is the starting point for the CI/CD pipeline.</li> <li>Pipeline Task (Fetch): A pipeline task is initiated to fetch the source code upon a new merge to the main branch. This is the first operational step in the pipeline where the code is retrieved from the source repository.</li> <li>Pipeline Task (Build): Subsequent to fetching the source code, the next pipeline task is to build the code. The build task compiles the source code and then creates a container image with the application.</li> <li>Container Registry: Once the build is successful, the resulting container image is then pushed to a container registry. This registry could be a public or a private store where the image is held for deployment.</li> <li>Pipeline Task (Fetch from Container Registry): Another pipeline task is executed to fetch the container image tag from the build-task. This step ensures that the latest built image is retrieved for deployment.</li> <li>Pipeline Task (Update and Deploy): The final pipeline task is to update the deployment with the new container image and then deploy it. This step involves updating the image tag in the deployment configuration and then rolling out the deployment to a live environment.</li> <li>Deployment: The deployment is the end state where the new version of the application is running in the production or test environment, ready for use.</li> </ol>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#repository-overview","title":"Repository overview","text":"<p>This is a overview and description of the different repositories used in the CI/CD workflow. As the whole workflow is a complex process it is necessary to explain where each component is located and how the different components and therefore repositories interact with each other. </p> <ol> <li> <p>Public-2S-pipeline-repo: This repository defines the different pipelines our customer can choose from to build and deploy their source code. The repository contains one pipeline called <code>build-and-push-python.yml</code> which build and deploys a python application, before updating the image-tag in the deployment. With time we intend to extend this repository to contain pipelines for multiple programming languages such as java, C# and so on.  </p> <ul> <li>https://github.com/TeamPoseidonOCP/openshift-pipelines.git</li> </ul> </li> <li> <p>Tenant-application-repo: This repository is where the Helm-chart, Helm-values and the resources for the ArgoCD Application is defined. In this repository we define all the resources needed for the Continuous Integration part of the workflow, such as all the Tekton Pipeline resources.</p> <ul> <li>https://github.com/TeamPoseidonOCP/openshift-pipelines.git</li> </ul> </li> <li> <p>Deployment-repo (customer): This repository is where the customer defines the values and enables a CI/CD workflow from Sopra Steria and from these values there will be create an Application in ArgoCD with a deployment that contains the source-code-application image.</p> <ul> <li>https://github.com/TeamPoseidonOCP/poseidon2_main_repo.git</li> </ul> </li> <li> <p>Source-code-repo (customer): This is the source code of the customers application. The customer will have their application in this repository and a merge-request to the main branch in this repository will trigger the CI/CD-process. </p> <ul> <li>https://SolidCloudv2@dev.azure.com/SolidCloudv2/TeamPoseidon/_git/source-to-image</li> </ul> </li> </ol> <p>Note</p> <p>In the parts that follow the repositories mentioned here will be referenced to the names described above. </p>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#in-depth-overview","title":"In-depth overview","text":"<p>This section of the documentation aims to describe the CI/CD process in detail, and will provide the reader with a in-depth overview over the workflow. It will ble divided into two main sections; Continuous Integration (Tekton pipeline part) and Continuous Delivery.</p> <p></p>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#continuous-integration-with-tekton-pipelines","title":"Continuous Integration with Tekton Pipelines","text":"<p>1. Triggering the Pipeline:</p> <ul> <li>The process begins with a merge-to-main from the source-code-repo, in the chart above called <code>Customer Application repo</code>.</li> <li>A webhook is set up to listen for code updates and will trigger on this update.</li> <li>Once an update is detected, the webhook triggers the Tekton Pipeline.</li> </ul> <p>2. Tekton Pipeline Execution:</p> <ul> <li>The pipeline starts with the <code>TriggerRoute</code> which directs the webhook to the appropriate service.</li> <li>The <code>TriggerBinding</code> translates the webhook event into Tekton resource parameters.</li> <li>The <code>TriggerEventListener</code> responds to the event by initiating the <code>TriggerTemplate</code>.</li> <li>The <code>TriggerTemplate</code> defines a series of steps and resources that make up a <code>pipelineRun</code>.</li> <li>The <code>pipelineRun</code> is an instance of a <code>pipelineRef</code>, which references the predefined pipeline in the Public-2S-pipeline-repo.</li> <li>The pipeline then perform the task <code>fetch-repository</code>, <code>build</code> and <code>update-image-tag</code>, which is described in the numbered section here: [[#Design overview]]</li> </ul> <p>3. Image Creation:</p> <ul> <li>After the integration steps are completed, a container image is created with a <code>image-tag</code> that is equal to the pull-request number on the merge that was completed in the source-code-repo.</li> <li>The image is then pushed to a public or private container registry, ready to be pulled for deployment.</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#continuous-delivery","title":"Continuous Delivery","text":"<p>1. Deployment Preparation:</p> <ul> <li>The new image pushed to the container registry is fetched as part of the deployment process.</li> <li>A image-tag is updated in the deployment-repo, which holds the deployment specifications.</li> <li>ArgoCD will after 3 minutes try to sync and then update the deployment with the new image-version.</li> </ul> <p>2. Deployment to OpenShift:</p> <ul> <li>The deployment object in OpenShift pulls the new container image and deploys it to the cluster.</li> <li>The deployment is monitored, and health checks are performed to ensure successful deployment.</li> <li>The running deployment is verified to be using the correct image.</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#ordering-a-cicd-pipeline","title":"Ordering a CI/CD pipeline","text":"<p>In this section will go through the process of setting the correct variables and Helm-values in the deployment-repo. This is the process our customer will have to go through when thay want to enable a CI/CD pipeline. First let's look at the <code>values.yaml</code> file and go through the variables needed for enabling a CI/CD workflow. </p>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have read and deployed a deployment as described in this document:   <ul> <li>Tenant-application deployment</li> <li>Have defined a container limit range in your tenant</li> </ul> </li> </ul> <p>Note</p> <p>The values set in the example below is just for this documentation purpose and should be replaced with the correct values for your configuration. </p>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#values","title":"Values","text":"values.yaml<pre><code>pipeline:\n  enable_pipeline: true\n  name: testing-pipeline\n  container_registry:\n    name: 'sourcetoimage.azurecr.io'\n    image_name: 'test-image/test-image'\n    secret_credentials:\n      name: docker-credentials\n      encrypted_dockerconfig: &lt;ENCRYPTED_DOCKERCONFIG&gt;\n  ci:\n    enable_ci: true\n    git_application_repo: \n      url: 'https://github.com/application/main_repo.git'\n      path_context: 'path/to/application/'\n      ## programming_language: should be either python, java or c#\n      programing_language: python\n      revision: HEAD\n      secret:\n        custom_secret: false\n        name: 'repo-credentials'\n        encrypted_username: &lt;ENCRYPTED_USERNAME&gt;\n        encrypted_password: &lt;ENCRYPTED_PASSWORD&gt;\n  cd:\n    enable_cd: true\n    git_deployment_repo:\n      url: 'https://github.com/deployment/main_repo.git'\n      path_value_file: 'path/to/value_file/'\n      secret:\n        name: 'git-deployment-credentials'\n        encrypted_username: &lt;ENCRYPTED_USERNAME&gt;\n        encrypted_password: &lt;ENCRYPTED_PASSWORD&gt;\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#configuring-the-valuesyaml-file","title":"Configuring the <code>values.yaml</code> file","text":"<p>After you've set up your tenant application deployment, the next step is to configure the CI/CD pipeline using the <code>values.yaml</code> file. This file contains the necessary parameters and settings that define how your CI/CD pipeline will operate. Here's a brief explanation of the variables you need to set:</p>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#pipeline-configuration","title":"Pipeline Configuration","text":"<ul> <li><code>enable_pipeline</code>: Set this to <code>true</code> to activate the pipeline process.</li> <li><code>name</code>: Provide a name for your pipeline, such as <code>testing-pipeline</code>.</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#container-registry-settings","title":"Container Registry Settings","text":"<ul> <li><code>container_registry.name</code>: Specify the name of the container registry, like <code>sourcetoimage.azurecr.io</code>.</li> <li><code>container_registry.image_name</code>: Define the image name, for example, <code>test-image/test-image</code>.</li> <li><code>container_registry.secret_credentials.name</code>: Enter the name of your docker credentials secret.</li> <li><code>container_registry.secret_credentials.encrypted_dockerconfig</code>: Insert your encrypted docker configuration credentials.</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#continuous-integration-settings","title":"Continuous Integration Settings","text":"<ul> <li><code>ci.enable_ci</code>: Set to <code>true</code> to enable continuous integration.</li> <li><code>ci.git_application_repo.url</code>: Provide the URL to your application's Git repository.</li> <li><code>ci.git_application_repo.path_context</code>: Indicate the path context to your application within the repository.</li> <li><code>ci.git_application_repo.programing_language</code>: Specify the programming language used, choices being <code>python</code>, <code>java</code>, or <code>c#</code>.</li> <li><code>ci.git_application_repo.revision</code>: Usually set to <code>HEAD</code> to indicate the latest commit in the specified branch.</li> <li><code>ci.git_application_repo.secret.custom_secret</code>: Set to <code>false</code> if you are not using a custom secret.</li> <li><code>ci.git_application_repo.secret.name</code>: Name your repository credentials secret.</li> <li><code>ci.git_application_repo.secret.encrypted_username</code>: Your encrypted repository username.</li> <li><code>ci.git_application_repo.secret.encrypted_password</code>: Your encrypted repository password.</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Pipelines/OpenShift-pipelines/#continuous-delivery-settings","title":"Continuous Delivery Settings","text":"<ul> <li><code>cd.enable_cd</code>: Set to <code>true</code> to enable continuous delivery.</li> <li><code>cd.git_deployment_repo.url</code>: Provide the URL to your deployment's Git repository.</li> <li><code>cd.git_deployment_repo.path_value_file</code>: Specify the path to the value file within the deployment repository.</li> <li><code>cd.git_deployment_repo.secret.name</code>: Name your Git deployment credentials secret.</li> <li><code>cd.git_deployment_repo.secret.encrypted_username</code>: Your encrypted Git username for the deployment repository.</li> <li><code>cd.git_deployment_repo.secret.encrypted_password</code>: Your encrypted Git password for the deployment repository.</li> </ul> <p>Ensure that you replace the placeholder values with the actual data relevant to your environment. These configurations will direct the CI/CD pipeline on how to access your repositories, build and push images, and deploy your applications. Once set, your pipeline will be ready to run and manage the lifecycle of your applications automatically.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/","title":"Building an OpenShift Serverless Image","text":""},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#introduction","title":"Introduction","text":"<p>To use OpenShift Serverless, you must create a container image of your serverless function or application. This image will be deployed to OpenShift and scaled dynamically based on demand. While there are multiple ways to create and push an image, this guide demonstrates how to do it using Knative Functions (<code>kn func</code>) and push the image to Azure Container Registry (ACR) using Podman as the container engine.</p> <p>Follow this step-by-step guide to get your function packaged into an image and ready for deployment on OpenShift Serverless.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed and configured on your local machine:</p> <ul> <li>Knative Functions CLI (<code>kn func</code>)</li> <li>Podman or Docker</li> <li>OpenShift CLI (<code>oc</code>)</li> <li>Access to Azure Container Registry (ACR) and an Azure service principal with <code>acrpush</code> permissions.</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#step-by-step-guide-to-building-an-openshift-serverless-image","title":"Step-by-Step Guide to Building an OpenShift Serverless Image","text":""},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#1-initialize-a-podman-machine-if-not-already-initialized","title":"1. Initialize a Podman Machine (if not already initialized)","text":"<p>Before building and pushing your function image, make sure Podman is running with enough resources:</p> <pre><code># Initialize a Podman machine with specific resources (only needed if not already initialized)\npodman machine init --memory=8192 --cpus=2 --disk-size=20 \n\n# Start the Podman machine\npodman machine start \n\n# Set the Docker host environment variable for Podman (replace with actual path from output)\nexport DOCKER_HOST='unix:///var/folders/j_/tgnt8z2143v7h75y9scfgc_w0000gq/T/podman/podman-machine-default-api.sock' \n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#2-create-a-new-knative-function-project","title":"2. Create a New Knative Function Project","text":"<p>Knative Functions (<code>kn func</code>) supports multiple runtimes such as Node.js, Python, Go, and others. In this example, we will use the Node.js runtime to create a simple function:</p> <pre><code>kn func create hello-world -l node\ncd hello-world\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#3-implement-a-simple-hello-world-application-optional","title":"3. Implement a Simple \"Hello, World!\" Application (Optional)","text":"<p>You can modify the function to handle HTTP requests by implementing a simple \"Hello, World!\" application:</p> <ul> <li>Create an <code>index.js</code> file with the following content:</li> </ul> <pre><code>const express = require('express');\nconst app = express();\nconst port = process.env.PORT || 8080;\n\napp.get('/healthz', (req, res) =&gt; res.status(200).send('OK'));\napp.get('/ready', (req, res) =&gt; res.status(200).send('Ready'));\n\napp.get('/', (req, res) =&gt; {\n  res.send('Hello, World!');\n});\n\napp.listen(port, () =&gt; {\n  console.log(`Server is running on port ${port}`);\n});\n</code></pre> <ul> <li>Create a <code>package.json</code> file with the necessary dependencies and scripts:</li> </ul> <pre><code>{\n  \"name\": \"hello-world-function\",\n  \"version\": \"1.0.0\",\n  \"description\": \"A simple Hello World serverless function for OpenShift.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node index.js\"\n  },\n  \"dependencies\": {\n    \"express\": \"^4.17.1\"\n  },\n  \"author\": \"Your Name\",\n  \"license\": \"MIT\"\n}\n</code></pre> <ul> <li>Install the required Node.js packages:</li> </ul> <pre><code>npm install\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#4-build-the-function-image","title":"4. Build the Function Image","text":"<p>Now that the function is implemented, use Podman to build the image and push it to your Azure Container Registry (ACR).</p> <ul> <li>Log in to ACR:</li> </ul> Login to ACRExample <pre><code>podman login &lt;acr&gt;.azurecr.io --username &lt;app_id&gt; --password &lt;password&gt;\n</code></pre> <pre><code>podman login demo.azurecr.io --username 390fug94-93j5-3er6-9263-dsf892hakjfe --password T8j8Q~qWrIavwhpsHRaRzs2aJdJJavpuLeJltdBJ\n</code></pre> <ul> <li>Build the function image:</li> </ul> Build a function imageExample <pre><code>kn func build --image &lt;acr&gt;.azurecr.io/&lt;repository&gt;:&lt;tag&gt;\n</code></pre> <pre><code>kn func build --image demo.azurecr.io/hello-world:latest\n</code></pre> <ul> <li>Push the image to ACR:</li> </ul> Push image to ACRExample <pre><code>podman push &lt;acr&gt;.azurecr.io/&lt;repository&gt;:&lt;tag&gt;\n</code></pre> <pre><code>podman push demo.azurecr.io/hello-world:latest\n</code></pre> <p>Replace the placeholders <code>&lt;acr&gt;</code>, <code>&lt;app_id&gt;</code>, <code>&lt;tag&gt;</code>, and <code>&lt;password&gt;</code> with your actual Azure Container Registry (ACR) name, application ID, and password.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#5-verify-the-image-in-acr","title":"5. Verify the Image in ACR","text":"<p>Once the image is pushed, you can verify that it is available in your ACR:</p> <ul> <li>Log in to the Azure portal and navigate to your Container Registry.</li> <li>Under Repositories, you should see the newly pushed image (<code>hello-world:latest</code>).</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Building-an-OpenShift-Serverless-Image/#conclusion","title":"Conclusion","text":"<p>You have now successfully built and pushed a serverless function image to Azure Container Registry using Knative Functions and Podman. This image is ready to be deployed in OpenShift Serverless, where it can scale up and down based on demand.</p> <p>While this guide used Knative Functions and Podman, you can also use other container engines (like Docker) or registries (such as Docker Hub) depending on your environment and preferences. </p> <p>The next step is to deploy this image as a Knative service in OpenShift Serverless, where it will be managed and scaled automatically according to the traffic it receives.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Introduction/","title":"OpenShift Serverless Introduction","text":"<p>This page introduces OpenShift Serverless for running event-driven applications that scale automatically with demand.</p> <p>Official Documentation: OpenShift Serverless</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Introduction/#what-is-openshift-serverless","title":"What is OpenShift Serverless?","text":"<p>OpenShift Serverless enables deploying and managing serverless workloads on OpenShift. Built on Kubernetes and Knative, it provides scalable, event-driven applications without infrastructure management.</p> <p>Focus on writing code while the platform automatically scales applications based on incoming requests, ensuring efficient resource utilization.</p> <p></p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Introduction/#key-components","title":"Key Components:","text":"<ul> <li>Knative Serving: Manages deployment, autoscaling, and networking for serverless applications.</li> <li>Knative Eventing: Facilitates event-driven architectures by routing and processing events.</li> <li>Knative Functions: Provides a streamlined way to build and deploy serverless functions.</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Introduction/#why-use-openshift-serverless","title":"Why Use OpenShift Serverless?","text":"<p>Enables focusing on code development rather than infrastructure management. Simplifies deployment of microservices and serverless functions while leveraging Kubernetes' scalability, reliability, and security.</p> <p>Key Benefits: - Automatic Scaling: Applications scale dynamically based on traffic, including scale-to-zero when idle - Cost Efficiency: Pay only for compute resources used, optimizing costs for sporadic workloads - Reduced Operational Overhead: No server/VM management, faster development cycles - Microservices &amp; Event-Driven: Ideal for microservices architectures, API endpoints, and event-driven functions  </p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Introduction/#deployment-process","title":"Deployment Process","text":"<p>1. Build Application Image Build your application into a container image with all necessary code and dependencies. Push the image to a container registry (Azure Container Registry, Docker Hub, etc.).</p> <p>2. Configure OpenShift Resources Create required OpenShift resources: - Network Policy: Controls traffic flow - Service Account: Manages permissions - Pull Secret: Enables image registry access</p> <p>3. Deploy Knative Service Create and deploy the Knative service. OpenShift Serverless will automatically manage scaling and availability based on traffic.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/Introduction/#further-reading","title":"Further Reading","text":"<ul> <li>Building an OpenShift Serverless Image</li> <li>Deploying a Knative Service with Tenant Helm Chart</li> <li>Deploying a Knative Service without Tenant Helm Chart</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/","title":"Deploying a Knative Service with Tenant Helm Chart","text":""},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#introduction","title":"Introduction","text":"<p>This guide explains how to deploy a Knative service in OpenShift using the Tenant Helm chart. By leveraging the Tenant Helm chart, all the necessary resources required to deploy a Knative service will be automatically created in your tenant. These resources include a service account, network policy, and Docker pull secret.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#prerequisites","title":"Prerequisites","text":"<ul> <li>kubeseal: Installed on your local machine for encrypting Docker credentials.</li> <li>Service Account Credentials: Ensure you have the credentials for the service account that will be used to pull the container image.</li> <li>Container Image: Your application has been built into a container image and pushed to a container registry (e.g., Docker Hub, Azure Container Registry, or Red Hat Quay).</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#steps-to-deploy-a-knative-service","title":"Steps to Deploy a Knative Service","text":""},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#1-generate-the-docker-config-json","title":"1. Generate the Docker Config JSON","text":"<p>First, generate a Docker config JSON for your registry if you don\u2019t have one already. You can do this using the following command:</p> CommandExample <pre><code>oc create secret docker-registry openshift-serverless-pull-secret \\\n--docker-server=&lt;acr&gt;.azurecr.io \\\n--docker-username=&lt;app_id&gt; \\\n--docker-password=&lt;password&gt; \\\n--dry-run=client -o yaml | kubeseal \\\n--controller-name sealed-secrets \\\n--controller-namespace sealed-secrets \\\n--scope cluster-wide -o yaml\n</code></pre> <pre><code>oc create secret docker-registry demo-openshift-serverless-pull-secret \\\n--docker-server=demo.azurecr.io \\\n--docker-username=390fug94-93j5-3er6-9263-dsf892hakjfe \\\n--docker-password=T8j8Q~qWrIavwhpsHRaRzs2aJdJJavpuLeJltdBJ \\\n--dry-run=client -o yaml | kubeseal \\\n--controller-name sealed-secrets \\\n--controller-namespace sealed-secrets \\\n--scope cluster-wide -o yaml\n</code></pre> <p>Replace the <code>&lt;acr&gt;</code>, <code>&lt;app_id&gt;</code>, and <code>&lt;password&gt;</code> placeholders with your actual credentials.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#2-add-the-encrypted-docker-config-to-helm-chart","title":"2. Add the Encrypted Docker Config to Helm Chart","text":"<p>In your Helm chart values file, populate the <code>openshift_serverless.dockerconfigjson</code> field with the encrypted Docker config from the previous step.</p> Helm Chart ConfigurationExample <pre><code>openshift_serverless:\n  dockerconfigjson: &lt;encrypted-dockerconfigjson&gt;\n</code></pre> <pre><code>openshift_serverless:\n  dockerconfigjson: AgCpPP5NfOs4vmTKooXs0rEgCImPiK/gB2mlvMDdhCcyqPIsvvxCWGSGN1dXZMopg5MaCbm2yyD2cSU+5ZlcbodUYAqjJ+xo1b8QPHZTgSNi4b4//iItSGVICeShD4nErfJLnQ8wcL9+kksvj8B/pgBYcakkCPeGx8JYIoCtDZ2D4pa1iexZEQbk68Npw0OSzpoV5/bE+QP8NKLPcRlPngPBjBxuaZG6O2Nk1m5VCwIC4chQ9/l1d2w+VYKSxnak9XWyYSNwFrXFk2gOQdEbvQZDyik4g+fRYCcsQmGoQT3MM/XfaMFzxG5E11K4Jd0YdOzqV66c1bLYBOLzNoLV2IHNFdSFV8Sne87b5/1LnDIfEeqlGs1v/Hgfo4tY/6jcIsg1ITsPOXTOikgy4wVgNGmq6N2bdkwVsnx1OW1H8JnB25q3+4pcy1bK8Y4xfq/KROD8OHUhW/DU9u+AVs0LH7z7yliSLijIB0QliOKMDdeyAqrJ2DaF1l2uNujA0WpFC7WqDqJdKfgdc8SLxrNaMCnTmGeUpZC7geNbf9D/7ceP4VewNNrknTzrxLW2JFy+9FWyKVCidwP34j3vTgyxQSeT7kqLxZBJfgTKmUe6w1aB09baQxNBLYsQSBP9Kz7BweRyyJrVOTZTMjx51I7DHftVRMON9PEsobgxy+NBaR1l9sOp+ww/17uCIsEayNTTsCZu/uTbEb8VEwjeTDxA5kjXnONCOGy28R8NbGItoy/wUAVo4ItvFWvvGOQ/Usz0mgDYlPIH2spx1c5QTBTu6yrHRx4G0Sausjc/TviJWCrUgeFMJfdB9UMGGyv0cCtDeReV7cFIQqM=\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#3-deploy-resources-to-your-tenant","title":"3. Deploy resources to Your Tenant","text":"<p>Create a pull request with your given credentials. Once it's approved, all required resources will be deployed to your tenant.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#4-deploy-the-knative-service","title":"4. Deploy the Knative Service","text":"<p>Once the resources are created in your tenant, you can deploy your Knative service. Here's an example of a Knative service YAML manifest:</p> Knative Service DefinitionExample <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: &lt;service_name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  template:\n    spec:\n      serviceAccountName: &lt;service_account&gt;\n      containers:\n      - image:  &lt;acr&gt;.azurecr.io/&lt;repository&gt;:&lt;tag&gt;\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: hello-world\n  namespace: demo-dev\nspec:\n  template:\n    spec:\n      serviceAccountName: demo-knative-sa\n      containers:\n      - image:  demo.azurecr.io/hello-world:latest\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#5-check-the-status-of-the-service","title":"5. Check the Status of the Service","text":"<p>Once deployed, you can check the status of the Knative service using the following command. This will give you detailed information on whether the service is running and any issues it might be encountering.</p> Check statusExample <pre><code>kn service describe &lt;service_name&gt; -n &lt;namespace&gt;\n</code></pre> <pre><code>kn service describe hello-world -n demo-dev\n</code></pre> <p>After deployment, you can retrieve the external URL where your service is exposed by running:</p> Get routeExample <pre><code>oc get ksvc -n &lt;namespace&gt;\n</code></pre> <pre><code>oc get ksvc -n demo\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-with-tenant-chart/#conclusion","title":"Conclusion","text":"<p>This guide demonstrates how to use the Tenant Helm chart to automatically set up the resources required to deploy a Knative service in OpenShift. </p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/","title":"Deploying a Knative Service without Tenant Helm Chart","text":""},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#introduction","title":"Introduction","text":"<p>This quick start guide will walk you through deploying a Knative service in OpenShift. By the end of this guide, you will have a Knative service running that automatically scales based on traffic.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#prerequisites","title":"Prerequisites","text":"<ul> <li>kubeseal: Installed on your local machine for encrypting Docker credentials.</li> <li>Service Account Credentials: Ensure you have the credentials for the service account that will be used to pull the container image.</li> <li>Container Image: Your application has been built into a container image and pushed to a container registry (e.g., Docker Hub, Azure Container Registry, or Red Hat Quay).</li> </ul>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#steps-to-deploy-a-knative-service","title":"Steps to Deploy a Knative Service","text":""},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#1-create-a-network-policy","title":"1. Create a Network Policy","text":"<p>This step ensures that your Knative service can receive traffic from the OpenShift Serverless system.</p> Create a Network PolicyExample <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-serverless\n  namespace: &lt;namespace&gt;\nspec:\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          knative.openshift.io/system-namespace: \"true\"\n  podSelector: {}\n  policyTypes:\n    - Ingress\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-serverless\n  namespace: demo-dev\nspec:\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          knative.openshift.io/system-namespace: \"true\"\n  podSelector: {}\n  policyTypes:\n    - Ingress\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#2-create-a-pull-secret","title":"2. Create a Pull Secret","text":"<p>This pull secret allows OpenShift to access your private image registry to pull the container image for your Knative service. You can either create it declaratively and imperativly:</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#a-declaratively","title":"a. Declaratively:","text":"Create a Pull SecretExample <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-serverless-pull-secret\n  namespace: &lt;namespace&gt;\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-pull-secret&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: demo-openshift-serverless-pull-secret\n  namespace: demo-dev\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: AgCpPP5NfOs4vmTKooXs0rEgCImPiK/gB2mlvMDdhCcyqPIsvvxCWGSGN1dXZMopg5MaCbm2yyD2cSU+5ZlcbodUYAqjJ+xo1b8QPHZTgSNi4b4//iItSGVICeShD4nErfJLnQ8wcL9+kksvj8B/pgBYcakkCPeGx8JYIoCtDZ2D4pa1iexZEQbk68Npw0OSzpoV5/bE+QP8NKLPcRlPngPBjBxuaZG6O2Nk1m5VCwIC4chQ9/l1d2w+VYKSxnak9XWyYSNwFrXFk2gOQdEbvQZDyik4g+fRYCcsQmGoQT3MM/XfaMFzxG5E11K4Jd0YdOzqV66c1bLYBOLzNoLV2IHNFdSFV8Sne87b5/1LnDIfEeqlGs1v/Hgfo4tY/6jcIsg1ITsPOXTOikgy4wVgNGmq6N2bdkwVsnx1OW1H8JnB25q3+4pcy1bK8Y4xfq/KROD8OHUhW/DU9u+AVs0LH7z7yliSLijIB0QliOKMDdeyAqrJ2DaF1l2uNujA0WpFC7WqDqJdKfgdc8SLxrNaMCnTmGeUpZC7geNbf9D/7ceP4VewNNrknTzrxLW2JFy+9FWyKVCidwP34j3vTgyxQSeT7kqLxZBJfgTKmUe6w1aB09baQxNBLYsQSBP9Kz7BweRyyJrVOTZTMjx51I7DHftVRMON9PEsobgxy+NBaR1l9sOp+ww/17uCIsEayNTTsCZu/uTbEb8VEwjeTDxA5kjXnONCOGy28R8NbGItoy/wUAVo4ItvFWvvGOQ/Usz0mgDYlPIH2spx1c5QTBTu6yrHRx4G0Sausjc/TviJWCrUgeFMJfdB9UMGGyv0cCtDeReV7cFIQqM=\n</code></pre> <p>To seal the secret, see the Secret Management/Encrypt secrets documentation.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#b-imperativly","title":"b. Imperativly:","text":"CommandExample <pre><code>oc create secret docker-registry openshift-serverless-pull-secret \\\n--docker-server=&lt;acr&gt;.azurecr.io \\\n--docker-username=&lt;app_id&gt; \\\n--docker-password=&lt;password&gt; \\\n--dry-run=client -o yaml | kubeseal \\\n--controller-name sealed-secrets \\\n--controller-namespace sealed-secrets \\\n--scope cluster-wide -o yaml\n</code></pre> <pre><code>oc create secret docker-registry demo-openshift-serverless-pull-secret \\\n--docker-server=demo.azurecr.io \\\n--docker-username=390fug94-93j5-3er6-9263-dsf892hakjfe \\\n--docker-password=T8j8Q~qWrIavwhpsHRaRzs2aJdJJavpuLeJltdBJ \\\n--dry-run=client -o yaml | kubeseal \\\n--controller-name sealed-secrets \\\n--controller-namespace sealed-secrets \\\n--scope cluster-wide -o yaml\n</code></pre> <p>Replace the <code>&lt;acr&gt;</code>, <code>&lt;app_id&gt;</code>, and <code>&lt;password&gt;</code> placeholders with your actual credentials.</p>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#3-create-a-service-account","title":"3. Create a Service Account","text":"<p>The service account is configured with the pull secret, allowing it to authenticate with the private image registry.</p> Create a Service AccountExample <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: openshift-knative-sa\n  namespace: &lt;namespace&gt;\nimagePullSecrets:\n  - name: openshift-serverless-pull-secret\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: demo-knative-sa\n  namespace: demo-dev\nimagePullSecrets:\n  - name: demo-openshift-serverless-pull-secret\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#4-deploy-the-knative-service","title":"4. Deploy the Knative Service","text":"<p>This YAML file defines the Knative service and ties it to the service account you created earlier. Replace the <code>&lt;acr&gt;</code> and <code>&lt;namespace&gt;</code> placeholders with your actual Azure Container Registry (ACR) and namespace details.</p> Deploy Knative ServiceExample <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: &lt;service_name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  template:\n    spec:\n      serviceAccountName: &lt;service_account&gt;\n      containers:\n      - image: &lt;acr&gt;.azurecr.io/&lt;repository&gt;:&lt;tag&gt;\n        ports:\n        - containerPort: 8080\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: demo-dev\nspec:\n  template:\n    spec:\n      serviceAccountName: demo-knative-sa\n      containers:\n      - image: demo.azurecr.io/hello-world:latest\n        ports:\n        - containerPort: 8080\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#5-check-the-status-of-the-service","title":"5. Check the Status of the Service","text":"<p>Once deployed, you can check the status of the Knative service using the following command. This will give you detailed information on whether the service is running and any issues it might be encountering.</p> Check statusExample <pre><code>kn service describe &lt;service_name&gt; -n &lt;namespace&gt;\n</code></pre> <pre><code>kn service describe hello-world -n demo-dev\n</code></pre> <p>After deployment, you can retrieve the external URL where your service is exposed by running:</p> Get routeExample <pre><code>oc get ksvc -n &lt;namespace&gt;\n</code></pre> <pre><code>oc get ksvc -n demo\n</code></pre>"},{"location":"Additional%20Documentation/OpenShift%20Serverless/quick-start-guide-without-tenant-chart/#conclusion","title":"Conclusion","text":"<p>Your Knative service will now automatically scale based on demand, scaling down to zero when idle to conserve resources and scaling up to meet incoming traffic.</p>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/","title":"Resource Management","text":"<p>This page explains how to manage CPU and memory resources in OpenShift using requests, limits, quotas, and scaling strategies to ensure stable and efficient workloads.</p> <p>Official Documentation: Managing Resources </p>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#resource-requests-and-limits","title":"Resource Requests and Limits","text":"<p>Each container can specify CPU and memory requirements:</p> <ul> <li>Resource Request: Amount OpenShift reserves on the node. Used by scheduler for pod placement</li> <li>Resource Limit: Maximum amount a container can consume</li> <li>Memory limit exceeded \u2192 pod terminated (OOMKilled)</li> <li>CPU limit exceeded \u2192 process throttled (not terminated)</li> </ul> <p>Properly defined requests and limits prevent resource contention and ensure fair allocation.</p>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#tenant-resource-quotas","title":"Tenant Resource Quotas","text":"<p>Each tenant has a ClusterResourceQuota defining maximum CPU and memory that can be requested across all tenant namespaces.</p>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#request-vs-limit-in-quotas","title":"Request vs. Limit in Quotas","text":"<ul> <li>Request quota: Hard cap on total requested resources across all pods in tenant</li> <li>Limits: Should be set at pod level, not quota level, for flexibility</li> </ul>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#best-practices","title":"Best Practices","text":"<ul> <li>Set requests based on actual resource needs under normal conditions</li> <li>Define limits at pod level to prevent overconsumption</li> <li>Monitor and adjust quotas regularly</li> <li>Requests don't restrict actual usage\u2014pods can consume more if available</li> </ul>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#scaling-workloads","title":"Scaling Workloads","text":""},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#horizontal-pod-autoscaling-hpa","title":"Horizontal Pod Autoscaling (HPA)","text":"<p>Automatically adjusts number of pod replicas based on CPU utilization or other metrics.</p>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#vertical-pod-autoscaling-vpa","title":"Vertical Pod Autoscaling (VPA)","text":"<p>Automatically adjusts CPU and memory requests based on actual usage. Restarts pods when necessary.</p>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#hpa-and-vpa-in-openshift-tenants","title":"HPA and VPA in OpenShift Tenants","text":"<p>Not recommended in environments with ClusterResourceQuotas due to:</p> <ul> <li>Quota Exhaustion: VPA may increase requests beyond quota limits</li> <li>Unpredictable Scaling: Difficult to plan quota allocation</li> <li>Service Disruptions: VPA requires pod restarts</li> <li>Resource Starvation: Uneven resource allocation</li> </ul> <p>Recommendation: Manually define and regularly review resource requests/limits for predictable resource usage.</p> <p>Because of these risks, we do not necessarily recommend using HPA or VPA in our environment, particularly where ClusterResourceQuotas are in place. Instead, resource requests and limits should be manually defined and regularly reviewed to ensure a predictable and balanced use of resources across tenants.</p>"},{"location":"Additional%20Documentation/Resource%20Management/resource-management/#resource-monitoring-and-optimization","title":"Resource Monitoring and Optimization","text":"<ul> <li>Track usage patterns and detect anomalies</li> <li>Fine-tune requests/limits based on actual behavior</li> <li>Prevent overprovisioning and quota exhaustion</li> <li>Maintain stable, predictable, and efficient environments</li> </ul>"},{"location":"OpenShift%20Teams/team-introduction/","title":"Team Introduction","text":""},{"location":"OpenShift%20Teams/team-introduction/#what-is-a-team","title":"What is a Team?","text":"<p>Teams are organizational units that enable centralized management of multiple related tenants. Teams provide shared resources and configurations that simplify administration for development teams managing multiple applications or microservices.</p> <p>When a team is created in OpenShift, a dedicated namespace is established where all common secrets, service accounts, and shared resources are managed centrally.</p>"},{"location":"OpenShift%20Teams/team-introduction/#team-tenant-relationship","title":"Team-Tenant Relationship","text":"<p>Teams act as a management layer above tenants:</p> <p>Below is an illustration of teams. </p>"},{"location":"OpenShift%20Teams/team-introduction/#team-features","title":"Team Features","text":""},{"location":"OpenShift%20Teams/team-introduction/#centralized-resource-management","title":"Centralized Resource Management","text":"<ul> <li>Observability Dashboards: Grafana instance with datasources for all team tenants</li> <li>Secret Management: ClusterSecretStores accessible by all team tenants  </li> <li>GitOps Credentials: Shared repository access credentials for ArgoCD</li> </ul>"},{"location":"OpenShift%20Teams/team-introduction/#benefits-for-development-teams","title":"Benefits for Development Teams","text":"<ul> <li>Reduced Duplication: Share credentials and configurations across tenants</li> <li>Unified Monitoring: Single Grafana instance for all team applications</li> <li>Simplified Administration: Manage multiple tenants from one central location</li> <li>Consistent Policies: Apply team-level policies to all member tenants</li> </ul>"},{"location":"OpenShift%20Teams/team-yaml/","title":"How to configure team","text":"<p>Within the team definitions yaml file you can configure the following:</p> <pre><code>team:\n  name: &lt;Name of team&gt;\n  description: &lt;Description of team&gt;\n\nresource_management: &lt;Defualt resources allowed in the team namespaces for running Grafana, External Secret and GitOps&gt;\n  requests:\n    cpu: &lt;Default value is set to 200m&gt;\n    memory: &lt;Default value is set to \"500Mi\"&gt;\n  storage:\n    enable_custom_storageclass: false \n\nrbac:  \n  team_admin: &lt;AD Group for team admin&gt;\n\nobservability:\n  grafana_instance: &lt;Enable grafana instance in team namespace (true/false). default false&gt;  \n  grafana_admin: &lt;AD Group for grafana admin&gt;\n  grafana_editor: &lt;AD Group for editors, can be left blank and everyone will be editor&gt;\n\ngitops:\n  argocd:\n    enable_user_defined_apps: &lt;Enable creating applications with the user-defined method- app of apps (true/false). Defualt false&gt;\n    enable_auto_defined_apps: &lt;Enable using automatic application creation with an ArgoCD applicationsets per environment(true/false). Defualt true &gt;\n    team_repo_url:  &lt;Git repository url that GitOps (ArgoCD) will use as its \"source of truth\" for the team namespace&gt; \n    path: &lt;Path to the folder that contains infrastructure that the applicationsets will insert into the team namespace&gt;\n    syncPolicy:\n      allowEmpty: &lt;Allows ArgoCD to sync an ApplicationSet even if it results in an empty application (true/false). Default true&gt;\n      selfHeal: &lt;Automatically repair out-of-sync resources to match the desired state in Git (true/false). Default true&gt;\n      prune: &lt;Remove resources that are not present in the Git repository during sync (true/false). Default true&gt;\n    resource_name_first: &lt;Nameingstandard for ArgoCD applications created by applicationSets. If true the name of the resource (folder) will come first if false then the name of the team will come first. Default true&gt;\n    custom_target_revision: &lt;Allows setting the targetRevision at the application level for different environments in OpenShift. The generator picks up component names and creates targetRevision values based on the application folder name instead of using HEAD if set to true. Default false&gt;\n  authentication:\n    external_secrets:\n      secretstore: &lt;Name of SecretStore that contains all credentials for different authentication methods&gt;\n      helm_registry:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        registry_url: &lt;ACR login server url&gt;\n      github_app:\n      - id: &lt;The app id for your GitHub App&gt;\n        installation_id: &lt;The installation id for your GitHub App&gt;\n        private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      ssh_key:\n      - private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      pat:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n    sealed_secrets:\n      helm_registry:\n      - username: &lt;ACR username encrypted with sealedsecret&gt;\n        password: &lt;ACR access token encrypted with sealedsecret&gt;\n        registry_url: &lt;ACR login server url encrypted with sealedsecret&gt;\n      github_app: \n      - id: &lt;The app id for your GitHub App encrypted with sealedsecrets&gt;\n        installation_id: &lt;The installation id for your GitHub App encrypted with sealedsecrets&gt;\n        private_key: &lt;Private key for your GitHub App encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      ssh_key:\n      - private_key: &lt;Private key for your SSH-private-key encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      pat:\n      - username: &lt;Username used with PAT encrypted with sealedsecrets&gt;\n        password: &lt;PAT encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n\nsecret_management:\n  external_secrets:\n    cluster_secret_stores: \n    - name: &lt;ClusterSecretStore name. Team name will be prefix and then this name&gt;\n      tenant_id: &lt;AZURE_TENANT_ID - Tenant ID of your organizations Azure tenant&gt;\n      keyvault_url: &lt;Url to Azure Key Vault - https://AZURE_KEY_VAULT_URL&gt; \n      client_id: &lt;Sealed Secret App Registration Credentials - SealedSecret_CLIENT_ID&gt; \n      client_secret: &lt;Sealed Secret - App Registration Credentials -SealedSecret_CLIENT_SECRET&gt; \n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/rbac/","title":"RBAC Feature","text":"<p>RBAC is used to grant specific Azure Entra ID groups access to OpenShift roles across the team namespace and all associated tenant namespaces.</p> <pre><code>rbac:  \n  team_edit: \"\"\n  team_view: \"\"\n  team_monitoring_edit: \"\"\n  team_monitoring_view: \"\"\n  team_security: \"\"\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/rbac/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"Variable Description Example Type Default Value <code>rbac</code> <code>team_edit</code> AD Group for team edit access \"team-2s-edit\" String \"\" <code>team_view</code> AD Group for team view access \"team-2s-view\" String \"\" <code>team_monitoring_edit</code> AD Group for grafana admin access \"team-2s-monitoring-edit\" String \"\" <code>team_monitoring_view</code> AD Group for grafana view access \"team-2s-monitoring-view\" String \"\" <code>team_security</code> AD Group for ACS access \"team-2s-security\" String \"\""},{"location":"OpenShift%20Teams/Team%20features/resource-management/","title":"Resource Management Feature","text":"<p>Resource managment is used to set the cpu and memory request quotas for the team namespace. The request quotas set in this section determined the resources that will be reserved in the cluster for the team namespace.</p> <pre><code>resource_management: &lt;Defualt resources allowed in the team namespaces for running Grafana, External Secret and GitOps&gt;\n  requests:\n    cpu: &lt;Default value is set to 200m&gt;\n    memory: &lt;Default value is set to \"500Mi\"&gt;\n  storage:\n    enable_custom_storageclass: false \n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/resource-management/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"Variable Description Example Type Default Value Resource Management <code>resources.requests</code> <code>cpu</code> Combined cpu requests for all tenant namespaces. Fractional values are allowed. A Container that requests 500m CPU is guaranteed half as much CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example 100m CPU, 100 milliCPU, and 0.1 CPU are all the same. A precision finer than 1m is not allowed. 1 String/Int 200m <code>memory</code> Combined memory requests for all tenant namespaces. The memory resource is measured in bytes. Memory can be expressed as a plain or fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki. 1Gi String/Int 500Mi Storage <code>enable_custom_storageClass</code> Allows ArgoCD to sync an ApplicationSet even if it results in an empty application True / False Boolean false"},{"location":"OpenShift%20Teams/Team%20features/secret-management/","title":"Secret Management in an OpenShift Team","text":"<p>The secret management feature in the team chart is used to create global <code>ClusterSecretStores</code> within the team namespace. These stores can be utilized by all tenants assigned to the team.</p> <p>You can read more in depth about this feature by following this link: Secret Managment with External Secrets</p>"},{"location":"OpenShift%20Teams/Team%20features/secret-management/#setting-up-clustersecretstore","title":"Setting up ClusterSecretStore","text":"<p>To create a <code>ClusterSecretStore</code> using the team chart, configure the following parameters:</p> Configuration TemplateExample <pre><code>secret_management:\n  external_secrets:\n    cluster_secret_store: \n    - name: &lt;cluster_secret_store_name&gt;\n      tenant_id: &lt;azure_tenant_id&gt; \n      keyvault_url: &lt;keyvault_url&gt; \n      client_id: &lt;encrypted_client_id&gt;          # Encrypted with SealedSecret \n      client_secret: &lt;encrypted_client_secret&gt;  # Encrypted with SealedSecret\n</code></pre> <pre><code>secret_management:\n  external_secrets:\n    cluster_secret_stores: \n    - name: cluster-secret-store\n      tenant_id: 8f3c5b3a-12d4-4e9f-9b92-7f04d2c44abc\n      keyvault_url: https://team-poseidon.vault.azure.net/\n      client_id: AgAlMkJ1FkdAaFFebrbwMsadZTdlz3BgP2dtsI3FZJmIl3McPD[...]\n      client_secret: AgB4MfXJu6oX4I3F+5JG1hSFHCnTtq9IdgdfhaL1Aw0HbX[...]\n</code></pre> <p>Notes</p> <p><code>name</code> is used as a suffix to generate the final name of the <code>ClusterSecretStore</code>, which follows the format <code>&lt;team_name&gt;-&lt;name&gt;</code>, for example: <code>team-poseidon-cluster-secret-store</code>.</p>"},{"location":"OpenShift%20Teams/Team%20features/secret-management/#encrypting-key-vault-credentials","title":"Encrypting Key Vault Credentials","text":"<p>To allow OpenShift to authenticate with Azure Key Vault, the credentials from the App Registration must be encrypted before being stored in the Git repository. This is done using <code>SealedSecrets</code>, which ensures that only the <code>SealedSecrets controller</code> running in the OpenShift cluster can decrypt the values.</p> <p>You can encrypt the credentials using the <code>scripts/encrypt_client_credentials.sh</code> script in your tenant repository, or follow this guide to perform the encryption manually.</p> <p>Additional details on <code>SealedSecrets</code> can be found here.</p>"},{"location":"OpenShift%20Teams/Team%20features/secret-management/#in-depth-description-of-parameters","title":"In-depth Description of parameters","text":"<p>The table below provides detailed descriptions of each variable available in the <code>secret_management.external_secrets</code> configuration.</p>"},{"location":"OpenShift%20Teams/Team%20features/secret-management/#clustersecretstore-parameters","title":"ClusterSecretStore parameters","text":"Variable Descriptopn Example Type Default Value <code>cluster_secret_stores</code> A list of ClusterSecretStore entries used to connect to one or more Azure Key Vaults shared across environments \u2013 list <code>[]</code> <code>cluster_secret_stores[].name</code> Suffix used to generate the full <code>ClusterSecretStore</code> name in the format <code>&lt;tenant_name&gt;-&lt;name&gt;</code> <code>gitops</code> string <code>\"\"</code> <code>cluster_secret_stores[].tenant_id</code> Azure Tenant ID used to access the Key Vault <code>d93d3d23-50e3-46db-b3ad-8c6c281b431e</code> string <code>\"\"</code> <code>cluster_secret_stores[].keyvault_url</code> URL of the Azure Key Vault instance <code>https://poseidon1.vault.azure.net/</code> string <code>\"\"</code> <code>cluster_secret_stores[].client_id</code> Azure App Registration Client ID, encrypted using SealedSecrets <code>AgAlMkJ1FkdAaFFebrbwMsadZTdlz3BgP2dtsI3FZJmIl3McPD[...]</code> sealed string <code>\"\"</code> <code>cluster_secret_stores[].client_secret</code> Azure App Registration Client Secret, encrypted using SealedSecrets <code>AgB4MfXJu6oX4I3F+5JG1hSFHCnTtq9IdgdfhaL1Awsdfs0HbX[...]</code> sealed string <code>\"\"</code>"},{"location":"OpenShift%20Teams/Team%20features/team/","title":"Team Feature","text":"<p>The team name chosen in team definitions value yaml is used in the tenant definitions yaml for connect tenants to the team. it is important that name is exactly the same.</p> <pre><code>team:\n  name: &lt;Name of team&gt;\n  description: &lt;Description of team&gt;\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/team/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"Variable Description Example Type Default Value Team <code>name</code> Name of your team My-Team String \"\" <code>description</code> Description of your team This is My-Team String \"\""},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-argocd/","title":"GitOps - Argo CD Applications &amp; Applicationsets","text":"<p>On this page you can find information related to the <code>gitops.argocd</code> feature in the team concept. This page contains an example of how to implement the Argo CD applications &amp; Applicationsets</p>"},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-argocd/#how-to-configure-argocd-applicationset","title":"How to Configure ArgoCD Applicationset","text":"<p>Below we will go through an example on how to implemet the gitops argocd feature in the team chart. The example utilizes the default values for <code>SyncPolicies</code>, <code>resource_name_first</code>, and <code>custom_target_revision</code>.</p> <p>Info</p> <p>To use the gitops argocd feature you will also need to define the login credentials Argo CD will use to access the repository</p> <p>How to configure gitops authentication: GitOps - Auth &amp; Creds</p> <p>Below is an example of how the <code>gitops.argocd</code> section for defining applicationset details can look like:</p> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: https://github.com/customer-repo/openshift\n    path: \"/path/to/applications\"\n</code></pre> <p>Below is all possible fields to configure for the <code>gitops.argocd</code> feature</p> <pre><code>gitops:\n  argocd:\n    enable_user_defined_apps: &lt;Enable creating applications with the user-defined method- app of apps (true/false). Defualt false&gt;\n    enable_auto_defined_apps: &lt;Enable using automatic application creation with an ArgoCD applicationsets per environment(true/false). Defualt true &gt;\n    team_repo_url:  &lt;Git repository url that GitOps (ArgoCD) will use as its \"source of truth\" for the team namespace&gt; \n    path: &lt;Path to the folder that contains infrastructure that the applicationsets will insert into the team namespace&gt;\n    syncPolicy:\n      allowEmpty: &lt;Allows ArgoCD to sync an ApplicationSet even if it results in an empty application (true/false). Default true&gt;\n      selfHeal: &lt;Automatically repair out-of-sync resources to match the desired state in Git (true/false). Default true&gt;\n      prune: &lt;Remove resources that are not present in the Git repository during sync (true/false). Default true&gt;\n    resource_name_first: &lt;Nameingstandard for ArgoCD applications created by applicationSets. If true the name of the resource (folder) will come first if false then the name of the team will come first. Default true&gt;\n    custom_target_revision: &lt;Allows setting the targetRevision at the application level for different environments in OpenShift. The generator picks up component names and creates targetRevision values based on the application folder name instead of using HEAD if set to true. Default false&gt;\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-argocd/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>The <code>gitops</code> feature in the team concept have the same functionality as in the tenant concept\u00b4s <code>argocd</code> feature.</p> <p>As in the tenant concept you also have the ability to enable the user-defined method and the auto-defined method.</p> <ol> <li> <p>User-defined method: Create ArgoCD applications in your repository under the path <code>&lt;path&gt;/&lt;all argocd apps here will be generated&gt;</code>.</p> <ul> <li>To enable this choice, you must set the field <code>gitops.argocd.enable_user_defined_apps</code> to true.</li> </ul> </li> <li> <p>Auto-defined method: Use an ArgoCD applicationSet to create your applications automatically under the path <code>&lt;path&gt;/&lt;all folders here will be generated&gt;</code>.</p> <ul> <li>To enable this choice you have to set the  <code>gitops.argocd.enable_auto_defined_apps field</code> to true. This will create an ApplicationSet for the team namespace and will configure new applications when you add new folders in your team folder.</li> </ul> </li> </ol> <p>More information about how to set up a Git Repository for ArgoCD on OpenShift can be found here:</p> <ul> <li>OpenShift GitOps - Introduction </li> </ul> Variable Description Example Type Default Value SyncPolicy <code>allowEmpty</code> Allows ArgoCD to sync an ApplicationSet even if it results in an empty application True / False Boolean true <code>selfHeal</code> Automatically repair out-of-sync resources to match the desired state in Git True / False Boolean true <code>prune</code> Remove resources that are not present in the Git repository during sync True / False Boolean true Argo CD Configuration <code>team_repo_url</code> The URL of the git repository which ArgoCD will use as its \"source of truth\" https://github.com/customer-repo/openshift String \"\" <code>path</code> The path specifying where Argo CD will search for applications/applicationsets /path/to/applications String \"\" <code>resource_name_first</code> Nameingstandard for ArgoCD applications created by applicationSets. If true the name of the resource (folder) will come first if false then the name of the team will come first. True / False Boolean True <code>custom_target_revision</code> Allows setting the targetRevision at the application level for different environments in OpenShift. The generator picks up component names and creates targetRevision values based on the application folder name instead of using HEAD if set to true True / False Boolean False"},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-authentication/","title":"GitOps - Authentication","text":""},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-authentication/#gitops-authentication-credentials","title":"GitOps - Authentication &amp; Credentials","text":"<p>On this page you can find information related to the <code>gitops.authentication</code> feature in the team concept. This page contains an example of how to implement the credentials Argo CD will utilize to authenticate towards a repository</p>"},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-authentication/#how-to-configure-authentication-credentials","title":"How to Configure authentication credentials","text":"<p>Below we will go through an example on how to implemet the <code>gitops.authentication</code> feature in the team chart using GitHub App credentials as the authentication method. We will show how this can be done by using either external secrets or sealedsecrets.</p> <p>Warning</p> <p>To use external secrets for defining authentication methodes, you need to have set up a ClusterSecretStore in your team to pull down the credentials from your Key Vault provider.</p> <p>How to set up a ClusterSecretStore: Secret Managment</p> <p>Below is an example creating an external secret as an authentication method for Argo CD with a GitHub APP:</p> <pre><code>gitops:\n  authentication:\n    external_secrets:\n      secretstore: gitops\n      github_app:\n      - id: 374237872\n        installation_id: 8947359869\n        private_key: GithubAppPrivateKey\n        repo_url: https://github.com/customer-repo/openshift\n</code></pre> <p>Below is an example using kubeseal to create a sealedsecret as an authentication method for Argo CD with a GitHub APP:</p> <pre><code>gitops:\n  authentication:\n    sealed_secrets:\n      github_app: \n      - id: ngwio847359JHUjigiIIG98796HJ7697gug898GiuG... # SealedSecret\n        installation_id: biUYGVUVh786758GU78gUYGujad78hjJ... # SealedSecret\n        private_key: dhvibibvwiIYFHUKBSBIOH&amp;ABCGFVW895487u... # SealedSecret\n        type: IBKhwofi8979jBHJv78gUi8011IIuhfew98... # SealedSecret\n        repo_url: BIbi8473rege786JKHhgj8BhdksuV78Jl... # SealedSecret\n</code></pre> <p>Below is all possible fields to configure for the <code>gitops.authentication</code> feature</p> <pre><code>gitops:\n  authentication:\n    external_secrets:\n      secretstore: &lt;Name of SecretStore that contains all credentials for different authentication methods&gt;\n      helm_registry:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        registry_url: &lt;ACR login server url&gt;\n      github_app:\n      - id: &lt;The app id for your GitHub App&gt;\n        installation_id: &lt;The installation id for your GitHub App&gt;\n        private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      ssh_key:\n      - private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      pat:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n    sealed_secrets:\n      helm_registry:\n      - username: &lt;ACR username encrypted with sealedsecret&gt;\n        password: &lt;ACR access token encrypted with sealedsecret&gt;\n        enableOCI: \"\"\n        type: \"\"\n        registry_url: &lt;ACR login server url encrypted with sealedsecret&gt;\n      github_app: \n      - id: &lt;The app id for your GitHub App encrypted with sealedsecrets&gt;\n        installation_id: &lt;The installation id for your GitHub App encrypted with sealedsecrets&gt;\n        private_key: &lt;Private key for your GitHub App encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      ssh_key:\n      - private_key: &lt;Private key for your SSH-private-key encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      pat:\n      - username: &lt;Username used with PAT encrypted with sealedsecrets&gt;\n        password: &lt;PAT encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-authentication/#connecting-to-a-git-repository","title":"Connecting to a Git repository","text":"<p>The <code>argocd</code> feature can connect to a Git repository through a Personal Access Token (PAT), a GitHub App or SSH. The table below shows a more detailed description of each variable in the <code>gitops.argocd</code> feature under <code>authentication</code>. The table is split into four categories: </p> <ul> <li>helm_registry: variables needed to connect with a remote helm registry</li> <li>PAT: variables needed to connect with a GitHub PAT token</li> <li>GitHub app: variables need to configure the GitHub app</li> <li>SSH: variables need to configure through SSH</li> </ul> Variable Description Example Type Default Value Argo CD Authentication Helm Registry list[] <code>username</code> ACR username - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal Encrypted String \"\" <code>password</code> ACR password - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal Encrypted String \"\" <code>registry_url</code> The url of the helm registry - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" GitHub App list[] <code>id</code> The app id for your Github App - Clear text or sealedsecret depending on the method See description below String / Kubeseal Encrypted String \"\" <code>installation_id</code> The installation id for your GitHub App - Clear text or sealedsecret depending on the method See description below String / Kubeseal Encrypted String \"\" <code>private_key</code> Private key for your GitHub App - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal Encrypted String \"\" <code>repo_url</code> The url of the git repository - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" SSH list[] <code>private_key</code> Private key for your SSH-private-key - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal encrypted String \"\" <code>repo_url</code> The url of the git repository - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" PAT list[] <code>username</code> The username of the service account connecting to the git repository - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal encrypted String \"\" <code>password</code> The git Personal Access Token for the service account connecting to the git repository - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal encrypted String \"\" <code>repo_url</code> The url of the git repository - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" SealedSecrets Method Specific parameters <code>type</code> Type should either be \"git\" or \"helm\" depending the authentication section - Encrypted with sealedsecrets See description below Kubeseal encrypted String \"\""},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-introduction/","title":"GitOps Introduction - Team Concept","text":"<p>The <code>gitops</code> feature contains two main sections: <code>argocd</code> &amp; <code>authentication</code>. The <code>authentication</code> section i used to define the authentication credential utilized by Argo CD. The <code>argocd</code> section is used to define the repository Argo CD will connect to and the path where Argo CD will search for applications &amp; applicationsets </p> <p>You can read more in depth about this feature by following this link: ArgoCD</p> <p>On this page we will go through a simple example to get started with the gitops feature in addition to showing all possible fields and give an in depth description of each of the fields</p>"},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-introduction/#get-started","title":"Get Started","text":"<p>To get started with using the team concept\u00b4s GitOps feature you need to have your repository url and your login credentials availible. This can be in the form of a GitHub App, SSH private key or PAT token. </p> <p>Warning</p> <p>To use external secrets for defining authentication methodes, you need to have set up a ClusterSecretStore in your team to pull down the credentials from your Key Vault provider.</p> <p>How to set up a ClusterSecretStore: Secret Managment</p> <p>Below is a simple example of how you can configuration ArgoCD Applicationset with GitHub App Credentials using external secrets:</p> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: https://github.com/customer-repo/openshift\n    path: \"/path/to/applications\"\n  authentication:\n    external_secret:\n      secretstore: gitops\n      github_app:\n      - id: 374237872\n        installation_id: 8947359869\n        private_key: GithubAppPrivateKey\n        repo_url: https://github.com/customer-repo/openshift\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/gitops/gitops-introduction/#gitops-configuration-team-concept","title":"GitOps configuration - Team Concept","text":"<p>Below is all the possible configuration fields avalible to modify in the gitops feature:</p> <pre><code>gitops:\n  argocd:\n    enable_user_defined_apps: &lt;Enable creating applications with the user-defined method- app of apps (true/false). Defualt false&gt;\n    team_repo_url:  &lt;Git repository url that GitOps (ArgoCD) will use as its \"source of truth\" for the team namespace&gt; \n    path: &lt;Path to the folder that contains infrastructure that the applicationsets will insert into the team namespace&gt;\n    syncPolicy:\n      allowEmpty: &lt;Allows ArgoCD to sync an ApplicationSet even if it results in an empty application (true/false). Default true&gt;\n      selfHeal: &lt;Automatically repair out-of-sync resources to match the desired state in Git (true/false). Default true&gt;\n      prune: &lt;Remove resources that are not present in the Git repository during sync (true/false). Default true&gt;\n    resource_name_first: &lt;Nameingstandard for ArgoCD applications created by applicationSets. If true the name of the resource (folder) will come first if false then the name of the team will come first. Default true&gt;\n    custom_target_revision: &lt;Allows setting the targetRevision at the application level for different environments in OpenShift. The generator picks up component names and creates targetRevision values based on the application folder name instead of using HEAD if set to true. Default false&gt;\n  authentication:\n    external_secret:\n      secretstore: &lt;Name of SecretStore that contains all credentials for different authentication methods&gt;\n      helm_registry:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        registry_url: &lt;ACR login server url&gt;\n      github_app:\n      - id: &lt;The app id for your GitHub App&gt;\n        installation_id: &lt;The installation id for your GitHub App&gt;\n        private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      ssh_key:\n      - private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      pat:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n    sealed_secret:\n      helm_registry:\n      - username: &lt;ACR username encrypted with sealedsecret&gt;\n        password: &lt;ACR access token encrypted with sealedsecret&gt;\n        enableOCI: \"\"\n        type: \"\"\n        registry_url: &lt;ACR login server url encrypted with sealedsecret&gt;\n      github_app: \n      - id: &lt;The app id for your GitHub App encrypted with sealedsecrets&gt;\n        installation_id: &lt;The installation id for your GitHub App encrypted with sealedsecrets&gt;\n        private_key: &lt;Private key for your GitHub App encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      ssh_key:\n      - private_key: &lt;Private key for your SSH-private-key encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      pat:\n      - username: &lt;Username used with PAT encrypted with sealedsecrets&gt;\n        password: &lt;PAT encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/","title":"Observability Examples","text":"<p>This page shows working configurations for common observability setups. See the parameter reference in the main page for details.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#table-of-contents","title":"Table of contents","text":"<ul> <li>Minimal (Grafana + Prometheus, small, no HA)</li> <li>With Alertmanager and routes exposed</li> <li>HA Prometheus (medium) with Alertmanager</li> <li>Grafana only (no Prometheus stack)</li> <li>Prometheus only (no Grafana)</li> <li>Enable Alertmanager</li> <li>Basic Slack routing (single channel)</li> <li>Severity-based routing (different channels)</li> <li>Inhibition rules (mute lower severities if critical exists)</li> </ul>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#minimal","title":"Minimal (Grafana + Prometheus, small, no HA)","text":"<pre><code>observability:\n  grafana_instance: true\n  monitoringStack:\n    enable: true\n    monitoringStack_size: small\n    monitoringStack_retention: 7d\n    monitoringStack_high_availability: false\n    monitoringStack_alertmanager: false\n    monitoringStack_expose_route: false\n\nrbac:\n  team_monitoring_edit: &lt;AD group for Grafana admin&gt;\n  team_monitoring_view: \"\"  # Optional\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#am-with-routes","title":"With Alertmanager and routes exposed","text":"<pre><code>observability:\n  grafana_instance: true\n  monitoringStack:\n    enable: true\n    monitoringStack_size: small\n    monitoringStack_retention: 7d\n    monitoringStack_high_availability: false\n    monitoringStack_alertmanager: true\n    monitoringStack_expose_route: true\n\nrbac:\n  team_monitoring_edit: &lt;AD group for Grafana admin&gt;\n  team_monitoring_view: &lt;AD group for Grafana view&gt;\n</code></pre> <p>Note: When routes are exposed, anyone with network access to OpenShift can reach Prometheus/Thanos endpoints.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#ha-medium-am","title":"HA Prometheus (medium) with Alertmanager","text":"<pre><code>observability:\n  grafana_instance: true\n  monitoringStack:\n    enable: true\n    monitoringStack_size: medium\n    monitoringStack_retention: 7d\n    monitoringStack_high_availability: true   # 2\u00d7 Prometheus\n    monitoringStack_alertmanager: true\n    monitoringStack_expose_route: false\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#grafana-only","title":"Grafana only (no Prometheus stack)","text":"<pre><code>observability:\n  grafana_instance: true\n  monitoringStack:\n    enable: false\n\nrbac:\n  team_monitoring_edit: &lt;AD group for Grafana admin&gt;\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#prometheus-only","title":"Prometheus only (no Grafana)","text":"<pre><code>observability:\n  grafana_instance: false\n  monitoringStack:\n    enable: true\n    monitoringStack_size: small\n    monitoringStack_retention: 7d\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#enable-alertmanager","title":"Enable Alertmanager","text":"<pre><code># values.yaml\nobservability:\n  grafana_instance: true\n  monitoringStack:\n    enable: true\n    monitoringStack_size: small\n    monitoringStack_retention: 7d\n    monitoringStack_high_availability: false\n    monitoringStack_alertmanager: true\n    monitoringStack_expose_route: false\n\nrbac:\n  team_monitoring_edit: &lt;AD group for Grafana admin&gt;\n  team_monitoring_view: \"\"  # Optional\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#slack-basic","title":"Basic Slack routing (single channel)","text":"<p>1) Create a Secret with your Slack webhook: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: alertmanager-slack-webhook\ntype: Opaque\nstringData:\n  webhook-url: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre></p> <p>Alternative: manage the webhook with External Secrets (recommended): <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: alertmanager-slack-webhook\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: &lt;your-secretstore-name&gt;  # e.g. team-poseidon-secrets\n  target:\n    name: alertmanager-slack-webhook       # same Secret name AlertmanagerConfig references\n    creationPolicy: Owner\n    template:\n      type: Opaque\n  data:\n  - secretKey: webhook-url                 # key in the generated k8s Secret\n    remoteRef:\n      key: &lt;remote-secret-name&gt;            # secret name in your external store (e.g. Azure Key Vault)\n      # property: value                    # optional if your secret is a JSON object and you need a specific property\n      # version: &lt;version&gt;                 # optional\n</code></pre></p> <p>2) Apply an AlertmanagerConfig: <pre><code>apiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n  name: team-alertmanager-config\n  labels:\n    alertmanagerConfig: \"true\"\nspec:\n  route:\n    receiver: slack-default\n    groupBy: [\"alertname\", \"namespace\"]\n    groupWait: 30s\n    groupInterval: 5m\n    repeatInterval: 3h\n  receivers:\n  - name: slack-default\n    slackConfigs:\n    - apiURL:\n        name: alertmanager-slack-webhook\n        key: webhook-url\n      channel: \"#alerts-team\"\n      sendResolved: true\n      title: \"[{{ .CommonLabels.cluster }}] {{ .CommonLabels.alertname }}\"\n      text: |\n        Severity: {{ .CommonLabels.severity }}\n        Namespace: {{ .CommonLabels.namespace }}\n        {{ range .Alerts -}}\n        Summary: {{ .Annotations.summary }}\n        Description: {{ .Annotations.description }}\n        {{ end }}\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#severity-routing","title":"Severity-based routing (different channels)","text":"<p>Use different receivers for critical vs. warning/info.</p> <pre><code>apiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n  name: team-alertmanager-config\n  labels:\n    alertmanagerConfig: \"true\"\nspec:\n  route:\n    receiver: slack-general\n    groupBy: [\"alertname\", \"namespace\"]\n    routes:\n    - matchers:\n      - name: severity\n        value: critical\n        matchType: =\n      receiver: slack-oncall\n  receivers:\n  - name: slack-oncall\n    slackConfigs:\n    - apiURL:\n        name: alertmanager-slack-webhook\n        key: webhook-url\n      channel: \"#alerts-oncall\"\n      sendResolved: true\n      title: \"[CRITICAL] {{ .CommonLabels.alertname }}\"\n      text: \"Namespace: {{ .CommonLabels.namespace }} | {{ .CommonAnnotations.summary }}\"\n  - name: slack-general\n    slackConfigs:\n    - apiURL:\n        name: alertmanager-slack-webhook\n        key: webhook-url\n      channel: \"#alerts-general\"\n      sendResolved: true\n      title: \"[{{ .CommonLabels.severity | toUpper }}] {{ .CommonLabels.alertname }}\"\n      text: \"Namespace: {{ .CommonLabels.namespace }} | {{ .CommonAnnotations.summary }}\"\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/examples/#inhibition-rules","title":"Inhibition rules (mute lower severities if critical exists)","text":"<p>Prevent duplicate noise when a higher severity alert is active for the same target.</p> <pre><code>apiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n  name: team-alertmanager-config\n  labels:\n    alertmanagerConfig: \"true\"\nspec:\n  route:\n    receiver: slack-general\n  receivers:\n  - name: slack-general\n    slackConfigs:\n    - apiURL:\n        name: alertmanager-slack-webhook\n        key: webhook-url\n      channel: \"#alerts-general\"\n      sendResolved: true\n  inhibitRules:\n  - sourceMatch:\n    - name: severity\n      value: critical\n    targetMatch:\n    - name: severity\n      value: warning\n    equal: [\"alertname\", \"namespace\"]\n  - sourceMatch:\n    - name: severity\n      value: critical\n    targetMatch:\n    - name: severity\n      value: info\n    equal: [\"alertname\", \"namespace\"]\n</code></pre> <p>Notes: - Alertmanager runs with 2 replicas by default when enabled. - Replace Slack channels and webhook Secret names as appropriate. - You can store the webhook in an external secret manager and sync it into the Secret shown above if preferred.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/monitoring-stack-sizing/","title":"Monitoring Stack Sizing","text":"<p>This page details the estimated resource reservations for the team monitoring stack components (Prometheus, Grafana, Thanos sidecar, Alertmanager) across supported sizes and deployment modes (with / without HA, with / without Alertmanager). In most cases small stacksize should suffice, if you are unsure, use the small.</p> <p>These numbers reflect container requests (not limits).</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/monitoring-stack-sizing/#component-baseline-per-size","title":"Component Baseline (Per Size)","text":"Component / Size small (CPU / Mem) medium (CPU / Mem) large (CPU / Mem) Prometheus (per replica) 300m / 300Mi 500m / 500Mi 1000m / 1Gi Grafana 100m / 150Mi 200m / 250Mi 300m / 400Mi Thanos Sidecar 50m / 50Mi 100m / 100Mi 150m / 150Mi Alertmanager (per replica) 50m / 100Mi 100m / 150Mi 150m / 200Mi <p>Alertmanager run with 2 replicas (HA) whenever enabled. All totals that include Alertmanager already account for both replicas.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/monitoring-stack-sizing/#aggregated-totals","title":"Aggregated Totals","text":"Size Prometheus Replicas Alertmanager Replicas Prometheus per Replica (CPU / Mem) Grafana (CPU / Mem) Thanos Sidecar (CPU / Mem) Alertmanager (2\u00d7 CPU / Mem) Total Base (No HA, No Alertmanager) Total With Alertmanager (2\u00d7) Total With HA (2\u00d7 Prometheus) Total With HA + Alertmanager (2\u00d7) small 1 2 300m / 300Mi 100m / 150Mi 50m / 50Mi 100m / 200Mi 450m / 500Mi 550m / 700Mi 750m / 800Mi 850m / 1000Mi medium 1 2 500m / 500Mi 200m / 250Mi 100m / 100Mi 200m / 300Mi 800m / 850Mi 1000m / 1.15Gi 1.3 / 1.35Gi 1.5 / 1.65Gi large 1 2 1000m / 1Gi 300m / 400Mi 150m / 150Mi 300m / 400Mi 1.45 / 1.53Gi 1.75 / 1.92Gi 2.45 / 2.53Gi 2.75 / 2.92Gi <p>Legend: - Base = Prometheus (1) + Grafana + Thanos - \u201cWith Alertmanager\u201d = Base + 2\u00d7 Alertmanager replicas - HA = 2\u00d7 Prometheus + Grafana + Thanos - \u201cHA + Alertmanager\u201d = HA + 2\u00d7 Alertmanager - Thanos Query (if deployed separately) is not included</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/monitoring-stack-sizing/#calculation-formulas","title":"Calculation Formulas","text":"<p>Let:</p> <ul> <li>P = Prometheus per replica</li> <li>G = Grafana</li> <li>T = Thanos sidecar</li> <li>A = Alertmanager per replica (always \u00d72 when enabled)</li> </ul> <p>Then:</p> <ul> <li>Base (no Alertmanager) = P + G + T</li> <li>Base + Alertmanager = P + G + T + (2 \u00d7 A)</li> <li>HA (no Alertmanager) = (2 \u00d7 P) + G + T</li> <li>HA + Alertmanager = (2 \u00d7 P) + G + T + (2 \u00d7 A)</li> </ul> <p>Example (small, HA + Alertmanager):</p> <ul> <li>CPU: (2 \u00d7 300m) + 100m + 50m + (2 \u00d7 50m) = 850m</li> <li>Memory: (2 \u00d7 300Mi) + 150Mi + 50Mi + (2 \u00d7 100Mi) = 1000Mi</li> </ul> <p>Return to: Observability</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/","title":"Observability Feature","text":"<p>With Observability you can:</p> <ul> <li>Run a team-hosted Grafana that uses OpenShift login</li> <li>Auto-provision datasources for all tenants managed by the team</li> <li>Deploy a team Prometheus stack (Prometheus + Thanos sidecar, optional Alertmanager)</li> <li>Build dashboards and set up alerting for your workloads</li> <li>Scrape application metrics using ServiceMonitor/PodMonitor</li> <li>Optionally expose routes for Prometheus and Thanos Query for easy access</li> </ul>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#minimal-configuration","title":"Minimal configuration","text":"<p>See full examples here: Observability Examples</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#parameters","title":"Parameters","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#observability","title":"Observability","text":"Parameter Type Default Description Link observability.grafana_instance Boolean false Enable Grafana with OpenShift auth Details"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#rbac","title":"RBAC","text":"Parameter Type Default Description Link rbac.team_monitoring_edit String \"\" AD group with admin access to Grafana Details rbac.team_monitoring_view String \"\" AD group with viewer access (optional) Details"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#observabilitymonitoringstack","title":"observability.monitoringStack","text":"Parameter Type Default Description Link observability.monitoringStack.enable Boolean false Enable Prometheus stack Details observability.monitoringStack.monitoringStack_size String small Size preset (small/medium/large) Details observability.monitoringStack.monitoringStack_high_availability Boolean false Run 2 Prometheus replicas Details observability.monitoringStack.monitoringStack_retention String 7d Metrics retention window Details observability.monitoringStack.monitoringStack_alertmanager Boolean false Enable Alertmanager (2 replicas) Details observability.monitoringStack.monitoringStack_expose_route Boolean false Expose Prometheus/Thanos routes Details"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#parameter-details","title":"Parameter details","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#observability_1","title":"Observability","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-observability-grafana_instance","title":"observability.grafana_instance","text":"<p>Enables a team Grafana instance using OpenShift OAuth. Automatically wires datasources for team-managed tenants. - Requires RBAC groups (see below). - Route format: <code>https://&lt;team-name&gt;-grafana.&lt;cluster-apps-domain&gt;</code></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#rbac_1","title":"RBAC","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-rbac-team_monitoring_edit","title":"rbac.team_monitoring_edit","text":"<p>AD group with admin rights in Grafana. Mandatory when enabling Grafana.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-rbac-team_monitoring_view","title":"rbac.team_monitoring_view","text":"<p>AD group with view-only rights. Optional.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#observabilitymonitoringstack_1","title":"observability.monitoringStack","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-observability-monitoringstack-enable","title":"observability.monitoringStack.enable","text":"<p>Enables the team monitoring stack (Prometheus + Thanos sidecar; Alertmanager optional).</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-observability-monitoringstack-size","title":"observability.monitoringStack.monitoringStack_size","text":"<p>Controls base requests for components (Prometheus, Grafana, Thanos sidecar; Alertmanager if enabled). - Options: small, medium, large - See sizing page for totals and math: Monitoring Stack Sizing</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-observability-monitoringstack-ha","title":"observability.monitoringStack.monitoringStack_high_availability","text":"<p>Runs 2 Prometheus replicas for resiliency. - Prometheus PVC: 10Gi per replica (total 20Gi with HA)</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-observability-monitoringstack-retention","title":"observability.monitoringStack.monitoringStack_retention","text":"<p>Sets metrics retention window (e.g., 3d, 7d, 30d, 24h). - Fit retention to allocated storage (10Gi per Prometheus replica) - Reduce retention if nearing capacity</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-observability-monitoringstack-alertmanager","title":"observability.monitoringStack.monitoringStack_alertmanager","text":"<p>Enables Alertmanager (assumed 2 replicas by default). - Required for alert routing/notifications - Size-derived resource requests; see Monitoring Stack Sizing</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#param-observability-monitoringstack-expose-route","title":"observability.monitoringStack.monitoringStack_expose_route","text":"<p>Creates routes for Prometheus and Thanos Query. - Prometheus: <code>https://prometheus-&lt;team-name&gt;.&lt;cluster-apps-domain&gt;</code> - Thanos Query: <code>https://thanos-query-&lt;team-name&gt;.&lt;cluster-apps-domain&gt;</code></p> <p>Warning</p> <p>When routes are exposed, anyone with network access to the OpenShift environment can access Prometheus and Thanos endpoints.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#operational-notes","title":"Operational notes","text":"<ul> <li>Storage: Prometheus uses a fixed 10Gi PVC per replica (cannot be increased in-place).</li> <li>If approaching storage limits, reduce <code>monitoringStack_retention</code> and contact Sopra Steria for storage expansion.</li> <li>Datasources in Grafana are provisioned automatically for team tenants.</li> </ul>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#discovering-endpoints","title":"Discovering endpoints","text":"<p>Using OC: <pre><code>oc get route &lt;team-name&gt;-grafana-route -n &lt;team-namespace&gt;\noc get routes -n &lt;team-namespace&gt;  # Prometheus/Thanos if expose_route=true\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#monitoring-your-applications","title":"Monitoring your applications","text":"<p>Use ServiceMonitor or PodMonitor to scrape application metrics: - ServiceMonitor: scrape via Services (LB across replicas) - PodMonitor: scrape pods directly (DaemonSet/StatefulSet-friendly)</p> <p>Examples: - ServiceMonitor Examples - PodMonitor Examples</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#getting-started","title":"Getting Started","text":"<ol> <li>Choose your stack size based on your workload</li> <li>Set retention period appropriate for your needs</li> <li>Enable high availability for production environments</li> <li>Enable alertmanager if you need notifications</li> <li>Deploy your configuration and access Grafana through your team's route</li> <li>Access your services:</li> <li>Grafana: <code>https://&lt;team-name&gt;-grafana.&lt;cluster-apps-domain&gt;</code></li> <li>Prometheus (if route enabled): <code>https://prometheus-&lt;team-name&gt;.&lt;cluster-apps-domain&gt;</code></li> <li>Thanos Query (if route enabled): <code>https://thanos-query-&lt;team-name&gt;.&lt;cluster-apps-domain&gt;</code></li> </ol> <p>Your monitoring stack will be automatically configured with proper labels and selectors to monitor resources in your team namespace.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#monitoring-your-applications_1","title":"Monitoring Your Applications","text":"<p>To monitor your applications with the team monitoring stack, you need to create monitoring resources that tell Prometheus how to discover and scrape metrics from your applications.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#servicemonitor-vs-podmonitor","title":"ServiceMonitor vs PodMonitor","text":"<ul> <li>ServiceMonitor: Monitors applications through Kubernetes services. Use this when you have services in front of your pods and want load balancing across replicas.</li> <li>PodMonitor: Monitors pods directly. Use this for more granular control, when monitoring DaemonSets/StatefulSets, or when you don't have services.</li> </ul> <p>For detailed configuration examples, see: - ServiceMonitor Examples - Basic and authenticated service monitoring - PodMonitor Examples - Direct pod monitoring and StatefulSet monitoring</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#service-discovery","title":"Service Discovery","text":"<p>All deployed observability services can be found in your team namespace:</p> <p><pre><code># List all services\noc get svc -n &lt;team-namespace&gt;\n\n# List all routes (external access)\noc get routes -n &lt;team-namespace&gt;\n\n# Check specific observability pods\noc get pods -l app.kubernetes.io/part-of=observability -n &lt;team-namespace&gt;\n</code></pre> - ServiceMonitor: Monitors applications through Kubernetes services. Use this when you have services in front of your pods and want load balancing across replicas. - PodMonitor: Monitors pods directly. Use this for more granular control, when monitoring DaemonSets/StatefulSets, or when you don't have services.</p> <p>For detailed configuration examples, see: - ServiceMonitor Examples - Basic and authenticated service monitoring - PodMonitor Examples - Direct pod monitoring and StatefulSet monitoring</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/observability/#service-discovery_1","title":"Service Discovery","text":"<p>All deployed observability services can be found in your team namespace:</p> <pre><code># List all services\noc get svc -n &lt;team-namespace&gt;\n\n# List all routes (external access)\noc get routes -n &lt;team-namespace&gt;\n\n# Check specific observability pods\noc get pods -l app.kubernetes.io/part-of=observability -n &lt;team-namespace&gt;\n</code></pre> <p>All deployed observability services can be found in your team namespace:</p> <pre><code># List all services\noc get svc -n &lt;team-namespace&gt;\n\n# List all routes (external access)\noc get routes -n &lt;team-namespace&gt;\n\n# Check specific observability pods\noc get pods -l app.kubernetes.io/part-of=observability -n &lt;team-namespace&gt;\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/","title":"PodMonitor Examples","text":"<p>PodMonitor resources allow Prometheus to directly scrape metrics from pods, bypassing Kubernetes services. This is useful for monitoring pods that don't have services or when you need more granular control.</p> <p>\u26a0\ufe0f CRITICAL: Team Label Required for Discovery</p> <p>All PodMonitor resources MUST include the team label <code>soprasteria/team: \"&lt;your-team-name&gt;\"</code> or they will be completely ignored by the team Prometheus stack and no metrics will be collected.</p> <p>The team monitoring stack uses a <code>resourceSelector</code> that only discovers monitoring resources with this specific label. This is a security feature that ensures teams can only monitor resources within their own namespace.</p> <p>Note: The target Pods themselves do not require the team label - only the PodMonitor resources need it.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>Team monitoring stack must be enabled (<code>observability.monitoringStack.enable: true</code>)</li> <li>Your application pods must expose metrics on an HTTP endpoint</li> <li>CRITICAL: PodMonitors must have the team label: <code>soprasteria/team: \"&lt;team-name&gt;\"</code></li> </ul>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#application-metrics-requirements","title":"Application Metrics Requirements","text":"<p>Your application must expose Prometheus-compatible metrics on an HTTP endpoint. Common requirements:</p> <ol> <li>Metrics Endpoint: Usually <code>/metrics</code> (configurable in PodMonitor)</li> <li>HTTP Port: Accessible port that Prometheus can scrape</li> <li>Metrics Format: Prometheus text format or OpenMetrics format</li> <li>Content-Type: Should return <code>text/plain</code> or <code>application/openmetrics-text</code></li> </ol> <p>Testing your metrics endpoint: <pre><code># Test from within your pod\noc exec -it &lt;pod-name&gt; -n &lt;team-namespace&gt; -- curl localhost:8080/metrics\n\n# Test from another pod in the same namespace\noc run test-pod --image=curlimages/curl -it --rm -- curl &lt;pod-ip&gt;:8080/metrics\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#example-1-basic-podmonitor","title":"Example 1: Basic PodMonitor","text":"<p>This example monitors pods directly by selecting them with labels:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: my-app-podmonitor\n  namespace: team-example\n  labels:\n    soprasteria/team: \"team-example\"  # Required for team monitoring stack\nspec:\n  selector:\n    matchLabels:\n      app: my-web-app\n  podMetricsEndpoints:\n  - port: metrics\n    path: /metrics\n    interval: 30s\n</code></pre> <p>Corresponding Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-web-app\n  namespace: team-example\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: my-web-app\n  template:\n    metadata:\n      labels:\n        app: my-web-app\n        soprasteria/team: \"team-example\"  # Optional - not required for monitoring discovery\n    spec:\n      containers:\n      - name: web-app\n        image: my-web-app:latest\n        ports:\n        - name: metrics  # This name is referenced in PodMonitor\n          containerPort: 8080\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#example-2-podmonitor-for-statefulset","title":"Example 2: PodMonitor for StatefulSet","text":"<p>Monitor StatefulSet pods with consistent naming and additional metadata:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: database-podmonitor\n  namespace: team-example\n  labels:\n    soprasteria/team: \"team-example\"\nspec:\n  selector:\n    matchLabels:\n      app: postgresql\n      statefulset: postgres-cluster\n  podMetricsEndpoints:\n  - port: metrics\n    path: /metrics\n    interval: 60s\n    relabelings:\n    - sourceLabels: [__meta_kubernetes_pod_name]\n      targetLabel: pod_name\n    - sourceLabels: [__meta_kubernetes_pod_node_name]\n      targetLabel: node_name\n</code></pre> <p>Corresponding StatefulSet: <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-cluster\n  namespace: team-example\nspec:\n  serviceName: postgres\n  replicas: 3\n  selector:\n    matchLabels:\n      app: postgresql\n      statefulset: postgres-cluster\n  template:\n    metadata:\n      labels:\n        app: postgresql\n        statefulset: postgres-cluster\n        soprasteria/team: \"team-example\"  # Optional - not required for monitoring discovery\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:13\n        ports:\n        - name: metrics\n          containerPort: 9187\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#podmonitor-vs-servicemonitor","title":"PodMonitor vs ServiceMonitor","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#use-podmonitor-when","title":"Use PodMonitor when:","text":"<ul> <li>You need to monitor pods without services</li> <li>You want more granular control over which pods to monitor</li> <li>You're monitoring DaemonSets or StatefulSets directly</li> <li>You need pod-specific metadata in metrics</li> </ul>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#use-servicemonitor-when","title":"Use ServiceMonitor when:","text":"<ul> <li>You have services in front of your pods</li> <li>You want load balancing across multiple pod replicas</li> <li>You prefer service-based discovery</li> </ul>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#common-issues","title":"Common Issues","text":"<ol> <li>Missing team label: Ensure the PodMonitor has <code>soprasteria/team: \"&lt;team-name&gt;\"</code> label</li> <li>Port name mismatch: The port name in PodMonitor must match the container port name</li> <li>Pod selector: Ensure the PodMonitor selector matches your pod labels</li> <li>Metrics endpoint: Verify your application exposes metrics on the specified path</li> </ol>"},{"location":"OpenShift%20Teams/Team%20features/observability/podmonitor-examples/#validation-commands","title":"Validation Commands","text":"<pre><code># Check PodMonitor exists\noc get podmonitor -n &lt;team-namespace&gt;\n\n# List pods that should be monitored\noc get pods -l app=&lt;app-label&gt; -n &lt;team-namespace&gt;\n\n# Test metrics endpoint directly from pod\noc exec -it &lt;pod-name&gt; -n &lt;team-namespace&gt; -- curl localhost:8080/metrics\n\n# Check if Prometheus is scraping the targets\noc port-forward svc/prometheus-operated -n &lt;team-namespace&gt; 9090:9090\n# Then visit http://localhost:9090/targets\n</code></pre>"},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/","title":"ServiceMonitor Examples","text":"<p>ServiceMonitor resources define how Prometheus discovers and scrapes metrics from Kubernetes services.</p> <p>\u26a0\ufe0f CRITICAL: Team Label Required for Discovery</p> <p>All ServiceMonitor resources MUST include the team label <code>soprasteria/team: \"&lt;your-team-name&gt;\"</code> or they will be completely ignored by the team Prometheus stack and no metrics will be collected.</p> <p>The team monitoring stack uses a <code>resourceSelector</code> that only discovers monitoring resources with this specific label. This is a security feature that ensures teams can only monitor resources within their own namespace.</p> <p>Note: The target Services themselves do not require the team label - only the ServiceMonitor resources need it.</p>"},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>Team monitoring stack must be enabled (<code>observability.monitoringStack.enable: true</code>)</li> <li>Your application must expose metrics on an HTTP endpoint</li> <li>CRITICAL: ServiceMonitors must have the team label: <code>soprasteria/team: \"&lt;team-name&gt;\"</code></li> </ul>"},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/#application-metrics-requirements","title":"Application Metrics Requirements","text":"<p>Your application must expose Prometheus-compatible metrics on an HTTP endpoint. Common requirements:</p> <ol> <li>Metrics Endpoint: Usually <code>/metrics</code> (configurable in ServiceMonitor)</li> <li>HTTP Port: Accessible port that Prometheus can scrape</li> <li>Metrics Format: Prometheus text format or OpenMetrics format</li> <li>Content-Type: Should return <code>text/plain</code> or <code>application/openmetrics-text</code></li> </ol> <p>Example metrics endpoint response: <pre><code># HELP http_requests_total Total number of HTTP requests\n# TYPE http_requests_total counter\nhttp_requests_total{method=\"GET\",status=\"200\"} 1027\nhttp_requests_total{method=\"POST\",status=\"200\"} 3\n\n# HELP http_request_duration_seconds HTTP request duration in seconds\n# TYPE http_request_duration_seconds histogram\nhttp_request_duration_seconds_bucket{le=\"0.1\"} 100\nhttp_request_duration_seconds_bucket{le=\"+Inf\"} 1030\nhttp_request_duration_seconds_sum 25.5\nhttp_request_duration_seconds_count 1030\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/#example-1-basic-servicemonitor","title":"Example 1: Basic ServiceMonitor","text":"<p>This example monitors a simple web application exposing metrics on <code>/metrics</code>:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: my-app-servicemonitor\n  namespace: team-example\n  labels:\n    soprasteria/team: \"team-example\"  # Required for team monitoring stack\nspec:\n  selector:\n    matchLabels:\n      app: my-web-app\n  endpoints:\n  - port: metrics\n    path: /metrics\n    interval: 30s\n</code></pre> <p>Corresponding Service: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app-service\n  namespace: team-example\n  labels:\n    app: my-web-app  # This label is used by ServiceMonitor selector\nspec:\n  selector:\n    app: my-web-app\n  ports:\n  - name: metrics  # This name is referenced in ServiceMonitor\n    port: 8080\n    targetPort: 8080\n    protocol: TCP\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/#example-2-servicemonitor-with-authentication","title":"Example 2: ServiceMonitor with Authentication","text":"<p>For applications requiring authentication to access metrics:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: authenticated-app-monitor\n  namespace: team-example\n  labels:\n    soprasteria/team: \"team-example\"\nspec:\n  selector:\n    matchLabels:\n      app: secure-app\n  endpoints:\n  - port: metrics\n    path: /metrics\n    interval: 30s\n    basicAuth:\n      username:\n        name: metrics-auth-secret\n        key: username\n      password:\n        name: metrics-auth-secret\n        key: password\n</code></pre> <p>Required Secret: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: metrics-auth-secret\n  namespace: team-example\ntype: Opaque\ndata:\n  username: &lt;base64-encoded-username&gt;\n  password: &lt;base64-encoded-password&gt;\n</code></pre></p>"},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/#common-issues","title":"Common Issues","text":"<ol> <li>Missing team label: Ensure the ServiceMonitor has <code>soprasteria/team: \"&lt;team-name&gt;\"</code> label</li> <li>Port name mismatch: The port name in ServiceMonitor must match the service port name</li> <li>Metrics endpoint: Verify your application exposes metrics on the specified path</li> </ol>"},{"location":"OpenShift%20Teams/Team%20features/observability/servicemonitor-examples/#validation-commands","title":"Validation Commands","text":"<pre><code># Check ServiceMonitor exists\noc get servicemonitor -n &lt;team-namespace&gt;\n\n# Test metrics endpoint directly\noc exec -it &lt;pod-name&gt; -n &lt;team-namespace&gt; -- curl localhost:8080/metrics\n\n# Check if Prometheus is scraping the target\noc port-forward svc/prometheus-operated -n &lt;team-namespace&gt; 9090:9090\n# Then visit http://localhost:9090/targets\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/backup/","title":"Backup","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/backup/#what-is-backup","title":"What is Backup?","text":"<p>Backup in OpenShift refers to the practice of creating copies of your application\u2019s data and configuration, ensuring that you can restore your system to a previous state in case of data loss, corruption, or disaster.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/backup/#how-to-configure-backup","title":"How to configure Backup","text":"<p>To configure backups for your OpenShift tenant using our Helm chart, include the following YAML configuration in your Helm values file:</p> <pre><code>...\n  backup:\n    backuplabel: &lt;Label name you want to use for backups&gt;\n    take_backup: &lt;Boolean if backup should be taken or not&gt;\n    schedules: \n      - schedule: &lt;Schedule for backups to be taken in cron format (e.g. \"30 * * * *\")&gt;\n        name: &lt;Name of schedule&gt; \n        ttl: &lt;Time until a backup is deleted in format XXXhYYmZZs e.g. 24h0m0s &gt;\n... \n</code></pre> <p>By setting these parameters, you can automate the process of taking regular backups, specifying the frequency, naming, and retention of each backup. This ensures that your data is consistently protected and can be restored as needed. </p>"},{"location":"OpenShift%20Tenants/Tenant%20features/backup/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>In the table below, there is a more detailed description of each variable in the <code>backup</code> feature:</p> Variable Description Example Type Default Value <code>backuplabel</code> The label to set for backups. If this label is not set, a backup of your applications will not be taken. It needs to be on the format <code>Key: value</code> \"backupresource: true\" String \"\" <code>take_backup</code> Used to specify whether backups should be taken or not in the Tenant. true Boolean false <code>schedules[]</code> A list of backup schedules that will be implemented for the tenant List \"\" <code>schedules[].schedule</code> The schedule at which backups should be taken in cron time format \"0 0 * * *\" String \"\" <code>schedules[].name</code> The name of the backup schedule. It is prefixed with the name of the tenant. If the tenant name is <code>poseidon1</code> and this variable is set to <code>daily</code>, the backup schedule will be called <code>poseidon1-daily</code> \"daily\" String \"\" <code>schedules[].ttl</code> The time until the backup is deleted. It is in the format of <code>XXXhYYmZZs</code>, where <code>XXX</code> is the number of hours, <code>YY</code> is the number of minutes, and <code>ZZ</code> is the number of seconds. For example, <code>24h0m0s</code> would indicate that the backup should be deleted after 24 hours. \"48h0m0s\" String \"\""},{"location":"OpenShift%20Tenants/Tenant%20features/backup/#further-reading","title":"Further reading","text":"<p>To learn more about our backup procedures and how the restore process works, please refer to the following resources.</p> <ul> <li>Backup Persistent Volumes using VolumeSnapshots</li> <li>Backup process for tenants using Velero</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/","title":"Configuring Monitoring for A Tenant","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#prerequisites","title":"Prerequisites","text":"<ul> <li>An OpenShift tenant created by Sopra Steria</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#introduction","title":"Introduction","text":"<p>Prometheus alerts help ensure the health and reliability of your systems by allowing you to define rules and respond quickly to issues. By understanding the alerting workflow and using Alertmanager for notifications, you can build an effective alerting setup in OpenShift.</p> <p>This user guide will provide you with further insights and detailed instructions on managing and creating alerts in Openshift.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#overview-of-alerts","title":"Overview of Alerts","text":"<p>We have created a set of alerts tailored to enhance your monitoring and alerting capabilities within your Prometheus environment. These alerts are designed to help you proactively detect issues. Below, we provide an overview of the various alert groups we've developed for you to use. Each group is carefully crafted to address specific aspects of your infrastructure and applications, empowering you to take action swiftly when needed. Explore the following alert groups to gain deeper insights into your monitoring and make informed decisions.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#job-cronjob-alerts","title":"Job &amp; CronJob Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes jobs and cron jobs. These alerts help you keep track of job and cron job executions and potential issues.</li> <li><code>KubernetesJobFailed</code>: This alert triggers when a Kubernetes job fails to complete.</li> <li><code>KubernetesCronjobSuspended</code>: This alert triggers when a Kubernetes CronJob is suspended.</li> <li><code>KubernetesCronjobTooLong</code>: This alert triggers when a Kubernetes CronJob takes too long to complete.</li> <li><code>KubernetesJobSlowCompletion</code>: This alert triggers when Kubernetes Job has not completed all expected tasks (completions) within 12 hours.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#storage-alerts","title":"Storage Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes storage and persistent volume claims. These alerts help you maintain storage reliability.</li> <li><code>KubernetesPersistentvolumeclaimPending</code>: This alert triggers when a PersistentVolumeClaim is in a pending state.</li> <li><code>KubernetesVolumeOutOfDiskSpace</code>: This alert triggers when a volume is almost full.</li> <li><code>KubernetesVolumeFullInFourDays</code>: This alert triggers when a volume is expected to fill up within four days.</li> <li><code>KubernetesPersistentvolumeError</code>: This alert triggers when a Persistent Volume is in a bad state.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#replicasets-alerts","title":"ReplicaSets Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes ReplicaSets. These alerts help you maintain the desired number of replicas.</li> <li><code>KubernetesReplicasetReplicasMismatch</code>: This alert triggers when a Kubernetes ReplicaSet's desired replicas do not match the ready replicas.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#deployments-alerts","title":"Deployments Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes Deployments. These alerts help you ensure that your deployments are running smoothly.</li> <li><code>KubernetesDeploymentReplicasMismatch</code>: This alert triggers when a Kubernetes Deployment's desired replicas do not match the available replicas.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#statefulsets-alerts","title":"StatefulSets Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes StatefulSets. These alerts help you ensure the correct behavior of your StatefulSets.</li> <li><code>KubernetesStatefulsetReplicasMismatch</code>: This alert triggers when a Kubernetes StatefulSet's ready replicas do not match the total replicas.</li> <li><code>KubernetesStatefulsetGenerationMismatch</code>: This alert triggers when a Kubernetes StatefulSet's observed generation does not match its metadata generation for more than 10 minutes.</li> <li><code>KubernetesStatefulsetUpdateNotRolledOut</code>: this alert trigger when a Kubernetes StatefulSet update has not been fully rolled out for more than 10 minutes.</li> <li><code>KubernetesStatefulsetDown</code>: This alert triggers when a Kubernetes StatefulSet goes down.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#resource-quota-alerts","title":"Resource Quota Alerts","text":"<ul> <li>This group of alerts focuses on monitoring resource quota usage within namespaces. These alerts help ensure that resource limits are respected to maintain the stability of the cluster.</li> <li><code>MemoryUsageExceeded</code>: This alert triggers when memory usage in a namespace exceeds 95% of the defined hard limit for 15 minutes.</li> <li><code>CpuUsageExceeded</code>: This alert triggers when CPU usage in a namespace exceeds 95% of the defined hard limit for 15 minutes.</li> <li><code>MemoryRequestsExceeded</code>: This alert triggers when the actual memory usage in the namespace exceeds 95% of the defined hard requests for 15 minutes.</li> <li><code>CpuRequestsExceeded</code>: This alert triggers when the actual CPU usage in the namespace exceeds 95% of the defined hard requests for 15 minutes.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#pod-alerts","title":"Pod Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes containers. These alerts help you stay informed about container-related issues that may affect your applications.</li> <li><code>KubernetesContainerOomKiller</code>: This alert triggers when a Kubernetes container is OOM-killed.</li> <li><code>KubernetesPodNotHealthy</code>: This alert triggers when a pod is not healthy.</li> <li><code>KubernetesPodCrashLooping</code>: This alert triggers when a pod is in a crash-loop.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#hpa-alerts","title":"HPA Alerts","text":"<ul> <li>This group of alerts focuses on monitoring Kubernetes workloads, including StatefulSets, Horizontal Pod Autoscalers (HPAs), and pods. These alerts help you maintain the health and performance of your workloads.</li> <li><code>KubernetesHpaScaleInability</code>: This alert triggers when an HPA is unable to scale.</li> <li><code>KubernetesHpaMetricsUnavailability</code>: This alert triggers when an HPA is unable to collect metrics.</li> <li><code>KubernetesHpaScaleMaximum</code>: This alert triggers when an HPA reaches its maximum scaling limit.</li> <li><code>KubernetesHpaUnderutilized</code>: This alert triggers when an HPA is underutilized.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#how-to-disable-group-alarms","title":"How to disable group alarms","text":"<p>To disable a group alert you need to set a group alert to false in your tenant configuration. </p> <pre><code>monitoring:\n  jobCronJobAlertsEnabled: false # Set to false to disable the JobCronJobAlerts group\n  storageAlertsEnabled: true\n  replicasSetsAlertsEnabled: true\n  deploymentsAlertsEnabled: true\n  statefulSetsAlertsEnabled: true\n  resourceQuotaAlertsEnabled: true\n  podAlertsEnabled: true\n  hpaAlertsEnabled: true\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#configure-your-own-alerts","title":"Configure your own alerts","text":"<p>If you want to make your own alerts you can do this by following the steps below. </p>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#configure-alerting-rules-for-user-defined-projects","title":"Configure alerting rules for user-defined projects","text":"<p>Before you start configuring alerts for your project, please familiarize yourself with the best practices for alerting as described in best-practices. For a guide on how to create Prometheus rules using PromQL, please refer to the prometheus documentation.</p> <p>Note</p> <p>You will not be allowed to create resources directly in openshift-user-workload-monitoring, as this namespace is used across independent teams.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#steps","title":"Steps","text":"<ol> <li> <p>Create a YAML definition for the alerting rules you want to create providing the namespace of your choice.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: example-alert\n  namespace: &lt;Tenant&gt;\nspec:\n  groups:\n  - name: example\n    rules:\n    - alert: VersionAlert\n      expr: version{job=\"prometheus-example-app\"} == 0\n</code></pre> </li> <li> <p>Apply the alerting rule to the cluster. This should be done using GitOps to ensure tracability and transparency of configuration, as well as securing an automated setup if the tenant would have to be rebuilt at some point.</p> </li> <li> <p>Verify that the alerting rules have been created by navigating to Observe --&gt; Alerts for your project in the Developer view of the OpenShift console, as seen below. </p> </li> </ol>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#sending-notifications-to-external-systems","title":"Sending notifications to external systems","text":"<p>You have access to configuring your Alertmanager configuration specific to your project. As for the alerting rules, all objects are project-scoped, so configure all your projects with the desired notification rules. </p> <p>Warning</p> <p>Never store secrets in git. In the AlertmanagerConfig object below, you should reference secrets created using a secure secret handling system like sealed-secrets or external secrets. You should not type in authentication tokens or passwords directly in the AlertmanagerConfig object.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#steps_1","title":"Steps","text":"<ol> <li> <p>Create a YAML definition for the Alertmanager configuration you want to create. An example is found below:</p> <pre><code>apiVersion: monitoring.coreos.com/v1beta1\nkind: AlertmanagerConfig\nmetadata:\n  name: example-routing\n  namespace: &lt;Tenant&gt;\nspec:\n  route:\n    receiver: default\n    groupBy: [job]\n  receivers:\n  - name: default\n    webhookConfigs:\n    - url: https://example.org/post\n</code></pre> </li> <li> <p>Apply the alerting rule to the cluster. This should be done using GitOps to ensure traceability and transparency of configuration, as well as securing an automated setup if the tenant has to be rebuilt at some point.</p> </li> <li>Verify that notifications are sent to the target when alerts are created. A routine for this will be documented shortly.</li> </ol>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#alert-code-examples","title":"Alert code examples","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#job-cronjob-alerts_1","title":"Job &amp; CronJob Alerts","text":"KubernetesJobFailedKubernetesCronjobSuspendedKubernetesCronjobTooLongKubernetesJobSlowCompletion <pre><code>alert: KubernetesJobFailed\nexpr: kube_job_status_failed &gt; 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes Job failed (instance {{`{{ $labels.instance }})`}}'\n  description: \"Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesCronjobSuspended\nexpr: kube_cronjob_spec_suspend != 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes CronJob suspended (instance {{`{{ $labels.instance }}`}})'\n  description: \"CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is suspended\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesCronjobTooLong\nexpr: time() - kube_cronjob_next_schedule_time &gt; 3600\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes CronJob too long (instance {{`{{ $labels.instance}}`}})'\n  description: \"CronJob {{`{{ $labels.namespace }}`}}/{{`{{ $labels.cronjob }}`}} is taking more than 1h to complete.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{     $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesJobSlowCompletion\nexpr: kube_job_spec_completions - kube_job_status_succeeded - kube_job_status_failed &gt; 0\nfor: 12h\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: \"Kubernetes Job failed (instance {{`{{ $labels.instance }}`}})\"\n  description: \"Job {{`{{ $labels.namespace }}`}}/{{`{{ $labels.job_name }}`}} failed to complete\\n  VALUE = {{`{{ $value }}`}}\\n  LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#storage-alerts_1","title":"Storage Alerts","text":"KubernetesPersistentvolumeclaimPendingKubernetesVolumeOutOfDiskSpaceKubernetesVolumeFullInFourDaysKubernetesPersistentvolumeError <pre><code>alert: KubernetesPersistentvolumeclaimPending\nexpr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes PersistentVolumeClaim pending (instance {{`{{ $labels.instance }}`}})'\n  description: \"PersistentVolumeClaim {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}} is pending\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesVolumeOutOfDiskSpace\nexpr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) * 100 &lt; 10\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes Volume out of disk space (instance {{`{{ $labels.instance }}`}})'\n  description: \"Volume is almost full (&lt; 10% left)\\n VALUE = {{`{{  $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesVolumeFullInFourDays\nexpr: predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) &lt; 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes Volume full in four days (instance {{`{{ $labels.instance }})`}}'\n  description: \"Volume under {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}} is expected to fill up within four days. Currently {{`{{ $value humanize }}`}}% is available.\\n VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesPersistentvolumeError\nexpr: kube_persistentvolume_status_phase{phase=~\"Failed|Pending\", job=\"kube-state-metrics\"} &gt; 0\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes PersistentVolume error (instance {{`{{ $labels.instance }})`}}'\n  description: \"Persistent volume {{`{{ $labels.persistentvolume }}`}} is in a bad state\\n VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#replicasets-alerts_1","title":"ReplicaSets Alerts","text":"KubernetesReplicasetReplicasMismatch <pre><code>alert: KubernetesReplicasetReplicasMismatch\nexpr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes ReplicaSet replicas mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"ReplicaSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.replicaset }}`}} replicas mismatch\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#deployments-alerts_1","title":"Deployments Alerts","text":"KubernetesDeploymentReplicasMismatch <pre><code>alert: KubernetesDeploymentReplicasMismatch\nexpr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes Deployment replicas mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} replicas mismatch\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#statefulsets-alerts_1","title":"StatefulSets Alerts","text":"KubernetesStatefulsetReplicasMismatchKubernetesStatefulsetGenerationMismatchKubernetesStatefulsetUpdateNotRolledOutKubernetesStatefulsetDown <pre><code>alert: KubernetesStatefulsetReplicasMismatch\nexpr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes StatefulSet replicas mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet does not match the expected number of replicas.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesStatefulsetGenerationMismatch\nexpr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes StatefulSet generation mismatch (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} has failed but has not been rolled back.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesStatefulsetUpdateNotRolledOut\nexpr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas !=     kube_statefulset_status_replicas_updated)\nfor: 10m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes StatefulSet update not rolled out (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} update has not been rolled out.\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesStatefulsetDown\nexpr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready &gt; 0\nfor: 1m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes StatefulSet down (instance {{`{{ $labels.instance }})`}}'\n  description: \"StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} went down\\n  VALUE = {{`{{ $value }}`}}\\n  LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#resource-quota-alerts_1","title":"Resource Quota Alerts","text":"MemoryUsageExceededCpuUsageExceededMemoryRequestsExceededCpuRequestsExceeded <pre><code>alert: MemoryUsageExceeded\nexpr: (sum by (name)(openshift_clusterresourcequota_usage{type=\"used\", resource=\"limits.memory\", name=\"{{ $.Values.namespace.name }}\"}) / sum by (name)(openshift_clusterresourcequota_usage{type=\"hard\", resource=\"limits.memory\", name=\"{{ $.Values.namespace.name }}\"})) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: Memory usage for {{ $.Values.namespace.name }} exceeded 95%\n  description: Memory usage for {{ $.Values.namespace.name }} has reached or exceeded 80% of its hard limit.\n</code></pre> <pre><code>alert: CpuUsageExceeded\nexpr: (sum by (name)(openshift_clusterresourcequota_usage{type=\"used\", resource=\"limits.cpu\", name=\"{{$.Values.namespace.name }}\"}) / sum by (name)(openshift_clusterresourcequota_usage{type=\"hard\", resource=\"limits.cpu\", name=\"{{ $.Values.namespace.name }}\"})) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: ClusterResourceQuota CPU Usage Exceeded for instance = {{`{{ $labels.name }}`}}\n  description: Resource {{ $.Values.namespace.name }} is using more than 95% of its hard limit for CPU. VALUE = {{`{{ $value }}`}}, LABELS = {{`{{ $labels.name }}`}}\n</code></pre> <pre><code>alert: MemoryRequestsExceeded\nexpr: sum(sum(container_memory_working_set_bytes{job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", cluster=\"\", namespace=~\"{{ $.Values.namespace.name }}.*\", container!=\"\", image!=\"\"}) by (pod))/sum(openshift_clusterresourcequota_usage{resource=\"requests.memory\", type=\"hard\",name=\"{{ $.Values.namespace.name }}\"}) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: Memory requests for {{ $.Values.namespace.name }} exceeded 95%\n  description: Memory requests for {{ $.Values.namespace.name }} has reached or exceeded 95% of its hard limit.\n</code></pre> <pre><code>alert: CpuRequestsExceeded\nexpr: sum(sum by (namespace)(avg_over_time(pod:container_cpu_usage:sum{namespace=~\"{{ $.Values.namespace.name }}.*\"}[1h])))/sum(openshift_clusterresourcequota_usage{resource=\"requests.cpu\", type=\"hard\",name=\"{{ $.Values.namespace.name }}\"}) * 100 &gt;= 95\nfor: 5m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ (index $.Values.environments 0).name }}\n  severity: warning\nannotations:\n  summary: ClusterResourceQuota CPU Requests Exceeded for instance = {{`{{ $labels.name }}`}}\n  description: Resource {{ $.Values.namespace.name }} is requesting more than 95% of its hard limit for CPU. VALUE = {{`{{ $value }}`}}, LABELS = {{`{{ $labels.name }}`}}\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#pod-alerts_1","title":"Pod Alerts","text":"KubernetesContainerOomKillerKubernetesPodNotHealthyKubernetesPodCrashLooping <pre><code>alert: KubernetesContainerOomKiller\nexpr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m &gt;= 1) and ignoring(reason) min_over_time    (kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: \"Kubernetes Container oom killer (instance {{`{{ $labels.instance }})`}}\"\n  description: \"Container {{`{{ $labels.container }}`}} in pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been OOMKilled {{`{{ $value }}`}} times in the     last 10 minutes.\\n VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesPodNotHealthy\nexpr: sum by (namespace, pod)(kube_pod_status_phase{phase=~\"Pending|Unknown|Failed\"}) &gt; 0\nfor: 15m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: critical\nannotations:\n  summary: 'Kubernetes Pod not healthy (instance {{`{{ $labels.instance }})`}}'\n  description: \"Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been in a non-running state for longer than 15 minutes.\\n  VALUE = {{`{{ $value }}`}}\\n  LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesPodCrashLooping\nexpr: increase(kube_pod_container_status_restarts_total[1m]) &gt; 3\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes pod crash looping (instance {{`{{ $labels.instance }})`}}'\n  description: \"Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is crash looping\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/configuring-monitoring-for-a-tenant/#hpa-alerts_1","title":"HPA Alerts","text":"KubernetesHpaScaleInabilityKubernetesHpaMetricsUnavailabilityKubernetesHpaScaleMaximumKubernetesHpaUnderutilized <pre><code>alert: KubernetesHpaScaleInability\nexpr: kube_horizontalpodautoscaler_status_condition{status=\"false\", condition=\"AbleToScale\"} == 1\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes HPA scale inability (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{$labels.horizontalpodautoscaler }}`}} is unable to scale\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>- alert: KubernetesHpaMetricsUnavailability\nexpr: kube_horizontalpodautoscaler_status_condition{status=\"false\", condition=\"ScalingActive\"} == 1\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: warning\nannotations:\n  summary: 'Kubernetes HPA metrics unavailability (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} is unable to collect metrics\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesHpaScaleMaximum\nexpr: kube_horizontalpodautoscaler_status_desired_replicas &gt;= kube_horizontalpodautoscaler_spec_max_replicas\nfor: 2m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: info\nannotations:\n  summary: 'Kubernetes HPA scale maximum (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} has hit the maximum number of desired pods\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre> <pre><code>alert: KubernetesHpaUnderutilized\nexpr: max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) &gt; 3\nfor: 0m\nlabels:\n  namespace: {{ $.Values.namespace.name }}-{{ .name }}\n  severity: info\nannotations:\n  summary: 'Kubernetes HPA underutilized (instance {{`{{ $labels.instance }})`}}'\n  description: \"HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}} has hit the maximum number of desired pods\\n  VALUE = {{`{{ $value }}`}}\\n LABELS = {{`{{ $labels }}`}}\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/environments/","title":"Environments","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/environments/#what-is-an-environment","title":"What is an Environment?","text":"<p>In OpenShift, <code>environments</code> are specific configurations within a namespace designed to represent different stages of development, testing, or production. Each environment can have unique settings for internet access, external URLs, and IP addresses, ensuring that the resources within the environment are properly isolated and managed according to their specific needs.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/environments/#how-to-configure-an-environment","title":"How to configure an Environment","text":"<p>To configure an environment within a namespace, include the following YAML configuration in your Helm values file:</p> <pre><code>...\n  environments:\n    - name: &lt;Name of environment 1 (e.g. dev)&gt;\n      allow_to_internet: &lt;True or false - Set to true if environment should be exposed to internet&gt;\n      custom_auto_defined_targetRevision: &lt;True or false - Set to true if targetRevision should be set by application folder name&gt;\n      externalURLs:\n        - &lt;URL's that should be reachable from the environment (e.g. google.com)&gt;\n      externalIPs:\n        - &lt;IP adresses that should be reachable from environment in CIDR format (e.g. 10.218.0.0/24)&gt;\n      egressip_selector: &lt;Label that matches namespaceselector in EgressIP objects&gt;\n...\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/environments/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>In the table below, you can find a more detailed description of each variable in the <code>environment</code> feature:</p> Variable Description Example Type Default Value <code>environments[].name</code> Name of environment test String \"\" <code>environments[].allow_to_internet</code> Set to <code>true</code> if environment should be exposed to internet. Default <code>false</code>. <code>true</code> Boolean false <code>environments[].custom_auto_defined_targetRevision</code> Set to true if targetRevision should be set by application folder name. Default <code>false</code> <code>true</code> Boolean false <code>environments[].externalURLs</code> A list of URLs that should be reached from a tenant environment [ testurl.com, google.com] List \"\" <code>environments[].externalIPs</code> A list of IP ranges that should be reached from the tenant environment [0.0.0.0/0, 92.0.2.1/24] List \"\" <code>environments[].egressip_selector</code> The label value for egressip-selector. Should match a EgressIP objects namespaceselector. Dev String \"\""},{"location":"OpenShift%20Tenants/Tenant%20features/environments/#custom-targetrevision","title":"Custom targetRevision","text":"<p><code>custom_auto_defined_targetRevision</code> allows setting the targetRevision at the application level for different environments in OpenShift. Read more about this feature here: </p> <ul> <li>Custom auto-defined targetRevision</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/image-pull-secret/","title":"Image Pull Secret","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/image-pull-secret/#what-is-a-image-pull-secret","title":"What is a Image Pull Secret?","text":"<p>In OpenShift, an image pull secret is a Kubernetes secret used to authenticate and pull container images from a private container registry. It stores credentials (username, password, or token) that allow OpenShift to securely access images from registries such as Azure Container Registry (ACR), Docker Hub, AWS ECR, and Google Container Registry (GCR).</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/image-pull-secret/#recommended-implementation-use-external-secrets","title":"Recommended implementation: Use External Secrets","text":"<p>The preferred method for managing image pull secrets is now through the External Secrets integration. This method allows secrets to be securely and automatically synced from Azure Key Vault into OpenShift using <code>ClusterSecretStore</code> configurations, reducing the need to manually manage SealedSecrets.</p> <p>We recommend using the External Secrets method unless you have a specific requirement for <code>SealedSecrets</code>.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/image-pull-secret/#configure-image-pull-secret-with-sealedsecret","title":"Configure Image Pull Secret with SealedSecret","text":"<p>If you still wish to use SealedSecrets, you can configure an image pull secret by including the following configuration in your Helm values file:</p> <pre><code>...\n  image_pull_secret:\n    enable: false # boolean - true/false\n    secrets:\n    - dockerconfigjson: &lt;dockerconfigjson_sealed_secret&gt; # SealedSecret dockerconfigjson object \n      environment: &lt;environment&gt; # Namespace where the image pull secret will be created\n...\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/image-pull-secret/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>In the table below, you can find a more detailed description of each variable in the <code>image_pull_secret</code> feature:</p> Variable Description Example Type Default Value <code>enable</code> When using image pull secret change this value to enable: True false boolean false <code>secrets.dockerconfigjson</code> Sealed Secret dockerconfigjson object containing credentials to acces remote image registry AgA2Z8F5bN... (long encrypted string) String \"\" <code>environment</code> Environment image pull secret will be deployed in \"preprod\" String \"\" <p>For documentation on how you can use your image pull secret to use an image from your private registry in a deployment see the official documentation at: Kubernetes Documentation </p>"},{"location":"OpenShift%20Tenants/Tenant%20features/landingpage/","title":"Tenant Features","text":"<ul> <li>Namespace</li> <li>Environments</li> <li>Network Policy</li> <li>Role Based Access Control</li> <li>GitOps</li> <li>Backup</li> <li>Secret Management</li> <li>Image Pull Secret</li> <li>Slack Alert Integration</li> <li>Team</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/namespace/","title":"Namespace","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/namespace/#what-is-a-namespace","title":"What is a Namespace?","text":"<p>In OpenShift, <code>namespaces</code> provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/namespace/#how-to-order-a-namespace","title":"How to order a Namespace","text":"<p>The <code>namespace</code> feature contains information about Tenant namespaces and Tenant resources. To order a namespace using a Helm chart, include the following YAML configuration in your Helm values file:</p> <pre><code>...\n  namespace:\n    name: &lt;Tenant Name&gt;\n    description: &lt;Tenant Description&gt;\n    displayName: &lt;Display name of the tenant&gt;\n    use_egress_firewall: &lt;Use egress firewall to limit egress traffic from tenant namespaces&gt;\n    enable_global_egress_IPs: &lt;Use egress firewall IPs defined in a global values file&gt;\n    enable_tooling: &lt;Creates a tooling namespace&gt;\n    deploy_grafana: &lt;Deploy Grafana instance in tooling namespace&gt;\n    storage:\n      enable_custom_storageclass: &lt;Enable option to have custom storageclasses&gt;\n    limits:\n      enable: true\n      memory: &lt;Memory limits of all tenants&gt;\n      cpu: &lt;Combined CPU limits of all namespaces in tenant&gt;\n    requests:\n      enable: false\n      memory: &lt;Memory requests of all tenants&gt;\n      cpu: &lt;Combined CPU requests of all namespaces in tenant&gt;\n    container_limitrange:\n      enable: true\n      limits:\n        memory: &lt;Default Memory limitfor containers in all tenant namspace&gt;\n        cpu: &lt;Default cpu limit for containers in all tenant namspaces&gt;\n      requests:\n        memory: &lt;Default Memory request for containers in all tenant namspace&gt;  \n        cpu: &lt;Default cpu request for containers in all tenant\n    labels:\n      custom_labels:\n        &lt;key&gt;: &lt;value&gt;\n        &lt;key&gt;: &lt;value&gt;\n...\n</code></pre> <p>By configuring these parameters, you customize the namespace to meet your specific requirements for your tenant\u2019s applications.</p> <p>To create a specific environment such as <code>&lt;namespace&gt;-test</code> within the namespace, ensure to configure the Environment section according to the instructions found here.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/namespace/#default-container-limits","title":"Default Container limits","text":"<p>By default, containers in the tenant have the following resource limits:</p> <ul> <li> <p>CPU: 250m (0.25 cores)</p> </li> <li> <p>Memory: 1 GiB</p> </li> </ul> <p>These default limits can be modified by adjusting the parameters in <code>namespace.container_limitrange</code> . Maximum Resource Limits:</p> <ul> <li> <p>Maximum CPU per standard Worker Node: 8 cores (excluding management overhead)</p> </li> <li> <p>Maximum Memory per standard Worker Node: 32 GiB (excluding management overhead)</p> </li> </ul> <p>To increase these maximum limits, Sopra Steria can provision larger worker nodes for your tenant. </p>"},{"location":"OpenShift%20Tenants/Tenant%20features/namespace/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>In the table below, you can find a more detailed description of each variable in the <code>namespace</code> feature:</p> Variable Description Example Type Default Value <code>name</code> Base name of tenant. Poseidon1 String \"\" <code>description</code> A description annotation  under each tenant namespace \" This is a test tenant used for testing\" String \"\" <code>displayName</code> Displayname given to each openshift namespace/project \"poseidon1-application1\" String \"\" <code>use_egress_firewall</code> To use egress firewall to limit egress traffic from tenant namespaces. true Boolean true <code>enable_global_egress_IPs</code> Use egress firewall IPs defined in a global values file false Boolean false <code>enable_tooling</code> Creates a tooling namespace which is needed for certain applications such as Grafana true Boolean true <code>deploy_grafana</code> Deploys a grafana instance in the tooling namespace true Boolean true <code>storage.enable_custom_storageclass</code> Option for custom storageclasses true Boolean false <code>limits.memory</code> Combined memory limit for all tenant namespaces. The memory resource is measured in bytes. Memory can be expressed as a plain or fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki. 1Gi String/Int 1Gi <code>limits.cpu</code> Combined cpu limit for all tenant namespaces. Fractional values are allowed. A Container that requests 500m CPU is guaranteed half as much CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example 100m CPU, 100 milliCPU, and 0.1 CPU are all the same. A precision finer than 1m is not allowed. 1 String/Int 1 <code>requests.memory</code> Combined memory requests for all tenant namespaces. The memory resource is measured in bytes. Memory can be expressed as a plain or fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki. 1Gi String/Int 1Gi <code>requests.cpu</code> Combined cpu requests for all tenant namespaces. Fractional values are allowed. A Container that requests 500m CPU is guaranteed half as much CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example 100m CPU, 100 milliCPU, and 0.1 CPU are all the same. A precision finer than 1m is not allowed. 1 String/Int 1 <code>container_limitrange.limits.memory</code> Default memory limits for containers in all tenant namespaces. This can be overridden by specifying values in the container configuration. The memory resource is measured in bytes. Memory can be expressed as a plain or fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki. 64Mi String/Int 1Gi <code>container_limitrange.limits.CPU</code> Default CPU limits for containers in all tenant namespaces. This can be overridden by specifying values in the container configuration. Fractional values are allowed. A Container that requests 0.5 CPU is guaranteed half as much CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example 100m CPU, 100 milliCPU, and 0.1 CPU are all the same. A precision finer than 1m is not allowed. 100m String/Int 250m <code>container_limitrange.requests.memory</code> Default memory request for containers in all tenant namespaces. This can be overridden by specifying values in the container configuration. Same values as for <code>container_limitrange.limits.memory</code> is allowed. 100m String/Int \"\" <code>container_limitrange.requests.CPU</code> Default CPU request for containers in all tenant namespaces. This can be overridden by specifying values in the container configuration. Same values as for <code>container_limitrange.limits.CPU</code> is allowed. 100m String/Int \"\" <code>labels.custom_labels</code> Add custom labels to all namespaces in a tenant test_label: label key: value \"\""},{"location":"OpenShift%20Tenants/Tenant%20features/namespace/#further-reading","title":"Further reading","text":"<p>Kubernetes Offical Documentation - Namespaces</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/networkpolicy/","title":"Network Policy","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/networkpolicy/#what-is-a-network-policy","title":"What is a Network Policy?","text":"<p>In OpenShift, network policies are specifications that define how pods within a namespace are allowed to communicate with each other and with other network endpoints. Network policies enable you to control the traffic flow at the IP address or port level, enhancing security and isolating different parts of your application.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/networkpolicy/#how-to-configure-a-network-policy","title":"How to configure a Network Policy","text":"<p>We offer a few Network Policies from our Helm Chart. To configure a network policy within a namespace using our Helm chart, include the following YAML configuration in your Helm values file:</p> <pre><code>...\n  networkPolicy:\n    allow_same_namespace: true\n    allow_from_openshift_monitoring: true\n    allow_from_openshift_ingress: true\n    allow_from_kube_apiserver_operator: true\n...\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/networkpolicy/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>Each sub-component should have either a <code>true</code> or <code>false</code> value. The default value is set to <code>true</code> for each sub-component.  In the table below, you can find a more detailed description of each variable in the <code>networkpolicy</code> feature:</p> Variable Description Example Type Default Value <code>allow_same_namespace</code> Allows or disallows network communication within the same namespace true Boolean true <code>allow_from_openshift_monitoring</code> Allows or disallows network communication to OpenShift monitoring true Boolean true <code>allow_from_openshift_ingress</code> Allows or disallows incoming network traffic from OpenShift Ingress controllers true Boolean true <code>allow_from_kube_apiserver_operator</code> Allows or disallows communication to Kubernetes API Server Operator. Allowing communication from the operator ensures proper management and maintenance of the API server. false Boolean true <code>allow_from_grafana_operator</code> Allows or disallows communication to Grafana Operator true Boolean true"},{"location":"OpenShift%20Tenants/Tenant%20features/networkpolicy/#further-reading","title":"Further reading","text":"<ul> <li>Kubernetes Offical Documentation - Network Policies</li> <li>OpenShift Offical Documentation - Network Policies</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/rbac/","title":"Role Based Access Control","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/rbac/#what-is-role-based-access-control-rbac","title":"What is Role-Based Access Control (RBAC)?","text":"<p>Role-Based Access Control (RBAC) in OpenShift is a method of regulating access to resources based on the roles of individual users within your organization.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/rbac/#how-to-configure-rbac","title":"How to configure RBAC","text":"<p>To configure RBAC settings for your OpenShift tenant using a Helm chart, include the following YAML configuration in your Helm values file:</p> <pre><code>...\n  rbac:\n    ad_group_write_access: &lt;Azure AD group with write access&gt;\n    ad_group_read_access: &lt;Azure AD group with read access&gt;\n...\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/rbac/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>The <code>rbac</code> feature includes the following variables:</p> Variable Description Example Type Defualt Value <code>ad_group_write_access</code> Azure AD group given admin access to the tenant AD-poseidon1-write String \"\" <code>ad_group_read_access</code> Azure AD group given read-only access to the tenant AD-poseidon1-read String \"\""},{"location":"OpenShift%20Tenants/Tenant%20features/rbac/#further-reading","title":"Further reading","text":"<ul> <li>Kubernetes Offical Documentation - Role Based Access Control</li> <li>OpenShift Offical Documentation - Role Based Access Control</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/","title":"Secret Managment In an OpenShift Tenant","text":"<p>OpenShift tenants support configuring one or more <code>ClusterSecretStore</code> and <code>SecretStore</code> resources.</p> <ul> <li>A ClusterSecretStore is available across all namespaces within the tenant.</li> <li>A SecretStore is scoped to a specific namespace or environment.</li> </ul> <p>Both types of stores integrate with external Key Management Services (KMS), such as Azure Key Vault, to fetch and manage secrets within OpenShift. You can read more about how External Secrets work in this section.</p> <p>In addition to reading secrets, tenants can now use <code>ClusterSecretStore</code> to automatically create common secrets through predefined templates. This includes: </p> <ul> <li>Image Pull Secret \u2014 used to authenticate container image pulls from private registries  </li> <li>Slack Webhook URL \u2014 used to send messages or alerts to Slack channels  </li> </ul> <p>These secrets are generated using values stored in the configured Key Vault and made available to workloads within the tenant\u2019s namespaces.</p> <p>Info</p> <p>OpenShift tenants are configured to use Azure Key Vault as the Key Management System (KMS). Contact your OpenShift administrator if you require support for a different KMS.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#pre-requisites","title":"Pre-requisites","text":"<p>Ensure you have the following:</p> <ul> <li>An Azure Key Vault instance</li> <li>An Azure App Registration with associated ClientID and ClientSecret</li> <li>The App Registration must be assigned the <code>Key Vault Secrets User</code> role for the relevant Key Vault</li> <li>Your Azure Tenant ID</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#setting-up-clustersecretstore","title":"Setting up ClusterSecretStore","text":"<p>A <code>ClusterSecretStore</code> defines a connection to an Azure Key Vault and enables sharing of secrets across multiple namespaces in OpenShift. Below is an example configuration for defining a <code>ClusterSecretStore</code> in your tenant values:</p> Configuration TemplateExample <pre><code>secret_management:\n  external_secrets:\n    cluster_secret_store: \n    - name: &lt;cluster_secret_store_name&gt;\n      environment: &lt;namespace&gt; \n      tenant_id: &lt;azure_tenant_id&gt; \n      keyvault_url: &lt;keyvault_url&gt; \n      client_id: &lt;encrypted_client_id&gt;          # Encrypted with SealedSecret \n      client_secret: &lt;encrypted_client_secret&gt;  # Encrypted with SealedSecret\n</code></pre> <pre><code>secret_management:\n  external_secrets:\n    cluster_secret_stores: \n    - name: cluster-secret-store\n      environment: poseidon1-dev\n      tenant_id: 8f3c5b3a-12d4-4e9f-9b92-7f04d2c44abc\n      keyvault_url: https://poseidon1.vault.azure.net/\n      client_id: AgAlMkJ1FkdAaFFebrbwMsadZTdlz3BgP2dtsI3FZJmIl3McPD[...]\n      client_secret: AgB4MfXJu6oX4I3F+5JG1hSFHCnTtq9IdgdfhaL1Aw0HbX[...]\n</code></pre> <p>Notes</p> <ul> <li><code>environment</code> refers to the environment/namespace where the Key Vault credentials (stored as an OpenShift Secret) will be placed.</li> <li><code>name</code> is used as a suffix to generate the final name of the <code>ClusterSecretStore</code>, which follows the format <code>&lt;tenant_name&gt;-&lt;name&gt;</code>, for example: <code>poseidon1-cluster-secret-store</code>.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#encrypting-key-vault-credentials","title":"Encrypting Key Vault Credentials","text":"<p>To allow OpenShift to authenticate with Azure Key Vault, the credentiwwwwwwals from the App Registration must be encrypted before being stored in the Git repository. This is done using <code>SealedSecrets</code>, which ensures that only the <code>SealedSecrets controller</code> running in the OpenShift cluster can decrypt the values.</p> <p>You can encrypt the credentials using the <code>scripts/encrypt_client_credentials.sh</code> script in your tenant repository, or follow this guide to perform the encryption manually.</p> <p>Additional details on <code>SealedSecrets</code> can be found here.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#setting-up-secretstore","title":"Setting up SecretStore","text":"<p>A <code>SecretStore</code> defines a connection to an Azure Key Vault that is scoped to a single namespace in OpenShift. This is useful when secrets should only be accessible within a specific environment rather than shared across multiple namespaces.</p> <p>Below is an example configuration for adding a <code>SecretStore</code> to your tenant:</p> Configuration TemplateExample <pre><code>secret_management:\n  external_secrets:\n    secret_stores:\n    - name: &lt;secret_store_name&gt;\n      environment: &lt;namespace&gt;\n      tenant_id: &lt;azure_tenant_id&gt;\n      keyvault_url: &lt;keyvault_url&gt;\n      client_id: &lt;encrypted_client_id&gt;\n      client_secret: &lt;encrypted_client_secret&gt;\n      cluster_secret_store_ref: &lt;cluster_secret_store_name&gt;\n</code></pre> <pre><code>secret_management:\n  external_secrets:\n    secret_stores:\n    - name: poseidon1-dev-secret-store\n      environment: poseidon1-dev\n      tenant_id: 8f3c5b3a-12d4-4e9f-9b92-7f04d2c44abc\n      keyvault_url: https://poseidon1.vault.azure.net/\n      client_id: a7b1f2c3-d4e5-678f-90ab-1cd2345ef678\n      client_secret: poseidon1-client-secret\n      cluster_secret_store_ref: team-poseidon-gitops\n</code></pre> <p>Notes</p> <ul> <li>The environment field specifies the target namespace in which the <code>SecretStore</code> and corresponding OpenShift secret will be created.</li> <li>The <code>cluster_secret_store_ref</code> links to a previously defined <code>ClusterSecretStore</code>, which holds the encrypted credentials. It is recommended to define this in the team concept.</li> <li>The <code>client_id</code> is provided in plain text, while <code>client_secret</code> refers to the name of the secret in Azure Key Vault. The External Secrets Operator uses this name to retrieve the value and inject it into the OpenShift Secret.</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#managing-common-secrets-with-predefined-templates","title":"Managing Common Secrets with Predefined Templates","text":"<p>OpenShift tenants can now define commonly used secrets more easily through a simplified configuration. This includes:</p> <ul> <li>Image Pull Secret \u2013 used to authenticate against private container registries</li> <li>Slack Webhook URL \u2013 used to post messages from OpenShift applications to Slack channels</li> </ul> <p>These secrets are automatically created by the External Secrets Operator using values fetched from an Azure Key Vault via a configured ClusterSecretStore.</p> Configuration TemplateExample <pre><code>secret_management:\n  external_secrets:\n    secrets:\n      image_pull_secret:\n        pull_secret: &lt;name_of_secret_in_keyvault&gt;\n        cluster_secret_store_ref: &lt;name_of_cluster_secret_store&gt;\n      slack_webhook_url:\n        webhook_url: &lt;name_of_secret_in_keyvault&gt;\n        cluster_secret_store_ref: &lt;name_of_cluster_secret_store&gt;\n</code></pre> <pre><code>secret_management:\n  external_secrets:\n    secrets:\n      image_pull_secret:\n        pull_secret: poseidon1-docker-pull-secret\n        cluster_secret_store_ref: team-poseidon-gitops\n      slack_webhook_url:\n        webhook_url: poseidon1-slack-webhook-url\n        cluster_secret_store_ref: team-poseidon-gitops\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#in-depth-description-of-parameters","title":"In-depth Description of parameters","text":"<p>The table below provides detailed descriptions of each variable available in the <code>secret_management.external_secrets</code> configuration.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#clustersecretstore-parameters","title":"ClusterSecretStore parameters","text":"Variable Descriptopn Example Type Default Value <code>cluster_secret_stores</code> A list of ClusterSecretStore entries used to connect to one or more Azure Key Vaults shared across environments \u2013 list <code>[]</code> <code>cluster_secret_stores[].name</code> Suffix used to generate the full <code>ClusterSecretStore</code> name in the format <code>&lt;tenant_name&gt;-&lt;name&gt;</code> <code>gitops</code> string <code>\"\"</code> <code>cluster_secret_stores[].environment</code> Namespace where the SealedSecret with Azure credentials will be stored <code>poseidon1-dev</code> string <code>\"\"</code> <code>cluster_secret_stores[].tenant_id</code> Azure Tenant ID used to access the Key Vault <code>d93d3d23-50e3-46db-b3ad-8c6c281b431e</code> string <code>\"\"</code> <code>cluster_secret_stores[].keyvault_url</code> URL of the Azure Key Vault instance <code>https://poseidon1.vault.azure.net/</code> string <code>\"\"</code> <code>cluster_secret_stores[].client_id</code> Azure App Registration Client ID, encrypted using SealedSecrets <code>AgAlMkJ1FkdAaFFebrbwMsadZTdlz3BgP2dtsI3FZJmIl3McPD[...]</code> sealed string <code>\"\"</code> <code>cluster_secret_stores[].client_secret</code> Azure App Registration Client Secret, encrypted using SealedSecrets <code>AgB4MfXJu6oX4I3F+5JG1hSFHCnTtq9IdgdfhaL1Awsdfs0HbX[...]</code> sealed string <code>\"\"</code>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#secretstore-parameters","title":"SecretStore parameters","text":"Variable Descriptopn Example Type Default Value <code>secret_stores</code> A list of SecretStore entries used to fetch secrets from Azure Key Vault scoped to a single environment \u2013 list <code>[]</code> <code>secret_stores[].name</code> Unique name for the SecretStore configuration <code>poseidon1-dev-secret-store</code> string <code>\"\"</code> <code>secret_stores[].environment</code> Namespace where the resulting OpenShift secrets will be created <code>poseidon1-dev</code> string <code>\"\"</code> <code>secret_stores[].tenant_id</code> Azure Tenant ID used to access the Key Vault <code>d93d3d23-50e3-46db-b3ad-8c6c281b431e</code> string <code>\"\"</code> <code>secret_stores[].keyvault_url</code> URL of the Azure Key Vault instance <code>https://poseidon1.vault.azure.net/</code> string <code>\"\"</code> <code>secret_stores[].client_id</code> Azure App Registration Client ID in plain text <code>a7b1f2c3-d4e5-678f-90ab-1cd2345ef678</code> string <code>\"\"</code> <code>secret_stores[].client_secret</code> Name of the secret in Azure Key Vault that contains the actual Client Secret <code>poseidon1-client-secret</code> string <code>\"\"</code> <code>secret_stores[].cluster_secret_store_ref</code> Name of the ClusterSecretStore used for authentication <code>team-poseidon-gitops</code> string <code>\"\"</code>"},{"location":"OpenShift%20Tenants/Tenant%20features/secret-management/#predefined-secrets-parameters","title":"Predefined Secrets Parameters","text":"Variable Description Example Type Default Value <code>secrets.image_pull_secret.pull_secret</code> Name of the secret in Azure Key Vault that contains the Docker config JSON <code>poseidon1-pull-secret</code> string <code>\"\"</code> <code>secrets.image_pull_secret.cluster_secret_store_ref</code> Reference to the ClusterSecretStore used to access the value in Azure Key Vault <code>team-poseidon-gitops</code> string <code>\"\"</code> <code>secrets.slack_webhook_url.webhook_url</code> Name of the secret in Azure Key Vault that contains the Slack webhook URL <code>poseidon1-slack-url</code> string <code>\"\"</code> <code>secrets.slack_webhook_url.cluster_secret_store_ref</code> Reference to the ClusterSecretStore used to access the value in Azure Key Vault <code>team-poseidon-gitops</code> string <code>\"\"</code>"},{"location":"OpenShift%20Tenants/Tenant%20features/slack-alert-integration/","title":"Slack Alert Integration","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/slack-alert-integration/#what-is-slack-alert-integration","title":"What is Slack Alert Integration?","text":"<p>Slack Alert Integration in OpenShift enables real-time monitoring and alerting by sending notifications to Slack channels. This allows teams to respond quickly to incidents. </p>"},{"location":"OpenShift%20Tenants/Tenant%20features/slack-alert-integration/#recommended-implementation-use-external-secrets","title":"Recommended implementation: Use External Secrets","text":"<p>The preferred method for configuring Slack webhook secrets is now through the External Secrets integration. This method allows the webhook URL to be securely retrieved from Azure Key Vault using a <code>ClusterSecretStore</code>, avoiding the need to manage and encrypt secrets manually.</p> <p>We recommend adopting the External Secrets method to simplify secret management and improve security.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/slack-alert-integration/#configure-slack-alert-integration-with-sealedsecret","title":"Configure Slack Alert Integration with SealedSecret","text":"<p>If you still need to use SealedSecrets, you can configure Slack alert integration by including the following in your Helm values file:</p> <pre><code>...\n  slack_alert_integration:\n    enable: False\n    alert_severity: critical # Supports multiple values separated by '|': critical|warning|info\n    webhook_secret:\n      encrypted_webhookURL: &lt;slack_webhook_url_sealed_secret&gt;\n... \n</code></pre> <p>By setting these parameters, you can enable and customize the integration to send alerts to your Slack channels.</p> <p>For more information about configuring the Slack alert integration, read the following guide: </p> <ul> <li>Slack Alert Integration with OpenShift Alerts</li> </ul>"},{"location":"OpenShift%20Tenants/Tenant%20features/slack-alert-integration/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>This integration uses an encrypted Slack webhook URL stored as a secret to send notifications to a Slack channel when an alert of specified severity is triggered.</p> <p>Below is a detailed description of each variable in the <code>slack_alert_integration</code> feature:</p> Variable Description Example Type Default Value <code>enable</code> Toggle the Slack alert integration feature. Set to <code>True</code> to enable. <code>False</code> Boolean false <code>alert_severity</code> The severity level of alerts to be sent. Multiple severities can be specified, separated by <code>|</code>. <code>critical</code> or <code>critical|warning|info</code> String \"\" <code>webhook_secret.encrypted_webhookURL</code> The encrypted Slack webhook URL to be decrypted at runtime for sending alerts. Encrypted String Kubeseal encrypted String \"\""},{"location":"OpenShift%20Tenants/Tenant%20features/team/","title":"Team","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/team/#what-is-a-team","title":"What is a team?","text":"<p>A <code>team</code> is a common definition for tenants that belongs to the same \"team\", configuration here is heavily dependent on configuration done on the team level wich you can find more info about on the  team-definitions page.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/team/#how-to-configure-team","title":"How to configure team","text":"<p>Within the tenant definitions yaml file you can add the following:</p> <p><pre><code>  team:\n    team_name: &lt;team name&gt;\n    grafana_datasource: true\n</code></pre> This will connect the tenant to the correct team.  grafana_datasource is enabled by default, this will add a rolebinding to team service account allowing team admins to extract the token to connect grafana datasources manually.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-argocd/","title":"GitOps - Argo CD Applications &amp; Applicationsets","text":"<p>On this page you can find information related to the <code>argocd</code> feature in the tenant concept. This page contains an example of how to implement the Argo CD applications &amp; Applicationsets</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-argocd/#how-to-configure-argocd-applicationset","title":"How to Configure ArgoCD Applicationset","text":"<p>Below we will go through an example on how to implemet the <code>argocd</code> feature in the tenant chart. The example utilizes the default values for <code>SyncPolicies</code>.</p> <p>Info</p> <p>To use the <code>argocd</code> feature you will also need to define the login credentials Argo CD will use to access the repository</p> <p>How to configure gitops authentication: GitOps - Auth &amp; Creds</p> <p>Below is an example of how the <code>argocd</code> section for defining applicationset details can look like:</p> <pre><code>argocd:\n  enable_auto_defined_apps: true\n  main_git_repository:\n    repourl: \"https://github.com/customer-repo/openshift\" \n    path: \"/path/to/applications\"\n</code></pre> <p>Below is all possible fields to configure for the <code>gitops</code> feature</p> <pre><code>argocd:\n  enable_user_defined_apps_legacy: true\n  enable_user_defined_apps: false\n  enable_auto_defined_apps: true\n  syncPolicy:\n    allowEmpty: true\n    selfHeal: true\n    prune: true\n  main_git_repository:\n    repourl: \"\" \n    basepath: \"\"\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-argocd/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>You can choose to enable the user-defined method or the auto-defined method (You can also choose both).</p> <ol> <li> <p>User-defined method: Create ArgoCD applications in your repository under the path <code>&lt;path&gt;/&lt;all argocd apps here will be generated&gt;</code>.</p> <ul> <li>To enable this choice, you must set the field <code>argocd.enable_user_defined_apps</code> to true.</li> </ul> </li> <li> <p>Auto-defined method: Use an ArgoCD applicationSet to create your applications automatically under the path <code>&lt;path&gt;/&lt;all folders here will be generated&gt;</code>.</p> <ul> <li>To enable this choice you have to set the  <code>argocd.enable_auto_defined_apps field</code> to true. This will create an ApplicationSet for the tenant namespace and will configure new applications when you add new folders in your tenant folder.</li> </ul> </li> </ol> <p>More information about how to set up a Git Repository for ArgoCD on OpenShift can be found here:</p> <ul> <li>OpenShift GitOps - Introduction </li> </ul> Variable Description Example Type Default Value SyncPolicy <code>allowEmpty</code> Allows ArgoCD to sync an ApplicationSet even if it results in an empty application True / False Boolean true <code>selfHeal</code> Automatically repair out-of-sync resources to match the desired state in Git True / False Boolean true <code>prune</code> Remove resources that are not present in the Git repository during sync True / False Boolean true Default <code>repourl</code> The URL of the git repository which ArgoCD will use as its \"source of truth\" https://github.com/customer-repo/openshift String \"\" <code>basepath</code> The basepath of the git repository where ArgoCD manifests are stored poseidon1/ String \"\""},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-authentication/","title":"GitOps - Authentication","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-authentication/#gitops-authentication-credentials","title":"GitOps - Authentication &amp; Credentials","text":"<p>On this page you can find information related to the <code>gitops.authentication</code> feature in the team concept. This page contains an example of how to implement the credentials Argo CD will utilize to authenticate towards a repository</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-authentication/#how-to-configure-authentication-credentials","title":"How to Configure authentication credentials","text":"<p>Below we will go through an example on how to implemet the <code>gitops.authentication</code> feature in the team chart using GitHub App credentials as the authentication method. We will show how this can be done by using either external secrets or sealedsecrets.</p> <p>Warning</p> <p>To use external secrets for defining authentication methodes, you need to have set up a ClusterSecretStore in your tenant to pull down the credentials from your Key Vault provider.</p> <p>How to set up a ClusterSecretStore: Secret Managment</p> <p>Below is an example creating an external secret as an authentication method for Argo CD with a GitHub APP:</p> <pre><code>gitops:\n  authentication:\n    external_secrets:\n      secretstore: gitops\n      github_app:\n      - id: 374237872\n        installation_id: 8947359869\n        private_key: GithubAppPrivateKey\n        repo_url: https://github.com/customer-repo/openshift\n</code></pre> <p>Below is an example using kubeseal to create a sealedsecret as an authentication method for Argo CD with a GitHub APP:</p> <pre><code>gitops:\n  authentication:\n    sealed_secrets:\n      github_app: \n      - id: ngwio847359JHUjigiIIG98796HJ7697gug898GiuG... # SealedSecret\n        installation_id: biUYGVUVh786758GU78gUYGujad78hjJ... # SealedSecret\n        private_key: dhvibibvwiIYFHUKBSBIOH&amp;ABCGFVW895487u... # SealedSecret\n        type: IBKhwofi8979jBHJv78gUi8011IIuhfew98... # SealedSecret\n        repo_url: BIbi8473rege786JKHhgj8BhdksuV78Jl... # SealedSecret\n</code></pre> <p>Below is all possible fields to configure for the <code>gitops.authentication</code> feature</p> <pre><code>gitops:\n  authentication:\n    external_secrets:\n      secretstore: &lt;Name of SecretStore that contains all credentials for different authentication methods&gt;\n      helm_registry:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        registry_url: &lt;ACR login server url&gt;\n      github_app:\n      - id: &lt;The app id for your GitHub App&gt;\n        installation_id: &lt;The installation id for your GitHub App&gt;\n        private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      ssh_key:\n      - private_key: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n      pat:\n      - username: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        password: &lt;Name of Azure Secret in Azure Key Vault&gt;\n        repo_url: &lt;The url of the git repository&gt;\n    sealed_secrets:\n      helm_registry:\n      - username: &lt;ACR username encrypted with sealedsecret&gt;\n        password: &lt;ACR access token encrypted with sealedsecret&gt;\n        enableOCI: \"\"\n        type: \"\"\n        registry_url: &lt;ACR login server url encrypted with sealedsecret&gt;\n      github_app: \n      - id: &lt;The app id for your GitHub App encrypted with sealedsecrets&gt;\n        installation_id: &lt;The installation id for your GitHub App encrypted with sealedsecrets&gt;\n        private_key: &lt;Private key for your GitHub App encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      ssh_key:\n      - private_key: &lt;Private key for your SSH-private-key encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n      pat:\n      - username: &lt;Username used with PAT encrypted with sealedsecrets&gt;\n        password: &lt;PAT encrypted with sealedsecrets&gt;\n        type: &lt;Type should always be git, but must encrypted with sealedsecrets&gt;\n        repo_url: &lt;The url of the git repository encrypted with sealedsecrets&gt;\n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-authentication/#connecting-to-a-git-repository","title":"Connecting to a Git repository","text":"<p>The <code>argocd</code> feature can connect to a Git repository through a Personal Access Token (PAT), a GitHub App or SSH. The table below shows a more detailed description of each variable in the <code>gitops.argocd</code> feature under <code>authentication</code>. The table is split into four categories: </p> <ul> <li>helm_registry: variables needed to connect with a remote helm registry</li> <li>PAT: variables needed to connect with a GitHub PAT token</li> <li>GitHub app: variables need to configure the GitHub app</li> <li>SSH: variables need to configure through SSH</li> </ul> Variable Description Example Type Default Value Argo CD Authentication Helm Registry list[] <code>username</code> ACR username - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal Encrypted String \"\" <code>password</code> ACR password - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal Encrypted String \"\" <code>registry_url</code> The url of the helm registry - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" GitHub App list[] <code>id</code> The app id for your Github App - Clear text or sealedsecret depending on the method See description below String / Kubeseal Encrypted String \"\" <code>installation_id</code> The installation id for your GitHub App - Clear text or sealedsecret depending on the method See description below String / Kubeseal Encrypted String \"\" <code>private_key</code> Private key for your GitHub App - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal Encrypted String \"\" <code>repo_url</code> The url of the git repository - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" SSH list[] <code>private_key</code> Private key for your SSH-private-key - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal encrypted String \"\" <code>repo_url</code> The url of the git repository - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" PAT list[] <code>username</code> The username of the service account connecting to the git repository - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal encrypted String \"\" <code>password</code> The git Personal Access Token for the service account connecting to the git repository - Azure Key Vault secret name or encrypted with sealedsecrets See description below String / Kubeseal encrypted String \"\" <code>repo_url</code> The url of the git repository - Clear text or sealedsecret depending on the method \"\" String / Kubeseal Encrypted String \"\" SealedSecrets Method Specific parameters <code>type</code> Type should either be \"git\" or \"helm\" depending the authentication section - Encrypted with sealedsecrets See description below Kubeseal encrypted String \"\""},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-introduction/","title":"GitOps &amp; ArgoCD Introduction","text":""},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-introduction/#what-is-argocd","title":"What is ArgoCD?","text":"<p>To implement GitOps in our delivery pipeline, we use ArgoCD, a declarative, continuous delivery tool for Kubernetes and OpenShift. ArgoCD allows you to define the desired state of your applications in Git repositories and ensures that your OpenShift cluster matches this state. </p> <p>This seamless integration with GitOps practices ensures that our applications are always up-to-date and that any changes are managed through a robust version control system.</p>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-introduction/#how-to-configure-argo-cd-tenant-concept","title":"How to configure Argo CD - Tenant Concept","text":"<p>Below is the configuration for setting up ArgoCD using our tenant concept:</p> <pre><code>...\n  argocd: \n    enable_user_defined_apps: &lt;Enable creating applications with the user-defined method- app of apps (true/false)&gt;\n    enable_auto_defined_apps: &lt;Enable using automatic application creation with an ArgoCD applicationsets per environment&gt;\n    syncPolicy:\n      allowEmpty: &lt;Allows ArgoCD to sync an ApplicationSet even if it results in an empty application (true/false)&gt;\n      selfHeal: &lt;Automatically repair out-of-sync resources to match the desired state in Git (true/false)&gt;\n      prune: &lt;Remove resources that are not present in the Git repository during sync (true/false)&gt;\n    main_git_repository:\n      repourl: \n      path:\n...    \n</code></pre>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-introduction/#in-depth-description-of-parameters","title":"In-depth description of parameters","text":"<p>ArgoCD provides different ways of automatically deploying and synchronising infrastructure in a cluster. When connecting your Git Repository to your tenant, you have two options for creating applications: </p> <ol> <li> <p>User-defined method: Create ArgoCD applications in your repository under the path <code>&lt;path&gt;/application/&lt;milj\u00f8&gt;/&lt;alle argocd apps generert her&gt;</code>.</p> <ul> <li>To enable this choice, you must set the field <code>argocd.enable_user_defined_apps</code> to true.</li> </ul> </li> <li> <p>Auto-defined method: Use an ArgoCD applicationSet to create your applications automatically under the path <code>&lt;path&gt;/applicationset/&lt;milj\u00f8&gt;/&lt;alle folders her blir generert&gt;</code>.  </p> <ul> <li>To enable this choice you have to set the  <code>argocd.enable_auto_defined_apps field</code> to true. This will create an ApplicationSet for each of the tenants' environments which will configure new applications when you hadd new folders in your repository.</li> </ul> </li> </ol>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/gitops-introduction/#connecting-to-a-git-repository","title":"Connecting to a Git repository","text":"<p>The <code>argocd</code> feature can connect to a Git repository through a Personal Access Token (PAT), a GitHub App or SSH. How to configure the authentication to a Git repository can be seen here </p>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/private-helm-registry-openshift-tenant/","title":"OpenShift Tenant configuration for Private Helm Registry","text":"<p>Configuring the OpenShift Tenant to use a private Helm Registry requires the following steps:</p> <ol> <li>Generate a sealedSecret with the required values</li> <li>Add the sealedSecrets to your OpenShift Tenant  </li> </ol>"},{"location":"OpenShift%20Tenants/Tenant%20features/GitOps/private-helm-registry-openshift-tenant/#configuring-a-sealed-secret-for-integration-with-argo-cd","title":"Configuring a Sealed Secret for integration with Argo CD","text":"<p>Before moving on to the next step we need to have the appropriate values noted down. Note that the chosen method of authentication will dictate the values needed for the username and password.</p> <p>The following values are not dependant on the authentication method:</p> <ul> <li>url: ACR Login server</li> <li>name: ACR Name</li> </ul> <p>The following values are required if method 1 is chosen for authentication:</p> <ul> <li>username: ACR username</li> <li>password: access token</li> </ul> <p>The following values are required if method 2 is chosen for authentication:</p> <ul> <li>username: Application ID (Client ID)</li> <li>password: Application Secret (Client Secret)</li> </ul> <p>Encode each of your values with the following terminal command:</p> <pre><code>echo -n '&lt;RAW_VALUE&gt;' | base64\n</code></pre> <p>Create a generic secrets.yml and insert your encoded values. The following code snippet shows how this can be done correctly:</p> secrets.yml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: example\n  namespace: gitops-developers\ntype: Opaque\nData:\n  name: &lt;ACR_NAME_BASE64_ENCODED&gt;\n  enableOCI: &lt;BASE64_ENCODED&gt; # base64 encoded value of 'True'\n  type: &lt;BASE64_ENCODED&gt; # base64 encoded value of 'helm'\n  password: &lt;ACR_ACCESS_TOKEN__BASE64_ENCODED&gt;\n  username: &lt;ACR_USERNAME_BASE64_ENCODED&gt;\n  url: &lt;ACR_LOGIN_SERVER_BASE64_ENCODED&gt;\n</code></pre> <p>Generate your sealed secret values using the following kubeseal command:</p> <pre><code>kubeseal --cert &lt;PATH_TO_CERTIFICATE&gt; --scope namespace-wide -f &lt;PATH_TO_SECRETS_FILE&gt; -n gitops-developers -o yaml\n</code></pre> <p>In the tenant definition file configure the gitops section and insert your sealed secret values. When empty, this is what section looks like:</p> values.yaml<pre><code>  gitops:\n    helm_registry:\n      enable_custom_helm_registries: false\n      enableOCI: \"\" # Global variable - decrypted value true for namespace gitops-developer - Encrypted and sat by cluster admins\n      type: \"\" # Global variable - decrypted helm for namespace gitops-developer - Encrypted and sat by cluster admins\n      helm_registries:\n      - repository_name: \"\" # Sealed secret ACR Name\n        url: \"\" # Sealed secret ACR Login Server\n        password: \"\" # Sealed secret ACR Access Token\n        username: \"\" # Sealed secret ACR Username\n</code></pre>"},{"location":"Quick%20Start/teams-quick-start/","title":"Quick Start Guide - OpenShift Teams","text":"<p>This guide provides the minimum steps to create a new developer team, create your first tenant, and deploy your first application.</p> <p>Example Names</p> <p>Throughout this guide, we use \"team-poseidon\" as the team name and \"poseidon-web-app\" and \"poseidon-backend-app\" as the application/tenant names. These are examples - replace them with your actual team and application names.</p>"},{"location":"Quick%20Start/teams-quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following components:</p>"},{"location":"Quick%20Start/teams-quick-start/#required-components","title":"Required Components","text":"<ol> <li>Infrastructure definitions configured by Sopra Steria technician (contact your Sopra Steria representative)</li> <li>Team namespace repository for Grafana configuration, secrets, and team-specific resources</li> <li>Application deployment repository containing your application manifests and deployment configurations</li> <li>Azure Key Vault for secret storage</li> <li>Access tokens for accessing Key Vault and repositories</li> </ol>"},{"location":"Quick%20Start/teams-quick-start/#key-vault-requirements","title":"Key Vault Requirements","text":"<p>Your Key Vault must contain these secrets:</p> <ul> <li>Group Sync Secret for RBAC integration</li> <li>Repository Secret for accessing private repositories  </li> <li>Pull Secret for Container Registry access (if not using public registries)</li> </ul>"},{"location":"Quick%20Start/teams-quick-start/#step-1-understanding-the-repository-structure","title":"Step 1: Understanding the Repository Structure","text":"<p>As part of the onboarding process, a repository named <code>infra-ocp4-tenants</code> will be established in your GitHub/Azure DevOps organization with this structure:</p> <pre><code>infra-ocp4-tenants/\n\u251c\u2500\u2500 team-definitions/\n\u2502   \u2514\u2500\u2500 wave-1/\n\u2502       \u251c\u2500\u2500 team-poseidon.yaml\n\u2502       \u251c\u2500\u2500 team-hera.yaml\n\u2502       \u251c\u2500\u2500 team-zeus.yaml\n\u2502       \u2514\u2500\u2500 team-apollo.yaml\n\u2514\u2500\u2500 tenant-definitions/\n    \u251c\u2500\u2500 wave-1/\n    \u2502   \u251c\u2500\u2500 poseidon-web-app-tenant.yaml\n    \u2502   \u2514\u2500\u2500 hera-web-app-tenant.yaml\n    \u2514\u2500\u2500 wave-2/\n        \u251c\u2500\u2500 zeus-web-app-tenant.yaml\n        \u2514\u2500\u2500 apollo-web-app-tenant.yaml\n</code></pre> <p>Wave Deployment Strategy</p> <p>The \"wave\" directories determine the rollout sequence for Helm chart updates. Teams and tenants in <code>wave-1</code> receive changes first, followed by <code>wave-2</code>, and so on. This staged approach enables controlled, incremental deployments and easier troubleshooting.</p>"},{"location":"Quick%20Start/teams-quick-start/#step-2-configure-your-team","title":"Step 2: Configure Your Team","text":""},{"location":"Quick%20Start/teams-quick-start/#sealing-azure-key-vault-credentials","title":"Sealing Azure Key Vault Credentials","text":"<p>To retrieve secrets stored in Azure KeyVault we must create a authentication secret for the keyvault. This must be done with sealed secret. The sealed secret will be sealed using a public certificate stored in the <code>infra-ocp4-tenants</code> repository. </p> <p>Sealed Secrets documentation.</p> <p>These credentials are used by the External Secrets Operator (via ClusterSecretStore / SecretStore) to authenticate to Azure Key Vault.</p> <p>Do NOT commit raw client credentials</p> <p>Never commit the Azure AD application <code>client_id</code> or <code>client_secret</code> in plain text (including in the team values YAML). Always store them as a Sealed Secret (or equivalent secure mechanism) and only reference the resulting Kubernetes Secret.</p>"},{"location":"Quick%20Start/teams-quick-start/#team-configuration-location","title":"Team Configuration Location","text":"<p>Create your team configuration file in <code>team-definitions/wave-1/team-poseidon.yaml</code>.</p>"},{"location":"Quick%20Start/teams-quick-start/#basic-team-configuration","title":"Basic Team Configuration","text":"Basic Team Configuration TemplateBasic Team Example <pre><code>teamname: team-poseidon\nvalues: |\n  team:\n    name: &lt;team name&gt;\n\n  rbac:\n    secret_name: &lt;Azure AD group sync secret name&gt;\n    team_edit: &lt;Azure AD group for team edit role&gt;\n    team_view: &lt;Azure AD group for team view role&gt;\n\n  secret_management:\n    external_secrets:\n      cluster_secret_store: \n      - name: &lt;cluster_secret_store_name&gt;\n        tenant_id: &lt;azure_tenant_id&gt; \n        keyvault_url: &lt;keyvault_url&gt; \n        client_id: &lt;encrypted_client_id&gt;          # Encrypted with SealedSecret \n        client_secret: &lt;encrypted_client_secret&gt;  # Encrypted with SealedSecret\n</code></pre> <pre><code>teamname: team-poseidon  # Replace with your actual team name\nvalues: |\n  team:\n    name: team-poseidon  # This will also be the team namespace name\n\n  rbac:\n    secret_name: team-poseidon-group-sync-secret  # Secret configured in Step 3\n    team_edit: \"AzureAD-Team-Poseidon-Developers\"  # Replace with your write group\n    team_view: \"AzureAD-Team-Poseidon-Viewers\"     # Replace with your read group\n\n  secret_management:\n    external_secrets:\n      cluster_secret_stores: \n      - name: team-poseidon-secrets\n        tenant_id: 12345678-1234-1234-1234-123456789012\n        keyvault_url: https://team-poseidon-kv.vault.azure.net/\n        client_id: ~XxYvTgHjKlMnPqRsT12345GUYGKc353abc435\n        client_secret: ~ZxYvTgHjKlMnPqRsT1234567890abcdef          \n</code></pre>"},{"location":"Quick%20Start/teams-quick-start/#gitops-authentication-options","title":"GitOps Authentication Options","text":"<p>Choose the authentication method that matches your Git provider:</p> GitHub AppPAT (Personal Access Token)SSH KeyHelm Repository <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: https://github.com/yourorg/poseidon-team-config\n    path: \"applications\"\n  authentication:\n    external_secrets:\n      secretstore: team-poseidon-secrets\n      github_app:\n      - id: 374237872\n        installation_id: 8947359869\n        private_key: poseidon-github-app-key\n        repo_url: https://github.com/yourorg/poseidon-app-deployments\n</code></pre> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: https://dev.azure.com/yourorg/poseidon-team/_git/openshift-config\n    path: \"applications\"\n  authentication:\n    external_secrets:\n      secretstore: team-poseidon-secrets\n      pat:\n      - repo_url: https://dev.azure.com/yourorg/poseidon-team/_git/team-config\n        username: poseidon-team\n        password: poseidon-team-pat-token\n</code></pre> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: git@github.com:yourorg/poseidon-team-config.git\n    path: \"team-config\"\n  authentication:\n    external_secrets:\n      secretstore: team-poseidon-secrets\n      ssh_key:\n      - private_key: poseidon-ssh-private-key\n        repo_url: git@github.com:yourorg/poseidon-app-deployments.git  # Replace with your deployment repo URL\n</code></pre> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true # This should default to false in the chart\n    team_repo_url: https://github.com/yourorg/poseidon-team-config  # Replace with your team repo URL\n    path: \"applications\"  # Replace with your config folder path\n  authentication:\n    external_secrets:\n      secretstore: team-poseidon-secrets # References cluster_secret_stores configured above\n      helm_registry:\n      - username: poseidon-acr-user     # Replace with your ACR username secret name in Key Vault\n        password: poseidon-acr-token    # Replace with your ACR token secret name in Key Vault\n        registry_url: poseidonteam.azurecr.io  # Replace with your ACR URL\n</code></pre>"},{"location":"Quick%20Start/teams-quick-start/#example-full-team-configuration","title":"Example full team configuration","text":"Configuration TemplateExample <pre><code>teamname: team-poseidon\nvalues: |\n  team:\n    name: &lt;team name&gt;\n\n  rbac:\n    secret_name: &lt;Azure AD group sync secret name&gt;\n    team_edit: &lt;Azure AD group for team edit role&gt;\n    team_view: &lt;Azure AD group for team view role&gt;\n\n  secret_management:\n    external_secrets:\n      cluster_secret_store: \n      - name: &lt;cluster_secret_store_name&gt;\n        tenant_id: &lt;azure_tenant_id&gt; \n        keyvault_url: &lt;keyvault_url&gt; \n        client_id: &lt;encrypted_client_id&gt;          # Encrypted with SealedSecret \n        client_secret: &lt;encrypted_client_secret&gt;  # Encrypted with SealedSecret\n\n  gitops:\n    argocd:\n      enable_auto_defined_apps: true\n      team_repo_url: &lt;your repo url to team repository&gt;\n      path: &lt;path in repository&gt;\n    authentication:\n      external_secrets:\n        secretstore: &lt;refrence to your ClusterSecretStore&gt;\n        pat:\n        - repo_url: &lt;repository url for authentication&gt;\n          username: &lt;name of Azure Keyvault Secret&gt;\n          password: &lt;name of Azure Keyvault Secret&gt;\n</code></pre> <pre><code>teamname: team-poseidon\nvalues: |\n  team:\n    name: team-poseidon\n\n  rbac:\n    secret_name: team-poseidon-group-sync-secret\n    team_edit: \"AzureAD-Team-Poseidon-Developers\"\n    team_view: \"AzureAD-Team-Poseidon-Viewers\"\n\n  secret_management:\n    external_secrets:\n      cluster_secret_stores: \n      - name: team-poseidon-secrets\n        tenant_id: 12345678-1234-1234-1234-123456789012\n        keyvault_url: https://team-poseidon-kv.vault.azure.net/\n        client_id: ~XxYvTgHjKlMnPqRsT12345GUYGKc353abc435\n        client_secret: ~ZxYvTgHjKlMnPqRsT1234567890abcdef\n\n  gitops:\n    argocd:\n      enable_auto_defined_apps: true\n      team_repo_url: https://dev.azure.com/yourorg/poseidon-team/_git/openshift-config\n      path: \"applications\"\n    authentication:\n      external_secrets:\n        secretstore: team-poseidon-secrets\n        pat:\n        - repo_url: https://dev.azure.com/yourorg/poseidon-team/_git/team-config\n          username: poseidon-team\n          password: poseidon-team-pat-token\n</code></pre> <p>More configuration options can be found in under the OpenShift Teams Features folder.</p>"},{"location":"Quick%20Start/teams-quick-start/#step-3-configure-group-sync-secret","title":"Step 3: Configure Group Sync Secret","text":"<p>Create the group sync secret in your team repository. This secret must be stored in the Azure Key Vault configured in Step 2.</p> <p>Important</p> <p>The service principal used for group sync must have access to read the Azure AD groups configured in your team definition.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: team-poseidon-group-sync-secret\n  namespace: team-poseidon # Replace with your team namespace\nspec:\n  refreshInterval: 10m\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: team-poseidon-team-poseidon-secrets-dev # Format: {namespace-name}-{secretstore-name}-{environment}\n  target:\n    name: team-poseidon-group-sync-secret # Must match secret_name in Step 2\n    creationPolicy: Owner\n    template:\n      type: Opaque\n  data:\n    - secretKey: AZURE_CLIENT_ID\n      remoteRef:\n        key: poseidon-groupsync-client-id\n    - secretKey: AZURE_CLIENT_SECRET\n      remoteRef:\n        key: poseidon-groupsync-client-secret\n    - secretKey: AZURE_TENANT_ID\n      remoteRef:\n        key: poseidon-groupsync-tenant-id\n</code></pre>"},{"location":"Quick%20Start/teams-quick-start/#step-4-set-up-your-application-tenant","title":"Step 4: Set Up Your Application Tenant","text":""},{"location":"Quick%20Start/teams-quick-start/#tenant-configuration-location","title":"Tenant Configuration Location","text":"<p>Create your tenant configuration file in <code>application-definitions/wave-1/poseidon-web-app.yaml</code>.</p>"},{"location":"Quick%20Start/teams-quick-start/#application-tenant-examples","title":"Application Tenant Examples","text":"Web Application TenantBackend Application Tenant <pre><code>appname: poseidon-web-app\nvalues: |\n  team:\n    team_name: \"team-poseidon\"  # Must match your team name from Step 2\n\n  namespace:\n    name: poseidon-web-app\n    description: \"This tenant handles the Poseidon team's web application\"\n    displayName: \"Poseidon Web Application\"\n\n    requests:\n      enable: true\n      memory: 2Gi\n      cpu: 1\n\n  environments:\n    - name: dev # Creates namespace: poseidon-web-app-dev\n      externalURLs:\n        - https://api.external-service.com\n        - https://database.yourorg.com\n    - name: test # Creates namespace: poseidon-web-app-test\n      externalURLs:\n        - https://api.external-service.com\n        - https://database.yourorg.com\n\n  argocd:\n    main_git_repository:\n      repourl: https://github.com/yourorg/poseidon-app-deployments\n      basepath: applicationsets\n</code></pre> <pre><code>appname: poseidon-backend-app\nvalues: |\n  team:\n    team_name: \"team-poseidon\"\n\n  namespace:\n    name: poseidon-backend-app\n    description: \"This tenant handles the Poseidon team's backend API services\"\n    displayName: \"Poseidon Backend API\"\n\n    requests:\n      enable: true\n      memory: 1Gi\n      cpu: 0.5\n\n  environments:\n    - name: dev # Creates: poseidon-backend-app-dev\n      externalURLs:\n        - https://database.yourorg.com\n        - https://auth-service.yourorg.com\n    - name: test # Creates: poseidon-backend-app-test\n      externalURLs:\n        - https://database.yourorg.com\n        - https://auth-service.yourorg.com\n\n  argocd:\n    main_git_repository:\n      repourl: https://github.com/yourorg/poseidon-backend-deployments\n      basepath: applicationsets\n</code></pre> <p>Multiple Applications</p> <p>Teams can manage multiple applications by creating separate tenant configuration files. Each application gets its own namespaces and can have different resource requirements.</p>"},{"location":"Quick%20Start/teams-quick-start/#step-5-structure-your-deployment-repository","title":"Step 5: Structure Your Deployment Repository","text":"<p>Your deployment repository should follow this structure:</p> <pre><code>poseidon-app-deployments/\n\u251c\u2500\u2500 applicationsets/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u251c\u2500\u2500 poseidon-web-app/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 servicemonitor.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 route.yaml\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-backend-app/\n\u2502   \u2502       \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502       \u251c\u2500\u2500 deployment.yaml\n\u2502   \u2502       \u251c\u2500\u2500 service.yaml\n\u2502   \u2502       \u251c\u2500\u2500 servicemonitor.yaml\n\u2502   \u2502       \u2514\u2500\u2500 route.yaml\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 poseidon-web-app/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-backend-app/\n\u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 test/\n\u2502       \u251c\u2500\u2500 poseidon-web-app/\n\u2502       \u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2502       \u2514\u2500\u2500 poseidon-backend-app/\n\u2502           \u2514\u2500\u2500 kustomization.yaml\n</code></pre>"},{"location":"Quick%20Start/teams-quick-start/#key-components","title":"Key Components","text":"<ul> <li><code>base/[app-name]/</code>: Contains base configuration for each application</li> <li><code>{env}/[app-name]/</code>: Environment-specific patches and customizations</li> <li><code>route.yaml</code>: OpenShift Route for external access</li> <li><code>servicemonitor.yaml</code>: Prometheus monitoring configuration</li> </ul>"},{"location":"Quick%20Start/teams-quick-start/#openshift-route-example","title":"OpenShift Route Example","text":"<pre><code>apiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: poseidon-web-app\nspec:\n  to:\n    kind: Service\n    name: poseidon-web-app\n  port:\n    targetPort: http\n  tls:\n    termination: edge\n</code></pre>"},{"location":"Quick%20Start/teams-quick-start/#step-6-using-secrets-in-applications","title":"Step 6: Using Secrets in Applications","text":"<p>Reference secrets from your cluster secret store in your applications:</p> <pre><code># For application secrets (database, APIs, etc.)\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: poseidon-database-secret\nspec:\n  secretStoreRef:\n    name: team-poseidon-app-secrets  # Reference the team secret store\n    kind: ClusterSecretStore\n  target:\n    name: database-credentials\n  data:\n  - secretKey: connection-string\n    remoteRef:\n      key: poseidon-db-connection-string\n</code></pre>"},{"location":"Quick%20Start/teams-quick-start/#next-steps","title":"Next Steps","text":"<ol> <li>Commit your configurations to the <code>infra-ocp4-tenants</code> repository</li> <li>Verify deployment - Check that namespaces and applications are created</li> <li>Set up monitoring - Configure alerts and dashboards</li> </ol> <p>For additional support, contact the platform team or refer to the detailed documentation.</p>"},{"location":"Quick%20Start/tenants-quick-start/","title":"Quick Start Guide - OpenShift Tenants","text":"<p>This guide provides the minimum steps to create a tenant and deploy your first application without requiring a pre-configured team. Key differences from the team-based approach include:</p> <ul> <li>No team-based RBAC: Access groups are configured directly in the tenant YAML files</li> <li>No team-based secret management: Secrets are managed directly in the tenant configuration</li> <li>No team-based GitOps: Each tenant has its own GitOps configuration</li> <li>No team-based monitoring: Monitoring is configured directly in the tenant</li> </ul> <p>Example Names</p> <p>Throughout this guide, we use \"poseidon-web-app\" and \"poseidon-backend-app\" as the application/tenant names. These are examples - replace them with your actual application names.</p>"},{"location":"Quick%20Start/tenants-quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p>"},{"location":"Quick%20Start/tenants-quick-start/#required-components","title":"Required Components","text":"<ol> <li>Infrastructure definitions configured by Sopra Steria technician (contact your Sopra Steria representative)</li> <li>Application deployment repository containing your application manifests and deployment configurations</li> <li>Access to the <code>infra-ocp4-tenants</code> repository in your GitHub/Azure DevOps organization</li> <li>Azure Key Vault for secret storage (optional)</li> <li>Access tokens for accessing repositories</li> </ol>"},{"location":"Quick%20Start/tenants-quick-start/#step-1-understanding-the-repository-structure","title":"Step 1: Understanding the Repository Structure","text":"<p>Tenants are configured in the <code>application-definitions</code> section of the <code>infra-ocp4-tenants</code> repository:</p> <pre><code>infra-ocp4-tenants/\n\u2514\u2500\u2500 application-definitions/\n    \u251c\u2500\u2500 wave-1/\n    \u2502   \u251c\u2500\u2500 poseidon-web-app-tenant.yaml    # Your new tenant\n    \u2502   \u2514\u2500\u2500 poseidon-backend-app-tenant.yaml # Another tenant (optional)\n    \u2514\u2500\u2500 wave-2/\n        \u2514\u2500\u2500 another-app-tenant.yaml\n</code></pre> <p>Wave Deployment Strategy</p> <p>The \"wave\" directories determine the rollout sequence for Helm chart updates. Tenants in <code>wave-1</code> receive changes first, followed by <code>wave-2</code>, and so on. This staged approach enables controlled, incremental deployments and easier troubleshooting.</p>"},{"location":"Quick%20Start/tenants-quick-start/#step-2-configure-your-tenant","title":"Step 2: Configure Your Tenant","text":""},{"location":"Quick%20Start/tenants-quick-start/#tenant-configuration-location","title":"Tenant Configuration Location","text":"<p>Create your tenant configuration file in <code>application-definitions/wave-1/poseidon-web-app-tenant.yaml</code>.</p>"},{"location":"Quick%20Start/tenants-quick-start/#tenant-examples","title":"Tenant Examples","text":"Basic Tenant TemplateBasic Tenant Example <pre><code>appname: &lt;tenant name&gt;\nvalues: |\n  namespace:\n    name: &lt;tenant name&gt;\n    description: &lt;tenant description&gt;\n    displayName: &lt;tenant display name&gt;        \n    requests:\n      enable: true\n      memory: 1Gi\n      cpu: 1\n\n  monitoring:\n    storageAlertsEnabled: false\n  environments:\n    - name: &lt;environment name&gt;\n      externalURLs:\n        - &lt;url&gt;\n        - &lt;url&gt;\n    - name: &lt;environment name&gt;\n      externalURLs:\n        - &lt;url&gt;\n        - &lt;url&gt;\n\n  rbac:\n    secret_name: &lt;Azure AD group sync secret name&gt; # This might allready have been defined globally.\n    ad_group_write_access: &lt;Azure AD group for write access&gt;\n    ad_group_read_access: &lt;Azure AD group for read access&gt;\n\n  argocd:\n    main_git_repository:\n      repourl: &lt;repository url&gt;\n      basepath: &lt;path in repository&gt;\n\n  gitops:\n    authentication:\n      external_secrets:\n        secretstore: &lt;refrence to your ClusterSecretStore&gt;\n        github_app:\n        - repo_url: &lt;repository url for authentication&gt;\n          id: &lt;GitHub App ID&gt;\n          installation_id: &lt;GitHub App Installation ID&gt;\n          private_key: &lt;name of Azure Keyvault Secret&gt;\n        helm_registry:\n        - registry_url: &lt;registry url for authentication&gt;\n          username: &lt;name of Azure Keyvault Secret&gt;\n          password: &lt;name of Azure Keyvault Secret&gt;\n\n  secret_management:\n    external_secrets:\n      cluster_secret_store: \n      - name: &lt;cluster_secret_store_name&gt;\n        environment: &lt;namespace&gt; \n        tenant_id: &lt;azure_tenant_id&gt; \n        keyvault_url: &lt;keyvault_url&gt; \n        client_id: &lt;encrypted_client_id&gt;          # Encrypted with SealedSecret \n        client_secret: &lt;encrypted_client_secret&gt;  # Encrypted with SealedSecret\n</code></pre> <pre><code>appname: zeus\nvalues: |\n  namespace:\n    name: zeus\n    description: \"zeus\"\n    displayName: \"zeus\"        \n    requests:\n      enable: true\n      memory: 1Gi\n      cpu: 1\n\n  monitoring:\n    storageAlertsEnabled: false\n  environments:\n    - name: dev\n      externalURLs:\n        - testurl.com\n        - login.microsoftonline.com\n    - name: test\n      externalURLs:\n        - woop.com\n        - hello.net\n\n  rbac:\n    secret_name: \"zeus-groupsync-secret\" # This might allready have been defined globally.\n    ad_group_write_access: \"EXAMPLE_OpenShift_Zeus_Write\"\n    ad_group_read_access: \"EXAMPLE_OpenShift_Zeus_Read\"\n\n  argocd:\n    main_git_repository:\n      repourl: \"https://git.example.com/example-org/zeus-deployments.git\"\n      basepath: applicationsets\n\n  gitops:\n    authentication:\n      external_secrets:\n        secretstore: zeus-secrets\n        github_app:\n        - repo_url: \"https://git.example.com/example-org/zeus-deployments\"\n          id: \"123456\"\n          installation_id: \"987654321\"\n          private_key: \"github-app-zeus-private-key\"\n        helm_registry:\n        - registry_url: \"registry.example.com\"\n          username: \"svc-zeus-gitops\"\n          password: \"kv-zeus-acr-token\"\n\n  secret_management:\n    external_secrets:\n      cluster_secret_stores:\n      - name: zeus-secrets\n        environment: dev\n        tenant_id: \"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\"\n        keyvault_url: \"https://kv-zeus.invalid.vault.azure.net/\"\n        client_id: \"bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb\"\n        client_secret: ~ZxYvTgHjKlMnPqRsT1234567890abcdef           # Replace with your client secret name in Key Vault\n</code></pre> <p>More configuration options can be found in under the OpenShift Tenants Features folder.</p> <p>Multiple Applications</p> <p>You can create multiple tenants by creating separate tenant configuration files. Each application gets its own namespaces and can have different resource requirements.</p>"},{"location":"Quick%20Start/tenants-quick-start/#step-3-gitops-configuration","title":"Step 3: GitOps Configuration","text":"<p>Choose an authentication method for Argo CD to access your repositories.</p> PAT (Personal Access Token)GitHub AppSSH KeyHelm Repository <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: https://dev.azure.com/yourorg/tenant/_git/openshift-config\n    path: \"applications\"\n  authentication:\n    external_secrets:\n      secretstore: zeus-secrets\n      pat:\n      - repo_url: https://dev.azure.com/yourorg/tenant/_git/deployments\n        username: svc-zeus\n        password: zeus-pat-token\n</code></pre> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: https://github.com/yourorg/tenant-config\n    path: \"applications\"\n  authentication:\n    external_secrets:\n      secretstore: zeus-secrets\n      github_app:\n      - id: 374237872\n        installation_id: 8947359869\n        private_key: zeus-github-app-key\n        repo_url: https://github.com/yourorg/tenant-deployments\n</code></pre> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: git@github.com:yourorg/tenant-config.git\n    path: \"applications\"\n  authentication:\n    external_secrets:\n      secretstore: zeus-secrets\n      ssh_key:\n      - private_key: zeus-ssh-private-key\n        repo_url: git@github.com:yourorg/tenant-deployments.git\n</code></pre> <pre><code>gitops:\n  argocd:\n    enable_auto_defined_apps: true\n    team_repo_url: https://github.com/yourorg/tenant-config\n    path: \"applications\"\n  authentication:\n    external_secrets:\n      secretstore: zeus-secrets\n      helm_registry:\n      - username: zeus-acr-user\n        password: zeus-acr-token\n        registry_url: zeusteam.azurecr.io\n</code></pre>"},{"location":"Quick%20Start/tenants-quick-start/#step-4-rbac-configuration","title":"Step 4: RBAC Configuration","text":"<p>Configure access groups directly in your tenant and provide credentials via secrets (or External Secrets).</p> <pre><code># Example ExternalSecret for Group Sync credentials (optional)\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: zeus-group-sync-secret\n  namespace: zeus\nspec:\n  refreshInterval: 10m\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: zeus-secrets\n  target:\n    name: zeus-group-sync-secret\n    creationPolicy: Owner\n    template:\n      type: Opaque\n  data:\n    - secretKey: AZURE_CLIENT_ID\n      remoteRef:\n        key: zeus-groupsync-client-id\n    - secretKey: AZURE_CLIENT_SECRET\n      remoteRef:\n        key: zeus-groupsync-client-secret\n    - secretKey: AZURE_TENANT_ID\n      remoteRef:\n        key: zeus-groupsync-tenant-id\n</code></pre>"},{"location":"Quick%20Start/tenants-quick-start/#step-5-structure-your-deployment-repository","title":"Step 5: Structure Your Deployment Repository","text":"<p>Your deployment repository should follow this structure:</p> Standard Kustomize StructureMultiple Repository Structure <pre><code>poseidon-app-deployments/              # Replace with your repository name\n\u251c\u2500\u2500 applicationsets/\n\u2502   \u251c\u2500\u2500 base/                          # Base configurations\n\u2502   \u2502   \u251c\u2500\u2500 poseidon-web-app/          # Web application base\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml     # Base kustomization configuration\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 deployment.yaml        # Web app deployment manifest\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service.yaml           # Web app service manifest\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 route.yaml             # OpenShift route for external access\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 servicemonitor.yaml    # ServiceMonitor for Prometheus\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-backend-app/      # Backend application base\n\u2502   \u2502       \u251c\u2500\u2500 kustomization.yaml     # Base kustomization configuration\n\u2502   \u2502       \u251c\u2500\u2500 deployment.yaml        # Backend app deployment manifest\n\u2502   \u2502       \u251c\u2500\u2500 service.yaml           # Backend app service manifest\n\u2502   \u2502       \u2514\u2500\u2500 servicemonitor.yaml    # ServiceMonitor for Prometheus\n\u2502   \u251c\u2500\u2500 dev/                           # Development environment\n\u2502   \u2502   \u251c\u2500\u2500 poseidon-web-app/          # Web app dev configuration\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml     # Dev environment-specific patches\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-backend-app/      # Backend app dev configuration\n\u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml     # Dev environment-specific patches\n\u2502   \u2514\u2500\u2500 test/                          # Test environment\n\u2502       \u251c\u2500\u2500 poseidon-web-app/          # Web app test configuration\n\u2502       \u2502   \u2514\u2500\u2500 kustomization.yaml     # Test environment-specific patches\n\u2502       \u2514\u2500\u2500 poseidon-backend-app/      # Backend app test configuration\n\u2502           \u2514\u2500\u2500 kustomization.yaml     # Test environment-specific patches\n</code></pre> <pre><code># Web Application Repository\nposeidon-web-app-deployments/\n\u251c\u2500\u2500 applicationsets/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-web-app/\n\u2502   \u2502       \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502       \u251c\u2500\u2500 deployment.yaml\n\u2502   \u2502       \u251c\u2500\u2500 service.yaml\n\u2502   \u2502       \u2514\u2500\u2500 route.yaml\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-web-app/\n\u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 test/\n\u2502       \u2514\u2500\u2500 poseidon-web-app/\n\u2502           \u2514\u2500\u2500 kustomization.yaml\n\n# Backend Application Repository  \nposeidon-backend-deployments/\n\u251c\u2500\u2500 applicationsets/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-backend-app/\n\u2502   \u2502       \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502       \u251c\u2500\u2500 deployment.yaml\n\u2502   \u2502       \u2514\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u2514\u2500\u2500 poseidon-backend-app/\n\u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 test/\n\u2502       \u2514\u2500\u2500 poseidon-backend-app/\n\u2502           \u2514\u2500\u2500 kustomization.yaml\n</code></pre>"},{"location":"Quick%20Start/tenants-quick-start/#key-components","title":"Key Components","text":"<ul> <li><code>base/[app-name]/</code>: Contains base configuration for each application</li> <li><code>{env}/[app-name]/</code>: Environment-specific patches and customizations</li> <li><code>route.yaml</code>: OpenShift Route for external access</li> <li><code>servicemonitor.yaml</code>: Prometheus monitoring configuration</li> </ul>"},{"location":"Quick%20Start/tenants-quick-start/#openshift-route-example","title":"OpenShift Route Example","text":"<pre><code>apiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: poseidon-web-app\nspec:\n  to:\n    kind: Service\n    name: poseidon-web-app\n  port:\n    targetPort: http\n  tls:\n    termination: edge\n</code></pre>"},{"location":"Quick%20Start/tenants-quick-start/#step-6-using-secrets-in-applications","title":"Step 6: Using Secrets in Applications","text":"<p>Reference secrets from your tenant configuration:</p> <pre><code># Examples of using secrets in tenant applications will be added here\n# This will show how to reference and use secrets without team-based secret stores\n</code></pre>"},{"location":"Quick%20Start/tenants-quick-start/#step-7-advanced-configuration","title":"Step 7: Advanced Configuration","text":""},{"location":"Quick%20Start/tenants-quick-start/#resource-quotas","title":"Resource Quotas","text":"<p>Each tenant must have a clusterquota defined with a request. this is how much compute you expect your applications to use. If you define like the example below, your applications cant combined exceed this amount.</p> <pre><code>  requests:\n    memory: 1Gi\n    cpu: 1\n</code></pre>"},{"location":"Quick%20Start/tenants-quick-start/#external-traffic","title":"External Traffic","text":"<p>Based on the specific loadbalancer configuration outside of Openshift the steps to get traffic from internett into your application might differ. You will allways be able to contact the <code>Openshift Route</code> from the same vlan as the openshift environment, so for testing this might suffice</p>"},{"location":"Quick%20Start/tenants-quick-start/#step-9-deployment-process","title":"Step 9: Deployment Process","text":"<ol> <li>Commit tenant configuration to the <code>infra-ocp4-tenants</code> repository</li> <li>Prepare deployment repository with the proper structure</li> <li>Commit application manifests to your deployment repository</li> <li>Verify deployment - Check that namespaces and applications are created</li> <li>Test connectivity - Ensure external URLs are accessible</li> </ol> <p>For additional support, contact the platform team or refer to the detailed documentation sections.</p>"},{"location":"about/about/","title":"Praesens inrorant","text":""},{"location":"about/about/#tamen-ite-magnae","title":"Tamen ite magnae","text":"<p>Lorem markdownum memorique fugabitur vidit urbe, modo tu bobus exsul meo referat quies Triopeida. Stratosque petis, prima et Mavortis Iovis quaeque nec nimbisque ipsa adsensu primum iuventus si suo eburno! Virbius rursusque, magnum oras reverentia relinquite totum ignavus? Sunt opus mentita refert hi ter amans velamine Latinis spumigeroque novercae venantum. Ales evomit sine est volumine caerula temporis quis ardet!</p> <ul> <li>Nymphas domus</li> <li>Frustra orsa</li> <li>Loqui deprendere neque infelix saevique et tigride</li> <li>Quanto occasus</li> </ul>"},{"location":"about/about/#restabat-spectata-contactu-phoebe","title":"Restabat spectata contactu Phoebe","text":"<p>Sonuit suum rerum Peneiaque caelum, Stygiisque longae Telephus oppida evanida Dianam; quam poenas, que dubitare densa. Effugies pectore, est quique aeripedes messes brevis ut loqui labores quis locis. Cornibus ille circumspexit reluctanti tanta, iam nec praeterita invita virides et. Nos patulos coniugis auratis, nutritaque socialia nuntia, hinc terra eadem reddebat perterrita occupat inque divesque medio.</p> <ul> <li>Ut onus sub</li> <li>Abscedat ducitur harenis</li> <li>Et Cenaeo has mecum coniugis pacem iubis</li> <li>Omnes geras te multa</li> </ul> <p>Struxerit sonabat quisque, per glandibus ego amore quo, maduere reliquit; non quod non iactura carpere. Successit tumentibus aves, sua ostro, et, o Europam exitio, mea pater; deus haut cratera? Ambustaque genuit. Sine nefas perdet discede tenuit.</p>"},{"location":"about/about/#scilicet-fidem-terga-fera-ense-quidem-onus","title":"Scilicet fidem terga fera ense quidem onus","text":"<p>Perpetuas omnique vires sororibus certum herbas! Loqui nec: dixit nec subito perterrita procul tenuissima forsitan regis crescitque terga quis alimenta. Adfatur suum, ut comitata laborum praesagia loca crudelia natorum, comas praeferri, aut. In quos manibus, imagine sub! Se voluptas. Denne teksten skal finnes via s\u00f8k.</p> <p>Dapibus miles levat, cui paene, pronos et inritata eiusdem horrifer in tamen ad! Iunonia coronam, fudit ex cubitique Phineus taedia concava. Arva suppressit cantatas in boum.</p> <ol> <li>Cursu fit divam et fratris nobis</li> <li>Gemitus oculos simulatque levavit</li> <li>Parari arcton</li> </ol> <p>Ne mersa dictaque colonos ibimus concidit fratrem est indueret locus crinis periturus obvius traxit doluit cognorat. Spectabere vulneris rediret tepidum Cycno venerabile Lacinia boni. Manifesta atque decuit, post legar; carina simul Duxerat.</p>"},{"location":"about/how-to-use-mkdocs/","title":"How to write documentation for MkDocs","text":""},{"location":"about/how-to-use-mkdocs/#introduction","title":"Introduction","text":"<p>This is a step by step guide on how to write documentation for the new Mkdocs setup. </p>"},{"location":"about/how-to-use-mkdocs/#file-structure-within-mkdocs","title":"File structure within MkDocs","text":"<p>MkDocs have a few special files and a file structure that needs to be followed. There's a single configuration file named <code>mkdocs.yml</code>, and a folder named <code>docs/</code> that will contain your documentation source files.</p> <pre><code>.\n\u251c\u2500 docs/\n\u2502  \u2514\u2500 Nav heading 1/\n\u2502     \u251c\u2500 file1.md\n\u2502     \u2514\u2500 file2.md\n\u2502  \u2514\u2500 Nav heading 2/\n\u2502     \u251c\u2500 file1.md\n\u2502     \u2514\u2500 file2.md\n\u2502  \u2514\u2500 index.md\n\u2514\u2500 mkdocs.yml\n</code></pre> <p>All documentation files should be stored under the folder named <code>docs/</code> and within this file there should be a subfolder for each subject with approriate files.</p> <p>The file <code>mkdocs.yml</code> is the file where all the configuration is stored and written, and it it in this file you will be creating the the navigation bar links. How this is done is explained in the secetions Create file under existing subfolder and Create file under new subfolder</p>"},{"location":"about/how-to-use-mkdocs/#start-writing-documentation","title":"Start writing documentation","text":""},{"location":"about/how-to-use-mkdocs/#clone-the-github-repository","title":"Clone the GitHub repository","text":"<p>The first thing you need to do is to clone the document repository found here: </p> <ul> <li>Sopra Steria - OpenShift</li> </ul> <p>Change directory to where you want the repository stored locally and run the git clone command of your preference.</p> HTTPSSSH <pre><code>git clone https://github.com/Sopra-Steria-Norge-Kubernetes/OpenShift.git\n</code></pre> <pre><code>git clone git@github.com:Sopra-Steria-Norge-Kubernetes/OpenShift.git\n</code></pre>"},{"location":"about/how-to-use-mkdocs/#install-mkdocs-and-material-theme","title":"Install MkDocs and Material Theme","text":"<p>After cloning the repository you'll need to install MkDocs and the theme we are using on our site. Run the command below</p> <pre><code>pip install mkdocs &amp;&amp; pip install mkdocs-material &amp;&amp; pip install mkdocs-glightbox\n</code></pre> <p>MkDocs is now installed in your commandline and to verify you can run <code>mkdocs -V</code> to see the version of MkDocs you running. You are now ready to start writing new documentation.</p>"},{"location":"about/how-to-use-mkdocs/#create-your-documentation","title":"Create your documentation","text":"<p>The firts thing you need to do before writing any documentation is to create a new branch to work on. All pushes to the <code>main</code> branch will trigger the workflow and publish a new version to the public facing documentation. Therefore always work on a branch and when finished create a new pull request to main, before merging and publishing your new documentation. All files should be written and stored in markdown-format (.md files).</p> Create new branch<pre><code>git checkout -b &lt;branch name&gt;\n</code></pre> <p>Warning</p> <p>All branch names and commit-messages will be visible to the public, so be smart when creating new branches and writing commit-messages. </p> <p>MkDocs have a built in feature that lets you see the changes you're making in live preview. To start a local instance of the site run the following command in your terminal</p> Start live preview<pre><code>mkdocs serve\n</code></pre> <p>The live preview will now be available in the browser on http://127.0.0.1:8000/</p>"},{"location":"about/how-to-use-mkdocs/#create-file-under-existing-subfolder","title":"Create file under existing subfolder","text":"<p>If you're writing documentation for a subject that already have a subfolder that suits the purpose of the documentation, create the new file under this subfolder. The next step is to create a new heading in the Navigation-bar on the site, so that the site is visible in MkDocs. </p> <p>In the configuration file <code>mkdocs.yml</code> you'll find the section <code>nav</code> containing all the navigation links visible on the site. Locate the already existing navigation heading that you have created the file under and add a new list entry as shown in the example below. </p> <pre><code>...\nnav: \n  - Get started: index.md\n  - OpenShift Tenants:\n    - New title: OpenShift Tenants/new-file.md\n    - Introduction: OpenShift Tenants/Introduction-to-openshift-tenants.md\n    - Quick start: OpenShift Tenants/Orderopenshift-tenant-quick-start-guide.md\n    - Order OpenShift tenant: OpenShift Tenants/Order-openshift-tenant.md\n    - Encrypt PAT and Github App variables: OpenShift Tenants/Encrypt-PAT-and-GitHub-App-variables-for-argo-CD.md\n  - OpenShift GitOps:\n    - Introduction: OpenShift GitOps/gitops-introduction.md\n    - CI/CD with Tekton Pipelines: OpenShift GitOps/continuous-integration-and-delivery-with-tekton-pipelines.md\n...\n</code></pre> <p>Your new site should now be visible in the navigation bar:</p> <p></p>"},{"location":"about/how-to-use-mkdocs/#create-file-under-new-subfolder","title":"Create file under new subfolder","text":"<p>If you're writing documentation for a subject that does not have a subfolder that suits the purpose of the documentation, then you have to create the new subfolder under <code>docs/</code>. This subfolder should have the same name as the name you want to have in the navigation bar. This will make it easier to navigate the source code and also for other to know where to place new documentation. When the folder is created, create the new file in this folder. </p> <p>In the configuration file <code>mkdocs.yml</code> you'll find the section <code>nav</code> containing all the navigation links visible on the site. In the <code>nav</code> section create a new top-level entry and add a new list entry as shown in the example below.</p> <pre><code>...\nnav: \n  - Get started: index.md\n  - OpenShift Tenants:\n    - Introduction: OpenShift Tenants/Introduction-to-openshift-tenants.md\n    - Quick start: OpenShift Tenants/Orderopenshift-tenant-quick-start-guide.md\n    - Order OpenShift tenant: OpenShift Tenants/Order-openshift-tenant.md\n    - Encrypt PAT and Github App variables: OpenShift Tenants/Encrypt-PAT-and-GitHub-App-variables-for-argo-CD.md\n  - New subject:\n    - New title: New subfolder/new-file.md\n  - OpenShift GitOps:\n    - Introduction: OpenShift GitOps/gitops-introduction.md\n    - CI/CD with Tekton Pipelines: OpenShift GitOps/continuous-integration-and-delivery-with-tekton-pipelines.md\n...\n</code></pre> <p>Your new site should now be visible udner the new subject-heading in the navigation bar:</p> <p></p>"},{"location":"about/how-to-use-mkdocs/#publish-the-changes","title":"Publish the changes","text":"<p>When you're finished writing your documentation it is time to publish these changes to our customers. To do this all you have to do is to create a pull request from your wokring branch to main branch. This will trigger a GitHub Actions Workflow that publishes the documentation to GitHub Pages. You can look at the workflow and how this work here <code>.github/workflows/ci.yml</code></p> <p>You can find our public documentation here:</p> <ul> <li>https://sopra-steria-norge-kubernetes.github.io/OpenShift/</li> </ul>"}]}